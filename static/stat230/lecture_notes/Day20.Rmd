---
title: "Logistic regression for binary responses: Inference"
subtitle: "<br/> STAT 230"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", css/xaringan-themer-solns.css, css/my-theme.css, css/my-font.css]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    seal: false
    nature:
      highlightStyle: googlecode  #http://arm.rbind.io/slides/xaringan.html#77 # idea, magula
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
      titleSlideClass: ["left", "middle", "inverse"]
      ratio: "16:9"
    includes:
      in_header: header.html
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(htmltools.preserve.raw = FALSE)
options(ggrepel.max.overlaps = Inf)

knitr::opts_chunk$set(echo = TRUE, 
                      dev = 'svg',
                      collapse = FALSE, 
                      comment = NA,  # PRINTS IN FRONT OF OUTPUT, default is '##' which comments out output
                      prompt = FALSE, # IF TRUE adds a > before each code input
                      warning = FALSE, 
                      message = FALSE,
                      fig.height = 3, 
                      fig.width = 4,
                      out.width = "100%"
                      )

# load necessary packages
library(Sleuth3)   # Data-set for Sleuth
library(tidyverse)
library(dplyr)
library(countdown)
library(mosaic)
library(ggthemes)
library(xaringanExtra)
library(forcats)
xaringanExtra::use_panelset()
xaringanExtra::use_tachyons()
xaringanExtra::use_clipboard()
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         
  mute_unhighlighted_code = TRUE  
)
library(flipbookr)
library(patchwork)
library(DT)
library(moderndive)
library(knitr)
library(grid)
library(gridExtra)
library(palmerpenguins)
#library(MASS)
library(broom)
#library(car)

select <- dplyr::select

# Set ggplot theme
theme_set(theme_tufte(base_size = 10))

yt <- 0

# read.csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/agstrat.csv")

```


```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


layout: true
  
---

class: title-slide, middle

# .fancy[Logistic regression for binary responses: Inference]

### .fancy[Stat 230]

`r format(Sys.Date(), ' %B %d %Y')`

---

# Overview

.pull-left[
```{r, echo=FALSE, fig.align='center', fig.width=4, fig.height=4, out.width="100%"}

sample <- read.csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/Sample4.csv")

# load library ggplot2
library(ggplot2)


# Plot Predicted data and original data points
ggplot(sample, aes(x=Predictor, y=Class)) + geom_point() +
	stat_smooth(method="glm", color="orange", se=FALSE,
				method.args = list(family=binomial))+
  geom_hline(yintercept = 0.5, linetype = "dashed", col = "grey") +
  geom_text(aes(200, 0.52, label =c("Threshold"))) +
  theme_void()

```

]

.pull-right[

Today: 
<br>
<br>
.blockquote-list[
logistic regression model 
  - inference
  
Deviance
  - model comparisons
]

]

---

# The logistic model

.blockquote-list[
- Our Bernoulli responses are modeled as a function of predictors $X_{i}=x_{1, i}, \ldots, x_{p, i}$ through the probability of success:
$$Y_{i} \mid X_{i} \stackrel{\text { indep. }}{\sim} \operatorname{Bern}\left(\pi\left(X_{i}\right)\right)$$
- Log odds of success (logit):
$$\eta_{i}=\log \left(\frac{\pi\left(X_{i}\right)}{1-\pi\left(X_{i}\right)}\right)=\beta_{0}+\beta_{1} x_{1, i}+\cdots+\beta_{p} x_{p, i}$$
- Probability of success:
$$\pi\left(X_{i}\right)=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}=\frac{e^{\beta_{0}+\beta_{1} x_{1, i}+\cdots+\beta_{p} x_{p, i}}}{1+e^{\beta_{0}+\beta_{1} x_{1, i}+\cdots+\beta_{p} x_{p, i}}}$$
]

---

class: middle

# Generalized linear model

.blockquote.font90[
The .bold[kernel mean function] defines the expected value (mean) of $Y$ as a function of $\eta$.
- in a logistic model, the kernel mean function is the logistic function $E(Y \mid X)=\pi(X)=\frac{e^{\eta}}{1+e^{\eta}}$
]

<br>

.blockquote.font90[
The .bold[link function] defines the linear combination $\eta$ as a function of the mean of $Y$.
- in a logistic model, the link function is the logit function $\eta=\log (\pi /(1-\pi))$
- These two functions are inverses of one another.
]

---

class: middle

# MLR vs logistic inference comparison

.pull-left[
.blockquote[
.bold[MLR]
- Estimation: Maximum likelihood
- One $\beta$ inference: t-distribution inference
- Model comparison inference: ANOVA F-tests
]
]
.pull-right[
.blockquote[
.bold[Logistic regression]
- Estimation: Maximum likelihood
- One $\beta$ inference: z-distribution inference
- Model comparison inference: Drop-in-deviance Chi-square tests
]
]

---

class: middle

# Inference and estimation

.blockquote-list[
- Estimation done using maximum likelihood estimation (MLE)
- Likelihood is the probability of the observed data, written as a function of our unknown $\beta$ 's
$$L(\beta ; d a t a)=\prod_{i=1}^{n} \pi\left(X_{i}\right)^{y_{i}}\left(1-\pi\left(X_{i}\right)\right)^{1-y_{i}}$$
- Find the $\beta^{\prime}$ s that maximize $L(\beta ;$ data $)$
- Unlike SLR or MLR, there is no "closed form" for these MLE $\hat{\beta}_{i}$
- Software uses a numerical optimization method to compute the MLEs $\hat{\beta}_{i}$ and the standard errors $S E\left(\hat{\beta}_{i}\right)$
]

---

class: middle

# Inference and estimation

.blockquote[
- MLE estimates of $\hat{\beta}_{i}$ are approximately normally distributed and unbiased when $n$ is "large enough."

.bold[Confidence intervals for one] $\beta_{i}$

- A $C \%$ confidence interval for $\beta_{i}$ equals
$$\hat{\beta}_{i} \pm z^{*} S E\left(\hat{\beta}_{i}\right)$$
where $z^{*}$ is the $(100-C) / 2$ percentile from the $N(0,1)$ distribution.
]

---

class: middle

# Inference and estimation

.blockquote[

Hypothesis tests for one $\beta_{i}$

- The usual test results given by standard regression output tests whether a parameter value (intercept or slope) is equal to 0 vs. not equal to 0 :
$$H_{0}: \beta_{i}=0 \quad H_{A}: \beta_{i} \neq 0$$
with a test stat of
$$z=\frac{\hat{\beta}_{i}-0}{S E\left(\hat{\beta}_{i}\right)}$$
The $N(0,1)$ is used to compute the $\mathrm{p}$-value that is appropriate for whatever $H_{A}$ is specified.
]


---

class: middle

# Inference and estimation

.blockquote[
.bold[Drop-in-deviance Model comparison tests]
- In a GLM, deviance is measures something similar to residual sum of squares
- When the GLM = MLR, deviance is the same as SSR.
- We use $G^{2}$ to denote deviance of a model
- Our model comparison test compares $G^{2}$ from two competing models
]


---

class: middle


# Drop in Deviance test

.blockquote.font90[
(1) .bold[Hypotheses:] 
\begin{align*}
H_{0} &: \text{ reduced model} \\
H_{A} &: \text{ full model}
\end{align*}

(2) .bold[Test Statistic:] The likelihood ratio test (LRT) stat compares the drop in deviance from the reduced to the full models
$$L R T=G_{\text {reduced }}^{2}-G_{\text {full }}^{2}$$
(3) When $n$ is "large enough", the LRT will have a chi-square $\left(\chi^{2}\right)$ distribution with $d f=d f_{\text {reduced }}-d f_{\text {full }}=\#$ terms tested.

The p-value is a right tailed area
$$\text {p-value}=P\left(\chi^{2}>L R T\right)=1-p c h i s q(L R T, d f)$$
]

---

# Drop in Deviance test

.blockquote.font80[
Special cases of drop in deviance tests:

- The overall drop in deviance test compares a null "intercept only" model to a logistic model:
$$\begin{align*}
H_{0} &: \text{ log(odds)} =\beta_{0}\\
H_{A} &: \text{ log(odds)}=\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{p} x_{p}
\end{align*}$$
- Null deviance is similar in spirit to the total sum of squares in ANOVA.
]

<br>

.blockquote.font80[
If our reduced and full models differ by one term, then the drop in deviance test will test the same hypotheses as the z-test (a.k.a. Wald test) for the term,
  - but the two methods of testing are not identical.
  - tests will usually agree, but if they do not, use the drop in deviance LRT test results.
]

---

# Example: NES

.blockquote.font90[The National Election Studies project recorded party identification for two random samples of people during 1980 and 2000.]

```{r}
nes <- read.csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/NES.csv")
head(nes)
```

---

`r chunk_reveal("nes-barplot", font_size_code="70%", title = "## How does temporal changes in party ID differ across regions?")`

```{r nes-barplot, fig.width = 3, fig.height = 3.5, out.width = "100%", include=FALSE}
library(dplyr)
library(ggthemes)
nes$party <- recode_factor(nes$dem, 
                           `1`="Democrat",
                           `0`="Other")
ggplot(nes, aes(x=year, fill = party)) +
 geom_bar(position="fill") +
 facet_wrap(~region) +
 scale_fill_wsj()
```


---

# Example: NES

$$\begin{aligned}
\operatorname{odds}(\operatorname{dem} \mid x) &= e^{\beta_{0}+\beta_{N E} N E+\beta_{S} S+\beta_{W} W+\beta_{2000} \text { year } 2000 +\beta_{N E: 2000} N E: \text { year } 2000+\beta_{S: 2000} S: \text { year } 2000+\beta_{W: 2000} W: \text { year } 2000}
\end{aligned}$$

.yellow-h[Interpretation in terms of odds]

```{r}
nes_glm1 <- glm(dem ~ region*year , data=nes, family = binomial)
tidy(nes_glm1, conf.int=TRUE, exponentiate=TRUE) #<< 
```


---

# Example: NES

$$\begin{aligned}
\operatorname{logit}(\operatorname{dem} \mid x) &=\beta_{0}+\beta_{N E} N E+\beta_{S} S+\beta_{W} W+\beta_{2000} \text { year } 2000 \\
&+\beta_{N E: 2000} N E: \text { year } 2000+\beta_{S: 2000} S: \text { year } 2000+\beta_{W: 2000} W: \text { year } 2000
\end{aligned}$$

.yellow-h[Interpretation in terms of log of odds]

```{r}
nes_glm1 <- glm(dem ~ region*year , data=nes, family = binomial)
tidy(nes_glm1, conf.int=TRUE) #<< 
```


---

class: action

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Your&nbsp;Turn&nbsp;`r (yt <- yt + 1)`</i>    

.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[

<br>
<br>

.blockquote[
- Go over to the in class activity file
- Go over the class activity in your group
]
]

`r countdown(minutes = 5, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`

---

# Example: NES

.blockquote.font80[
.bold[1980 (baseline year):] The difference in log-odds between the $S$ region and the $N C$ (baseline) region is $\beta_{\text {south }}$ :
$$\operatorname{logit}(\text { region }=S, \text { year }=1980)-\operatorname{logit}(\text { region }=N C, \text { year }=1980)=\beta_{\text {south }}$$
]

<br>

.blockquote.font80[
.bold[1980 (baseline year):] In 1980 , the odds of being a Democrat in the south was $1.57$ times the odds in the north central region. (part $\mathrm{f}$ )
$$\frac{\widehat{\text{odds } (\text { region }=S, \text { year }=1980)}}{\text { odds }(\text { region }=N C, \text { year }=1980)}=e^{\hat{\beta}_{\text {south }}}=e^{0.451}=1.57$$
- This effect is statistically significant $(\mathrm{z}=2.79, \mathrm{p}=0.00534)$.
$$\begin{array}{c}
H_{0}: \beta_{S}=0 \quad \text { test stat: } z=\frac{0.451-0}{0.162}=2.79 \\
p-\text { value }=2 \times P(Z<-2.79)=2 \times \operatorname{pnorm}(-2.79)=0.00534
\end{array}$$
]

---

# Example: NES

.blockquote.font80[
- region = S: The odds of being a Democrat in 2000 is $35 \%$ lower than being a Dem in 1980 in the South region. (part h)
$$\widehat{O R_{\text {South }}}(2000 \text { vs.1980 })=e^{\hat{\beta}_{\text {year 2000 }}} e^{\hat{\beta}_{\text {year2000:South }}(1)}=e^{0.199} e^{-0.633}=0.648$$
]

<br>

.blockquote.font80[
.bold[Compare OR in S vs. NC:] $e^{\hat{\beta}_{2000: S \text { suth }}}=0.531$ is the factor change between the odds ratio for the South compared to NC regions: 
$$\frac{\widehat{O R_{\text {South }}(2000 \text{ vs } .1980)}}{O R_{N C}(2000 \text { vs. 1980 })}=e^{\hat{\beta}_{2000: \text { South }}}=e^{-0.633}=0.531$$
- This is a statistically significant change $(\mathrm{z}=-2.86, \mathrm{p}=0.00427$ )
]

---

class: middle

# Deviance in GLMs

.blockquote-list[
(Residual) Deviance is the term used to measure "unexplained" variation in the response.
- In MLR: deviance $=$ SSR

Small deviance:
- predicted $\hat{\pi}\left(X_{i}\right)$ are close to 1 when $y_{i}=1$
- predicted $\hat{\pi}\left(X_{i}\right)$ are close to 0 when $y_{i}=0$

Deviance will decrease as model terms are added.
]

---

# Deviance

.blockquote-list.font80[
Logistic GLM deviance is the difference of two likelihoods

$$\begin{aligned}
G^{2} &=2[\ln L(\bar{\pi})-\ln L(\hat{\pi}(X))] \\
&=2 \sum_{i=1}^{n}\left[y_{i} \ln \left(\frac{y_{i}}{\hat{\pi}\left(X_{i}\right)}\right)+\left(1-y_{i}\right) \ln \left(\frac{1-y_{i}}{1-\hat{\pi}\left(X_{i}\right)}\right)\right]
\end{aligned}$$
]

<br>

.blockquote.font80[
$L(\hat{\pi}(X))$ : likelihood of the data that plugs in estimates $\hat{\pi}\left(X_{i}\right)$ from the logistic model.

$L(\bar{\pi})$ : likelihood of the data that plugs in estimates $\bar{\pi}=y_{i}$, basing a case's "predicted" value only on the response observed for that case.

- called a saturated model
- will always have a higher likelihood than the logistic model:
$$L(\bar{\pi}) \geq L(\hat{\pi}(X))$$
]

---

class: middle

# Deviance and model comparison in R

.blockquote[
`anova(my.glm)`
- gives the extra deviance explained by a term when it is added to the model above it in the table

`anova(reduced.glm, full.glm, test = "Chisq")`
- gives drop in deviance test results
]

---

class: middle

# Example: NES

.pull-left-60[

.code90[
```{r, collapse=TRUE}
anova(nes_glm1)
```
]
]

.pull-right-40[
<br>
<br>
.blockquote.font80[
Null deviance (no predictors): $3083.3$

Deviance for region model: 3081.9
- adding region drops deviance by $1.4306$
]
]

---

class: middle

# Example: NES

.pull-left-60[
.code90[
```{r, collapse=TRUE}
anova(nes_glm1)
```
]
]
.pull-right-40[
.blockquote.font80[
Deviance for region and year model: $3081.9$
- adding year drops deviance by $0.0006$

Deviance for region, year, region:year model: $3065.5$ 
- adding region: year drops deviance by $16.3611$
]
]

---


class: middle

# Example: NES

.blockquote.font90[
Does the effect of year on odds of being a Democrat depend on region?

$$\begin{aligned}
H_{0}: \log (\text { odds }) &=\beta_{0}+\beta_{1} N E+\beta_{2} S+\beta_{3} W+\beta_{4} Y e a r 2000 \\
H_{A}: \log (\text { odds }) &=\beta_{0}+\beta_{1} N E+\beta_{2} S+\beta_{3} W+\beta_{4} Y \text { ear } 2000+\beta_{5} N E: 2000 \\
&+\beta_{6} S: 2000+\beta_{7} W: 2000
\end{aligned}$$

]


.code90[
```{r, collapse=TRUE}
nes_glm_red <- glm(dem ~ region+year , data=nes, family = binomial) # null model
tidy(nes_glm_red)
```
]

---

# Example: NES

.code90[
```{r, collapse=TRUE}
anova(nes_glm_red, nes_glm1, test = "Chisq")
```
]

.blockquote.font80[
- The LRT stat equals $L R T=3081.9-3065.5=16.361$
- degrees of freedom is 3 , so the $p$-value is
$$P\left(\chi^{2}>16.361\right)=1-p c h i s q(16.361,3)=0.00096$$
- We can conclude that the full model is better than the smaller model. There is at least one region's change in party affiliation between 1980 and 2000 that is different from the other regions.
]

