---
title: "Model Accuracy and Evaluation"
subtitle: "<br/> Fall 2022"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", css/xaringan-themer-solns.css, css/my-theme.css, css/my-font.css]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    seal: false
    nature:
      highlightStyle: googlecode  #http://arm.rbind.io/slides/xaringan.html#77 # idea, magula
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
      titleSlideClass: ["left", "middle", "inverse"]
      ratio: "16:9"
    includes:
      in_header: header.html
editor_options: 
  chunk_output_type: console
header-includes:
    - \usepackage{caption}
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(htmltools.preserve.raw = FALSE)
options(ggrepel.max.overlaps = Inf)

knitr::opts_chunk$set(echo = TRUE, 
                      dev = 'svg',
                      collapse = TRUE, 
                      comment = NA,  # PRINTS IN FRONT OF OUTPUT, default is '##' which comments out output
                      prompt = FALSE, # IF TRUE adds a > before each code input
                      warning = FALSE, 
                      message = FALSE,
                      fig.height = 3, 
                      fig.width = 4,
                      out.width = "100%",
                      prompt = FALSE,
                      rows.print=7
                      )



# load necessary packages
library(tidyr)
library(dplyr)
library(ggplot2)
library(countdown)
library(ggthemes)
library(tidyverse)
library(stringr)
library(xaringanExtra)
xaringanExtra::use_panelset()
xaringanExtra::use_tachyons()
library(flipbookr)
library(htmlwidgets)
library(lubridate)
library(palmerpenguins)
library(fontawesome)
library(caret)
library(class)
library(patchwork)
library(tidymodels)
library(mlbench)     # for PimaIndiansDiabetes2 dataset
library(janitor)
library(parsnip)
library(kknn)
library(paletteer)
library(corrr)
library(scico)
library(skimr)
library(janitor)
library(vip)
library(rpart.plot)
library(ranger)
library(palmerpenguins)
library(ISLR2)
library(forcats)


select <- dplyr::select

# Set ggplot theme
# theme_set(theme_stata(base_size = 10))

yt <- 0

data(PimaIndiansDiabetes)
db <- PimaIndiansDiabetes
```




```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


layout: true
  
---

class: title-slide, middle


# .fancy[Decision Trees and Random Forest]

### .fancy[Stat 220]

.large[Bastola]

`r format(Sys.Date(), ' %B %d %Y')`

---

class: middle

## Decision Tree

.bql[
- learns how to best split the dataset into smaller and smaller subsets to predict the target value
]

.pull-left[
<br>
.bq.font90[
- Data is continuously split according to a certain parameter

- Two main entities:

  + .out-t[nodes]: where the data is split
  + .out-t[leaves]: decisions or final outcomes
]
]
.pull-right[
<br>
```{r echo = FALSE}
knitr::include_graphics("images/taylor-tree.png")
```

]

---

class: middle

# Decision Tree

.yellow-h[Use features to make subsets of cases that are as similar (“pure”) as possible with respect to the outcome]

.bq[
- Start with all observations in one group

- Find the variable/feature/split that best separates the outcome

- Divide the data into two groups (leaves) on the split (node)

- Within each split, find the best variable/split that separates the outcomes

- Continue until the groups are too small or sufficiently “pure”
]


---

# Dataset

.code120[
```{r}
data(PimaIndiansDiabetes2)
db <- PimaIndiansDiabetes2 %>% drop_na() %>%
  mutate(diabetes = fct_relevel(diabetes, ref = "neg"))
```
]

--

```{r}
glimpse(db)
```

---


class: middle

# Data preparation and pre-processing

.code120[
```{r}
set.seed(314) 
db_split <- initial_split(db, prop = 0.75)
db_train <- db_split %>% training()
db_test <- db_split %>% testing()
```
]

.code120[
```{r}
# scaling not needed
db_recipe <- recipe(diabetes ~ ., data = db_train) %>%
 step_dummy(all_nominal(), -all_outcomes()) %>%
 prep()
```
]

---

class: middle

# Model Specification

.bq[
- .out-t.bold[cost_complexity:] The cost complexity parameter, the minimum improvement in the model needed at each node
- .out-t.bold[tree_depth:] The maximum depth of a tree
- .out-t.bold[min_n:] The minimum number of data points in a node that are required for the node to be split further.
]

```{r}
tree_model <- decision_tree(cost_complexity = tune(),
                            tree_depth = tune(),
                            min_n = tune()) %>% 
              set_engine('rpart') %>% 
              set_mode('classification')
```

---

class: middle

# Workflow

.code120[
```{r}
# Combine the model and recipe into a workflow 
tree_workflow <- workflow() %>% 
                 add_model(tree_model) %>% 
                 add_recipe(db_recipe)
```
]

---

class: middle

# Hyperparameter tuning

.code120[
```{r}
# Create folds for cross validation on the training data set
db_folds <- vfold_cv(db_train, v = 5, strata = diabetes)
```
]

.code120[
```{r}
## Create a grid of hyperparameter values to test
tree_grid <- grid_random(cost_complexity(),
                          tree_depth(),
                          min_n(), 
                          size = 10)
```
]

---

class: middle

# View grid

.font130[
```{r}
tree_grid
```
]

---

class: middle

# Tuning Hyperparameters with `tune_grid()`

.code120[
```{r}
# Tune decision tree workflow
set.seed(314)
tree_tuning <- tree_workflow %>% 
               tune_grid(resamples = db_folds,
                         grid = tree_grid)
```
]

---

class: middle

# Best model

```{r}
# Select best model based on accuracy
best_tree <- tree_tuning %>% 
             select_best(metric = 'accuracy')
```



```{r}
# View the best tree parameters
best_tree
```

---

<br>

# Finalize workflow

```{r}
final_tree_workflow <- tree_workflow %>% 
                       finalize_workflow(best_tree)
```

--

# Fit the model

```{r}
tree_wf_fit <- final_tree_workflow %>% 
               fit(data = db_train)
```

--

# Extract fit 

```{r}
tree_fit <- tree_wf_fit %>% 
            extract_fit_parsnip()
```

---

.code80[
```{r, echo=TRUE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "55%", fig.cap="Variable Importance"}
vip(tree_fit)
```
]

---
.code80[
```{r, echo=TRUE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "55%", fig.cap="Decision Tree"}
rpart.plot(tree_fit$fit, roundint = FALSE)
```
]

---

class: middle

# Train and Evaluate With `last_fit()`

```{r}
tree_last_fit <- final_tree_workflow %>% 
                 last_fit(db_split)
```


```{r}
tree_last_fit %>% collect_metrics()
```

---

# Confusion matrix

```{r, echo=FALSE, fig.height = 4, fig.width = 4.5, fig.align='center', out.width=600, warning=FALSE}
tree_predictions <- tree_last_fit %>% collect_predictions()
conf_mat(tree_predictions, truth = diabetes, estimate = .pred_class) %>%  autoplot()
```

---

class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Get the class activity 26.Rmd file from  [moodle](https://moodle.carleton.edu/course/view.php?id=39491) 
- Let's work on group activity 1 together
]



]

`r countdown(minutes = 5, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`

---

class: inverse, middle


> Now, let's talk about random forest


---

class: middle

# Random Forest

.out-t[Random forests take decision trees and construct more powerful models in terms of prediction accuracy.]



.font80.bq[
- <!--The main mechanism that powers this algorithm is--> Repeated sampling (with replacement) of the training data to produce a sequence of decision tree models. 

- These models are then averaged to obtain a single prediction for a given value in the predictor space.

- The random forest model selects a random subset of predictor variables for splitting the predictor space in the tree building process. <!--This is done for every iteration of the algorithm, typically 100 to 2,000 times.-->
]

---

class: middle

# Model Specification

.bq[
- .out-t.bold[mtry:] The number of predictors that will be randomly sampled at each split when creating the tree models

- .out-t.bold[trees:] The number of decision trees to fit and ultimately average

- .out-t.bold[min_n:] The minimum number of data points in a node that are required for the node to be split further
]

---
<br>
<br>

# Model Specification

```{r}
rf_model <- rand_forest(mtry = tune(),
                        trees = tune(),
                        min_n = tune()) %>% 
            set_engine('ranger', importance = "impurity") %>% 
            set_mode('classification')
```

--

# Workflow

```{r}
rf_workflow <- workflow() %>% 
               add_model(rf_model) %>% 
               add_recipe(db_recipe)
```

---

class: middle

# Hyperparameter Tuning

```{r}
## Create a grid of hyperparameter values to test
set.seed(314)
rf_grid <- grid_random(mtry() %>% range_set(c(2, 7)),
                       trees(),
                       min_n(),
                       size = 15)
```

---

# View Grid

```{r}
rf_grid
```

---

class: middle

# Tuning Hyperparameters with `tune_grid()`


```{r}
## Tune random forest workflow
set.seed(314)

rf_tuning <- rf_workflow %>% 
             tune_grid(resamples = db_folds,
                       grid = rf_grid)
```

---

class: middle

# Select best

```{r}
## Select best model based on roc_auc
best_rf <- rf_tuning %>% 
           select_best(metric = 'accuracy')
```

```{r}
# View the best parameters
best_rf
```

---

<br>

<br>

# Finalize workflow

```{r}
final_rf_workflow <- rf_workflow %>% 
                     finalize_workflow(best_rf)
```

--

# Variable Importance

```{r}
rf_wf_fit <- final_rf_workflow %>% 
             fit(data = db_train)

rf_fit <- rf_wf_fit %>% 
          extract_fit_parsnip()

```

---

# Variable Importance

```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
vip(rf_fit)
```


---

class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Please continue working on group activity 2
]

]

`r countdown(minutes = 10, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`
