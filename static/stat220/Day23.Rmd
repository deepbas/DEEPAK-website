---
title: "Linear Regression"
subtitle: "<br/> Spring 2023"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", css/xaringan-themer-solns.css, css/my-theme.css, css/my-font.css]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    seal: false
    nature:
      highlightStyle: googlecode  #http://arm.rbind.io/slides/xaringan.html#77 # idea, magula
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
      titleSlideClass: ["left", "middle", "inverse"]
      ratio: "16:9"
    includes:
      in_header: header.html
editor_options: 
  chunk_output_type: console
header-includes:
    - \usepackage{caption}
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(htmltools.preserve.raw = FALSE)
options(ggrepel.max.overlaps = Inf)

knitr::opts_chunk$set(echo = TRUE, 
                      dev = 'svg',
                      collapse = TRUE, 
                      comment = NA,  # PRINTS IN FRONT OF OUTPUT, default is '##' which comments out output
                      prompt = FALSE, # IF TRUE adds a > before each code input
                      warning = FALSE, 
                      message = FALSE,
                      fig.height = 3, 
                      fig.width = 4,
                      out.width = "100%",
                      prompt = FALSE,
                      rows.print=7
                      )



# load necessary packages
library(tidyr)
library(dplyr)
library(ggplot2)
library(countdown)
library(ggthemes)
library(tidyverse)
library(stringr)
library(xaringanExtra)
xaringanExtra::use_panelset()
xaringanExtra::use_tachyons()
library(flipbookr)
library(htmlwidgets)
library(lubridate)
library(palmerpenguins)
library(fontawesome)
library(class)
library(patchwork)
library(tidymodels)
library(mlbench)     # for PimaIndiansDiabetes2 dataset
library(janitor)
library(parsnip)
library(kknn)
library(paletteer)
library(corrr)
library(scico)
library(gridExtra)


select <- dplyr::select

# Set ggplot theme
# theme_set(theme_stata(base_size = 10))

yt <- 0

standardize <- function(x, na.rm = FALSE) {
  (x - mean(x, na.rm = na.rm)) / sd(x, na.rm = na.rm)
}

# Load the fire data
fire <- read_csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/Algeriafires.csv")
fire <- fire %>% clean_names() %>% na.omit() %>% mutate_at(c(10,13), as.numeric)
fire1 <- fire %>% mutate(across(where(is.numeric), standardize))

fire_raw <- fire %>% select(temperature, isi, classes)


fire1 <- fire %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  mutate(classes = as.factor(classes))


fire_recipe <- recipe(classes ~ ., data = fire_raw) %>%
 step_scale(all_predictors()) %>%
 step_center(all_predictors()) %>%
 prep()

fire_scaled <- bake(fire_recipe, fire_raw)

fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_fit <- fire_knn_spec %>%
 fit(classes ~ ., data = fire_scaled)


data(PimaIndiansDiabetes2)
db <- PimaIndiansDiabetes2
db <- db %>% drop_na() %>% mutate(diabetes = fct_rev(factor(diabetes))) 
```



```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


layout: true
  
---

class: title-slide, middle

# .fancy[Cross Validation and Linear Regression]

### .fancy[Stat 220]

`r format(Sys.Date(), ' %B %d %Y')`


---

class: plain-background


# Resampling methods

<center>
<img src="images/resampling.png" style="width:60%; height:auto;"> <br>
</center>


.yellow-h[Create a series of data sets similar to the training/testing split, always used with the training set]


.footnote[[Kuhn and Johnson (2019)](https://bookdown.org/max/FES/resampling.html)]
---


```{r echo=FALSE, comment=NULL}
set.seed(12345)
fire1 <- fire %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  mutate(classes = as.factor(classes))

fire_split <- initial_split(fire1, prop = 0.5)

# Create training data
fire_train <- fire_split %>%
                    training()


# Create testing data
fire_test <- fire_split %>%
                    testing()
```


```{r, echo=FALSE}
fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_wkflow <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(fire_knn_spec)


fire_knn_fit <- fit(fire_knn_wkflow, data = fire_train)

test_features <- fire_test %>% select(temperature, isi) %>% data.frame()

nn1_pred <- predict(fire_knn_fit, test_features, type = "raw")

fire_results <- fire_test %>% 
  select(classes) %>% 
  bind_cols(predicted = nn1_pred)
  

#create a prediction pt grid

temp_grid <- seq(min(fire1$temperature), max(fire1$temperature), length.out = 100)
isi_grid <- seq(min(fire1$isi), max(fire1$isi), length.out = 100)
plot_grid <- expand.grid(temperature = temp_grid, isi = isi_grid)

knn_pred_grid <- predict(fire_knn_fit, plot_grid, type = "raw")

prediction_table <- bind_cols(plot_grid, data.frame(classes = knn_pred_grid))
```



## Why not to use single (training) test set

```{r, echo=FALSE, fig.width=8, fig.height=4.5, fig.align='center', out.width = "90%"}
ts1 <- ggplot(data = fire_train) +
  geom_point(data = prediction_table, aes(x = temperature, y = isi, color = classes), alpha = 0.05) +
  geom_point(aes(x = temperature, y = isi, color = classes), alpha = 0.5) +
  labs(x = "Temperature", y = "Initial Spread Index (ISI)") +
  scale_fill_wsj() +
  scale_color_wsj() +
  theme_light() +
  theme(legend.position = "none")  +
  ggtitle("Training set #1") + theme(plot.title = element_text(hjust = 0.5))



set.seed(143)
fire_split1 <- initial_split(fire1, prop = 0.75)

# Create training data
fire_train1 <- fire_split1 %>%
                    training()


# Create testing data
fire_test1 <- fire_split1 %>%
                    testing()


fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_wkflow <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(fire_knn_spec)


fire_knn_fit1 <- fit(fire_knn_wkflow, data = fire_train1)

test_features1 <- fire_test1 %>% select(temperature, isi) %>% data.frame()

nn1_pred1 <- predict(fire_knn_fit1, test_features1, type = "raw")

fire_results1 <- fire_test1 %>% 
  select(classes) %>% 
  bind_cols(predicted = nn1_pred1)
  

#create a prediction pt grid

knn_pred_grid1 <- predict(fire_knn_fit1, plot_grid, type = "raw")

prediction_table1 <- bind_cols(plot_grid, data.frame(classes = knn_pred_grid1))



ts2 <- ggplot(data = fire_train1) +
  geom_point(data = prediction_table1, aes(x = temperature, y = isi, color = classes), alpha = 0.05) +
  geom_point(aes(x = temperature, y = isi, color = classes), alpha = 0.5) +
  labs(x = "Temperature", y = "Initial Spread Index (ISI)") +
  scale_fill_wsj() +
  scale_color_wsj() +
  theme_light() +
  theme(legend.position = "none") +
  ggtitle("Training set #2") + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(ts1,ts2, ncol = 2) 

```


---

background-image: url(images/k-folds.png)
background-position: 50% 90%
background-size: 90%

## Cross validation

.bq[
.bold[Idea:] Split the training data up into multiple training-validation pairs, evaluate the classifier on each split and average the performance metrics
]

.footnote[Image courtesy of Dennis Sun]

---

class: middle

## k-fold cross validation

.bq.font90[
1. split the data into $k$ subsets

2. combine the first $k-1$ subsets into a training set and train the classifier

3. evaluate the model predictions on the last (i.e. $k$th) held-out subset

4. repeat steps 2-3 $k$ times (i.e. $k$ "folds"), each time holding out a different one of the $k$ subsets

5. calculate performance metrics from each validation set

6. average each metric over the $k$ folds to come up with a single estimate of that metric
]

---

```{r echo=FALSE, out.width = "75%", fig.width = 5, fig.height = 3, fig.align='center'}
#rm(accuracy, ppv)
#detach(package:caret)
train_complete <- fire_raw

fire_recipe <- recipe(
  classes ~  temperature + isi, 
  data = train_complete
) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors())

knn_spec <- nearest_neighbor(
  weight_func = "rectangular", 
  neighbors = tune() #<<
) %>%
  set_engine("kknn") %>%
  set_mode("classification")

set.seed(1234)

fire_vfold <- vfold_cv(fire_raw, v = 5, strata = classes, repeats = 10)

k_vals <- tibble(neighbors = seq(from = 1, to = 15, by = 1))

knn_fit <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(knn_spec) %>%
  tune_grid(
    resamples = fire_vfold, 
    grid = k_vals,
    metrics = metric_set(accuracy, sensitivity, specificity, ppv, kap, mcc)
    )

cv_metrics <- collect_metrics(knn_fit)

ggplot(cv_metrics %>% filter(.metric == "accuracy"), aes(x = neighbors, y = mean)) +
  geom_point() + 
  geom_line() +
  labs(x = "#Neighbors", y = "Accuracy (Cross-Validation)") +
  theme_light() +
  scale_x_continuous(breaks = 1:15, minor_breaks = NULL) + 
  scale_y_continuous(limits = c(0.93,0.96))

```

.hljs.purple[
- Based on accuracy, $k=4$ appears best, We can look at other metrics. Notice that accuracy doesn't always decrease with $k$
]

---

class: middle

## 5-fold cross validation

Creating the recipe


```{r}
fire_recipe <- recipe(classes ~  temperature + isi, data = fire_train) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>% 
  prep()
```


---

class: middle

## 5-fold cross validation

Create your model specification and use `tune()` as a placeholder for the number of neighbors

```{r}
knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular", 
                             neighbors = tune())  #<<
```


Split the `fire_train` data set into `v = 5` folds, stratified by `classes`

```{r}
# replicate 5-fold cross-validation 10 times, with unique partitions for each iteration
fire_vfold <- vfold_cv(fire_train, v = 5, strata = classes, repeats = 10)
```


---

class: middle

## 5-fold cross validation

Create a grid of $K$ values, the number of neighbors

```{r}
k_vals <- tibble(neighbors = seq(from = 1, to = 40, by = 1))
```


Run 5-fold CV on the `k_vals` grid, storing four performance metrics

```{r}
knn_fit <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(knn_spec) %>%
  tune_grid(resamples = fire_vfold, 
            grid = k_vals,
            metrics = metric_set(accuracy, sensitivity, specificity, ppv))
```


---

class: middle

## Choosing K

Collect the performance metrics 

.font110[
```{r}
cv_metrics <- collect_metrics(knn_fit)
cv_metrics %>% head(6)
```
]

---

class: middle

## Choosing K

Collect the performance metrics and find the best model


```{r}
cv_metrics %>%
  group_by(.metric) %>%
  slice_max(mean) 

```

---

# Choosing K: Visualization

```{r, echo=FALSE, fig.width=8, fig.height=4.5, fig.align='center', out.width = "90%"}
final.results <- cv_metrics %>%  mutate(.metric = as.factor(.metric)) %>%
  select(neighbors, .metric, mean)

final.results %>%
  ggplot(aes(x = neighbors, y = mean, color = forcats::fct_reorder2(.metric, neighbors, mean))) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_color_wsj() + 
  scale_x_continuous(breaks = k_vals[[1]]) +
  theme(panel.grid.minor.x = element_blank())+
  labs(color='Metric', y = "Estimate", x = "K") +
  theme(panel.grid.minor.x = element_blank(),
        axis.text=element_text(size=6, angle = 20))
```

---
background-image: url(images/cv-5.png)
background-position: middle, center
background-size: 55%
## The full process

.footnote[Image source: [rafalab.github.io/dsbook/](rafalab.github.io/dsbook/)]

---


class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Get the class activity 23.Rmd file from  [moodle](https://moodle.carleton.edu/course/view.php?id=43045) 
- Let's work on group activity 1 together
]

]

`r countdown(minutes = 10, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`


---

class: middle

# Supervised Learning 

.bql[
- .b[Supervised learning:] Learning a function that maps an input to an output based on example `inputoutput` pairs.
- .b[Function:] $\mathrm{y}=\mathrm{f}(\mathrm{x})$, where $\mathrm{y}$ is the label (output) we predict, and $\mathrm{x}$ is the feature(s) (input).
- .b[Goal:] Find a function that calculates $y$ from $x$ values accurately for all cases in the training dataset.
]


---

class: middle

## Linear Regression: The Basics

.hljs[Linear regression fits a linear equation to observed data to describe the relationship between variables.]

### Formulation: $y=\beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \epsilon$, where:

.bq.font90[
- $y$ is the dependent variable (what we're trying to predict),
- $x_1, x_2, \cdots$ are independent variables (features),
- $\beta_0, \beta_1, \beta_2, \cdots$ are coefficients, and $\epsilon$ represents the error term.
]

.yellow-h[.b[Objective:] Minimize the differences between the observed values and the values predicted by the linear equation.]


---

class: middle

# Relevant Metrics

.bql[
$$\text{MSE} =\frac{1}{n} \sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2$$


$$\text{RMSE} =\sqrt{\frac{1}{n} \sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2}$$
]


---

class: middle

# Case study: Bike Share Scheme

.hljs[Data from a real study on a bicycle sharing scheme was collected to predict rental numbers based on seasonality and weather conditions.]

.scrollable-table[
```{r, echo=FALSE, results='asis'}
url_data <- "https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv"
# Load the data into the R session
bike <- read_csv(url_data) %>% drop_na() %>% 
  select(-instant, -dteday) %>% 
  mutate_at(vars(c("holiday", "mnth", "season", "weathersit", "weekday", "workingday", "yr")), as.factor)

# View first few rows
bike %>% 
  slice_head(n = 7) %>% 
  knitr::kable()
```
]

---

class: middle

.font70[
| Variable        | Description |
|--------------|-------------|
| instant      | An identifier for each unique row |
| season       | Encoded numerical value for the season (1 for spring, 2 for summer, 3 for fall, 4 for winter) |
| yr           | Observation year in the study, spanning two years (0 for 2011, 1 for 2012) |
| mnth         | Month of observation, numbered from 1 (January) to 12 (December) |
| holiday      | Indicates if the observation was on a public holiday (binary value) |
| weekday      | Day of the week of the observation (0 for Sunday to 6 for Saturday) |
| workingday   | Indicates if the day was a working day (binary value, excluding weekends and holidays) |
| weathersit   | Weather condition category (1 for clear, 2 for mist/cloud, 3 for light rain/snow, 4 for heavy rain/hail/snow/fog) |
| temp         | Normalized temperature in Celsius |
| atemp        | Normalized "feels-like" temperature in Celsius |
| hum          | Normalized humidity level |
| windspeed    | Normalized wind speed |
| rentals      | Count of bicycle rentals recorded |

]

---


.panelset[

.panel[

.panel-name[Descriptive Statistics]

```{r, echo=FALSE}
bike %>%
  select(atemp, temp, hum, windspeed, rentals) %>%
  map_df(~summary(.), .id = "variable") %>%
  mutate(across(-variable, round, digits = 6)) %>% knitr::kable()
```

]


.panel[

.panel-name[Code]


```{r, eval=FALSE}
bike %>%
  select(atemp,temp, hum, windspeed, rentals) %>%
  map_df(~summary(.), .id = "variable") %>%
  mutate(across(-variable, round, digits = 6))
```

]

]

---

class: middle

```{r, echo=FALSE, out.width="80%", fig.width=7, fig.height=5, fig.align='center'}

library(GGally)
library(ggplot2)

numeric_features <- bike %>% 
  select(c(atemp, temp, hum, windspeed, rentals))


# Custom function for upper panel (correlation coefficients)
upper_panel <- function(data, mapping, ...) {
  ggally_cor(data = data, mapping = mapping, ...)
}

# Custom function for lower panel (scatter plot with smooth line)
lower_panel <- function(data, mapping, ...) {
  ggally_smooth(data = data, mapping = mapping, ...)
}

# Custom function for diag panel (density plot)
diag_panel <- function(data, mapping, ...) {
  ggally_densityDiag(data = data, mapping = mapping, ...)
}

# Create the ggpairs plot
ggpairs(numeric_features, 
        upper = list(continuous = wrap(upper_panel, size = 4)), 
        lower = list(continuous = wrap(lower_panel, size = 1, color = 'blue', alpha = 0.4)),
        diag = list(continuous = wrap(diag_panel, size = 1, color = 'darkgreen')),
        legends = FALSE) +
  theme_light() +
  theme(legend.position = "none") # Remove legends to clean up the plot



```


---


class: middle

```{r, echo=FALSE, out.width="80%", fig.width=7, fig.height=5, fig.align='center'}
categorical_features <- bike %>% 
  select(c(season, mnth, holiday, weekday, workingday, weathersit, rentals, yr))
categorical_features <- categorical_features %>% 
  pivot_longer(!rentals, names_to = "features", values_to = "values") %>%
  group_by(features) %>% 
  mutate(values = factor(values))

# Plot a box plot for each feature
categorical_features %>%
  ggplot() +
  geom_boxplot(aes(x = values, y = rentals, fill = features), alpha = 0.9, show.legend = F) +
  facet_wrap(~ features, scales = 'free') +
  paletteer::scale_fill_paletteer_d("RColorBrewer::Set3") +
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_text(angle = 90))
```

---

class: middle

# Data preparation and train-test split

```{r}
set.seed(2056)
bike_select <- bike %>% 
  select(c(season, mnth, holiday, weekday, workingday, weathersit,
           temp, atemp, hum, windspeed, rentals)) %>% 
    mutate(across(1:6, factor))


bike_split <- bike_select %>% 
  initial_split(prop = 0.75, strata = holiday) # proportion stratified by holiday

bike_train <- training(bike_split)
bike_test <- testing(bike_split)
```

---

class: middle

# Model specification

```{r}

lm_spec <- # your model specification
  linear_reg() %>%  # model type
  set_engine(engine = "lm") %>%  # model engine
  set_mode("regression") # model mode
```


```{r}
# Show your model specification
lm_spec
```

---

class: middle

# Fit the model

```{r}
# Train a linear regression model
lm_mod <- lm_spec %>% 
  fit(rentals ~ ., data = bike_train)
```


.scrollable-table[
```{r}
glance(lm_mod$fit) %>% knitr::kable()
```
]


---

class: middle

# Predict on test data

```{r}
results <- bike_test %>% 
  bind_cols(lm_mod %>% 
    predict(new_data = bike_test)) %>% 
      select(c(rentals, .pred))
```

```{r}
results %>% slice_head(n =6) %>% knitr::kable()
```

---

class: middle


# Evaluate the model


```{r}
eval_metrics <- metric_set(rmse, rsq)

# Evaluate RMSE, R2 based on the results
eval_metrics(data = results,
             truth = rentals,
             estimate = .pred) %>% 
  select(-2)
```


---

class: middle


```{r, echo=FALSE, out.width="80%", fig.width=7, fig.height=5, fig.align='center'}
results %>%
  ggplot(mapping = aes(x = rentals, y = .pred)) +
  geom_point(size = 2, aes(color = "Predictions"), alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = 'darkorange', linetype = "dashed", size = 1) +
  scale_color_manual("", 
                     values = "steelblue") +
  labs(title = "Daily Bike Share Predictions",
       subtitle = "Comparison of Actual vs. Predicted Rentals",
       x = "Actual",
       y = "Predicted",
       caption = "Data source: Bike Share System") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 14),
        plot.caption = element_text(size = 8, margin = margin(t = 10)),
        legend.position = "none",
        legend.title = element_blank(),
        legend.text = element_text(size = 12),
        axis.title = element_text(face = "bold", size = 14),
        axis.text = element_text(size = 12)) 
```


---

class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Please continue working on group activity 2
]

]

`r countdown(minutes = 10, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`

