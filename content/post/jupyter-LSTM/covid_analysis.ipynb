{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Predicting covid cases with LSTM Machine Learning Model\"\n",
    "date: 2020-03-20\n",
    "tags: [\"data science\", \"machine learning\", \"hugo\"]\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various libraries and routines needed for computation\n",
    "import math \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from datetime import date, timedelta, datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"lines.color\": \"white\",\n",
    "    \"patch.edgecolor\": \"white\",\n",
    "    \"text.color\": \"black\",\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"axes.edgecolor\": \"lightgray\",\n",
    "    \"axes.labelcolor\": \"white\",\n",
    "    \"xtick.color\": \"white\",\n",
    "    \"ytick.color\": \"white\",\n",
    "    \"grid.color\": \"lightgray\",\n",
    "    \"figure.facecolor\": \"black\",\n",
    "    \"figure.edgecolor\": \"black\",\n",
    "    \"savefig.facecolor\": \"black\",\n",
    "    \"savefig.edgecolor\": \"black\"})\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('covid_final.csv')  \n",
    "dataset = df.set_index(['date'])\n",
    "dataset.drop(dataset.tail(10).index,\n",
    "        inplace = True)\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>reproduction_rate</th>\n",
       "      <th>new_tests_smoothed_per_thousand</th>\n",
       "      <th>new_vaccinations_smoothed_per_million</th>\n",
       "      <th>people_fully_vaccinated_per_hundred</th>\n",
       "      <th>total_boosters_per_hundred</th>\n",
       "      <th>stringency_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-08</th>\n",
       "      <td>38934.286</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2.748</td>\n",
       "      <td>621</td>\n",
       "      <td>65.24</td>\n",
       "      <td>28.89</td>\n",
       "      <td>53.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-09</th>\n",
       "      <td>36641.429</td>\n",
       "      <td>0.66</td>\n",
       "      <td>2.699</td>\n",
       "      <td>601</td>\n",
       "      <td>65.25</td>\n",
       "      <td>28.91</td>\n",
       "      <td>53.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-10</th>\n",
       "      <td>36330.429</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.613</td>\n",
       "      <td>583</td>\n",
       "      <td>65.27</td>\n",
       "      <td>28.94</td>\n",
       "      <td>53.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-11</th>\n",
       "      <td>36104.714</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.580</td>\n",
       "      <td>557</td>\n",
       "      <td>65.29</td>\n",
       "      <td>28.97</td>\n",
       "      <td>53.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-12</th>\n",
       "      <td>35464.143</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2.561</td>\n",
       "      <td>540</td>\n",
       "      <td>65.30</td>\n",
       "      <td>28.99</td>\n",
       "      <td>53.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            new_cases_smoothed  reproduction_rate  \\\n",
       "date                                                \n",
       "2022-03-08           38934.286               0.65   \n",
       "2022-03-09           36641.429               0.66   \n",
       "2022-03-10           36330.429               0.69   \n",
       "2022-03-11           36104.714               0.71   \n",
       "2022-03-12           35464.143               0.71   \n",
       "\n",
       "            new_tests_smoothed_per_thousand  \\\n",
       "date                                          \n",
       "2022-03-08                            2.748   \n",
       "2022-03-09                            2.699   \n",
       "2022-03-10                            2.613   \n",
       "2022-03-11                            2.580   \n",
       "2022-03-12                            2.561   \n",
       "\n",
       "            new_vaccinations_smoothed_per_million  \\\n",
       "date                                                \n",
       "2022-03-08                                    621   \n",
       "2022-03-09                                    601   \n",
       "2022-03-10                                    583   \n",
       "2022-03-11                                    557   \n",
       "2022-03-12                                    540   \n",
       "\n",
       "            people_fully_vaccinated_per_hundred  total_boosters_per_hundred  \\\n",
       "date                                                                          \n",
       "2022-03-08                                65.24                       28.89   \n",
       "2022-03-09                                65.25                       28.91   \n",
       "2022-03-10                                65.27                       28.94   \n",
       "2022-03-11                                65.29                       28.97   \n",
       "2022-03-12                                65.30                       28.99   \n",
       "\n",
       "            stringency_index  \n",
       "date                          \n",
       "2022-03-08             53.24  \n",
       "2022-03-09             53.24  \n",
       "2022-03-10             53.24  \n",
       "2022-03-11             53.24  \n",
       "2022-03-12             53.24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = dataset.copy()\n",
    "data_clean_ext = dataset.copy()\n",
    "data_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed']\n",
    "data_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows in the data\n",
    "nrows = data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to numpy values\n",
    "np_data_unscaled = np.array(data_clean)\n",
    "np_data = np.reshape(np_data_unscaled, (nrows, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all data is float\n",
    "values = values.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data by scaling each feature to a range between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "np_data_scaled = scaler.fit_transform(np_data_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate scaler that works on a single column for scaling predictions\n",
    "scaler_pred = MinMaxScaler()\n",
    "df_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed'])\n",
    "np_cases_scaled = scaler_pred.fit_transform(df_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sequence length - this is the timeframe used to make a single prediction\n",
    "sequence_length = 31\n",
    "\n",
    "# Prediction Index\n",
    "index_cases = dataset.columns.get_loc(\"new_cases_smoothed\")\n",
    "\n",
    "# Split the training data into train and train data sets\n",
    "# As a first step, we get the number of rows to train the model on 80% of the data \n",
    "train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
    "\n",
    "# Create the training and test data\n",
    "train_data = np_data_scaled[0:train_data_len, :]\n",
    "test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
    "\n",
    "# The RNN needs data with the format of [samples, time steps, features]\n",
    "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
    "def partition_dataset(sequence_length, data):\n",
    "    x, y = [], []\n",
    "    data_len = data.shape[0]\n",
    "    for i in range(sequence_length, data_len):\n",
    "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "        y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction\n",
    "    \n",
    "    # Convert the x and y to numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "# Generate training data and test data\n",
    "x_train, y_train = partition_dataset(sequence_length, train_data)\n",
    "x_test, y_test = partition_dataset(sequence_length, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure the neural network model\n",
    "model = Sequential()\n",
    "# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
    "n_neurons = x_train.shape[1] * x_train.shape[2]\n",
    "model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Compiling the LSTM\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'my_best_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             monitor='val_loss',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0)\n",
    "callbacks = [checkpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04746, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.04746\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.04746 to 0.04266, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.04266\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04266\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.04266 to 0.03714, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.03714 to 0.03641, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.03641 to 0.03282, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.03282 to 0.02978, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02978\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02978 to 0.02796, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02796 to 0.02269, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02269\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.02269\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.02269\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.02269 to 0.01654, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.01654\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01654 to 0.01532, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.01532\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.01532 to 0.01486, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.01486 to 0.01379, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.01379\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.01379\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01379 to 0.01270, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.01270\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.01270 to 0.01244, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.01244 to 0.01137, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.01137\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.01137\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.01137\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.01137\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.01137 to 0.00865, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00865 to 0.00840, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00840\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00840\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00840\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00840\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00840\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00840\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00840 to 0.00837, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00837\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00837\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00837 to 0.00746, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00746\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00746\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00746 to 0.00710, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00710\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00710 to 0.00710, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00710\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00710\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.00710 to 0.00697, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00697 to 0.00479, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00479\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00479 to 0.00463, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00463\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00463 to 0.00419, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00419\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00419 to 0.00365, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00365\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00365\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00365 to 0.00360, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00360\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.00360 to 0.00289, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00289\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00289\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00289\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00289\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.00289 to 0.00273, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00273\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00273\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.00273 to 0.00248, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00248\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.00248 to 0.00240, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00240\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00240 to 0.00181, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.00181\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.00181 to 0.00159, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.00159\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.00159 to 0.00147, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.00147\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00147 to 0.00142, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.00142\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.00142 to 0.00139, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.00139\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.00139 to 0.00115, saving model to my_best_model.hdf5\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.00115\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.00115\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 300\n",
    "batch_size = 20\n",
    "history = model.fit(x_train, y_train,\n",
    "                     batch_size=batch_size, \n",
    "                     epochs=epochs,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     callbacks = callbacks,\n",
    "                     verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_from_saved_checkpoint = load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "y_pred_scaled = model_from_saved_checkpoint.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscale the predicted values\n",
    "y_pred = scaler_pred.inverse_transform(y_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error (MAE): 16729.47\n",
      "Mean Absolute Percentage Error (MAPE): 9.34 %\n",
      "Median Absolute Percentage Error (MDAPE): 4.69 %\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
    "print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
    "\n",
    "# Median Absolute Percentage Error (MDAPE)\n",
    "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
    "print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test_unscaled, label='True')\n",
    "plt.plot(y_pred, label='LSTM')\n",
    "plt.title(\"LSTM's_Prediction\")\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Cases Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = data_clean[-sequence_length:]\n",
    "N = sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last N day closing price values and scale the data to be values between 0 and 1\n",
    "last_N_days = new_df[-sequence_length:].values\n",
    "last_N_days_scaled = scaler.transform(last_N_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list and Append past N days\n",
    "X_test_new = []\n",
    "X_test_new.append(last_N_days_scaled)\n",
    "\n",
    "# Convert the X_test data set to a numpy array and reshape the data\n",
    "pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))\n",
    "pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print last price and predicted price for the next day\n",
    "cases_today = np.round(new_df['new_cases_smoothed'][-1])\n",
    "predicted_cases = np.round(pred_cases_unscaled.ravel()[0])\n",
    "change_percent = np.round(100 - (cases_today * 100)/predicted_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The close covid cases count today is  35464.0\n",
      "The predicted case count for the next day is 50939.0 (+30.0%)\n"
     ]
    }
   ],
   "source": [
    "plus = '+'; minus = ''\n",
    "print(f'The close covid cases count today is  {cases_today}')\n",
    "print(f'The predicted case count for the next day is {predicted_cases} ({plus if change_percent > 0 else minus}{change_percent}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook covid_analysis.ipynb to markdown\n",
      "---\n",
      "title: \"Predicting covid cases with LSTM Machine Learning Model\"\n",
      "date: 2020-03-20\n",
      "tags: [\"data science\", \"machine learning\", \"hugo\"]\n",
      "draft: false\n",
      "---\n",
      "\n",
      "\n",
      "```python\n",
      "# Import various libraries and routines needed for computation\n",
      "import math \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from math import sqrt\n",
      "from numpy import concatenate\n",
      "from matplotlib import pyplot\n",
      "from pandas import read_csv\n",
      "from pandas import DataFrame\n",
      "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense, Dropout\n",
      "import keras.backend as K\n",
      "from keras.layers import LSTM\n",
      "from keras.callbacks import EarlyStopping\n",
      "from datetime import date, timedelta, datetime \n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "plt.rcParams.update({\n",
      "    \"lines.color\": \"white\",\n",
      "    \"patch.edgecolor\": \"white\",\n",
      "    \"text.color\": \"black\",\n",
      "    \"axes.facecolor\": \"white\",\n",
      "    \"axes.edgecolor\": \"lightgray\",\n",
      "    \"axes.labelcolor\": \"white\",\n",
      "    \"xtick.color\": \"white\",\n",
      "    \"ytick.color\": \"white\",\n",
      "    \"grid.color\": \"lightgray\",\n",
      "    \"figure.facecolor\": \"black\",\n",
      "    \"figure.edgecolor\": \"black\",\n",
      "    \"savefig.facecolor\": \"black\",\n",
      "    \"savefig.edgecolor\": \"black\"})\n",
      "plt.rcParams['figure.figsize'] = [10, 7]\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('covid_final.csv')  \n",
      "dataset = df.set_index(['date'])\n",
      "dataset.drop(dataset.tail(10).index,\n",
      "        inplace = True)\n",
      "values = dataset.values\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "date_index = dataset.index\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "data_clean = dataset.copy()\n",
      "data_clean_ext = dataset.copy()\n",
      "data_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed']\n",
      "data_clean.tail()\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<div>\n",
      "<style scoped>\n",
      "    .dataframe tbody tr th:only-of-type {\n",
      "        vertical-align: middle;\n",
      "    }\n",
      "\n",
      "    .dataframe tbody tr th {\n",
      "        vertical-align: top;\n",
      "    }\n",
      "\n",
      "    .dataframe thead th {\n",
      "        text-align: right;\n",
      "    }\n",
      "</style>\n",
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <thead>\n",
      "    <tr style=\"text-align: right;\">\n",
      "      <th></th>\n",
      "      <th>new_cases_smoothed</th>\n",
      "      <th>reproduction_rate</th>\n",
      "      <th>new_tests_smoothed_per_thousand</th>\n",
      "      <th>new_vaccinations_smoothed_per_million</th>\n",
      "      <th>people_fully_vaccinated_per_hundred</th>\n",
      "      <th>total_boosters_per_hundred</th>\n",
      "      <th>stringency_index</th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>date</th>\n",
      "      <th></th>\n",
      "      <th></th>\n",
      "      <th></th>\n",
      "      <th></th>\n",
      "      <th></th>\n",
      "      <th></th>\n",
      "      <th></th>\n",
      "    </tr>\n",
      "  </thead>\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <th>2022-03-08</th>\n",
      "      <td>38934.286</td>\n",
      "      <td>0.65</td>\n",
      "      <td>2.748</td>\n",
      "      <td>621</td>\n",
      "      <td>65.24</td>\n",
      "      <td>28.89</td>\n",
      "      <td>53.24</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2022-03-09</th>\n",
      "      <td>36641.429</td>\n",
      "      <td>0.66</td>\n",
      "      <td>2.699</td>\n",
      "      <td>601</td>\n",
      "      <td>65.25</td>\n",
      "      <td>28.91</td>\n",
      "      <td>53.24</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2022-03-10</th>\n",
      "      <td>36330.429</td>\n",
      "      <td>0.69</td>\n",
      "      <td>2.613</td>\n",
      "      <td>583</td>\n",
      "      <td>65.27</td>\n",
      "      <td>28.94</td>\n",
      "      <td>53.24</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2022-03-11</th>\n",
      "      <td>36104.714</td>\n",
      "      <td>0.71</td>\n",
      "      <td>2.580</td>\n",
      "      <td>557</td>\n",
      "      <td>65.29</td>\n",
      "      <td>28.97</td>\n",
      "      <td>53.24</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <th>2022-03-12</th>\n",
      "      <td>35464.143</td>\n",
      "      <td>0.71</td>\n",
      "      <td>2.561</td>\n",
      "      <td>540</td>\n",
      "      <td>65.30</td>\n",
      "      <td>28.99</td>\n",
      "      <td>53.24</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "# number of rows in the data\n",
      "nrows = data_clean.shape[0]\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Convert the data to numpy values\n",
      "np_data_unscaled = np.array(data_clean)\n",
      "np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# ensure all data is float\n",
      "values = values.astype('float64')\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Transform the data by scaling each feature to a range between 0 and 1\n",
      "scaler = MinMaxScaler()\n",
      "np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Creating a separate scaler that works on a single column for scaling predictions\n",
      "scaler_pred = MinMaxScaler()\n",
      "df_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed'])\n",
      "np_cases_scaled = scaler_pred.fit_transform(df_cases)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Set the sequence length - this is the timeframe used to make a single prediction\n",
      "sequence_length = 31\n",
      "\n",
      "# Prediction Index\n",
      "index_cases = dataset.columns.get_loc(\"new_cases_smoothed\")\n",
      "\n",
      "# Split the training data into train and train data sets\n",
      "# As a first step, we get the number of rows to train the model on 80% of the data \n",
      "train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
      "\n",
      "# Create the training and test data\n",
      "train_data = np_data_scaled[0:train_data_len, :]\n",
      "test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
      "\n",
      "# The RNN needs data with the format of [samples, time steps, features]\n",
      "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
      "def partition_dataset(sequence_length, data):\n",
      "    x, y = [], []\n",
      "    data_len = data.shape[0]\n",
      "    for i in range(sequence_length, data_len):\n",
      "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
      "        y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction\n",
      "    \n",
      "    # Convert the x and y to numpy arrays\n",
      "    x = np.array(x)\n",
      "    y = np.array(y)\n",
      "    return x, y\n",
      "\n",
      "# Generate training data and test data\n",
      "x_train, y_train = partition_dataset(sequence_length, train_data)\n",
      "x_test, y_test = partition_dataset(sequence_length, test_data)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "\n",
      "# Configure the neural network model\n",
      "model = Sequential()\n",
      "# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
      "n_neurons = x_train.shape[1] * x_train.shape[2]\n",
      "model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
      "model.add(Dense(1))\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
      "# Compiling the LSTM\n",
      "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "checkpoint_path = 'my_best_model.hdf5'\n",
      "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
      "                             monitor='val_loss',\n",
      "                             verbose=1, \n",
      "                             save_best_only=True,\n",
      "                             mode='min')\n",
      "\n",
      "earlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0)\n",
      "callbacks = [checkpoint, earlystopping]\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Training the model\n",
      "epochs = 300\n",
      "batch_size = 20\n",
      "history = model.fit(x_train, y_train,\n",
      "                     batch_size=batch_size, \n",
      "                     epochs=epochs,\n",
      "                     validation_data=(x_test, y_test),\n",
      "                     callbacks = callbacks,\n",
      "                     verbose = 0)\n",
      "```\n",
      "\n",
      "    \n",
      "    Epoch 00001: val_loss improved from inf to 0.04746, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00002: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00003: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00004: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00005: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00006: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00007: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00008: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00009: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00010: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00011: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00012: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00013: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00014: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00015: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00016: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00017: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00018: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00019: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00020: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00021: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00022: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00023: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00024: val_loss did not improve from 0.04746\n",
      "    \n",
      "    Epoch 00025: val_loss improved from 0.04746 to 0.04266, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00026: val_loss did not improve from 0.04266\n",
      "    \n",
      "    Epoch 00027: val_loss did not improve from 0.04266\n",
      "    \n",
      "    Epoch 00028: val_loss improved from 0.04266 to 0.03714, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00029: val_loss improved from 0.03714 to 0.03641, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00030: val_loss improved from 0.03641 to 0.03282, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00031: val_loss improved from 0.03282 to 0.02978, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00032: val_loss did not improve from 0.02978\n",
      "    \n",
      "    Epoch 00033: val_loss improved from 0.02978 to 0.02796, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00034: val_loss improved from 0.02796 to 0.02269, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00035: val_loss did not improve from 0.02269\n",
      "    \n",
      "    Epoch 00036: val_loss did not improve from 0.02269\n",
      "    \n",
      "    Epoch 00037: val_loss did not improve from 0.02269\n",
      "    \n",
      "    Epoch 00038: val_loss improved from 0.02269 to 0.01654, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00039: val_loss did not improve from 0.01654\n",
      "    \n",
      "    Epoch 00040: val_loss improved from 0.01654 to 0.01532, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00041: val_loss did not improve from 0.01532\n",
      "    \n",
      "    Epoch 00042: val_loss improved from 0.01532 to 0.01486, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00043: val_loss improved from 0.01486 to 0.01379, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00044: val_loss did not improve from 0.01379\n",
      "    \n",
      "    Epoch 00045: val_loss did not improve from 0.01379\n",
      "    \n",
      "    Epoch 00046: val_loss improved from 0.01379 to 0.01270, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00047: val_loss did not improve from 0.01270\n",
      "    \n",
      "    Epoch 00048: val_loss improved from 0.01270 to 0.01244, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00049: val_loss improved from 0.01244 to 0.01137, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00050: val_loss did not improve from 0.01137\n",
      "    \n",
      "    Epoch 00051: val_loss did not improve from 0.01137\n",
      "    \n",
      "    Epoch 00052: val_loss did not improve from 0.01137\n",
      "    \n",
      "    Epoch 00053: val_loss did not improve from 0.01137\n",
      "    \n",
      "    Epoch 00054: val_loss improved from 0.01137 to 0.00865, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00055: val_loss improved from 0.00865 to 0.00840, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00056: val_loss did not improve from 0.00840\n",
      "    \n",
      "    Epoch 00057: val_loss did not improve from 0.00840\n",
      "    \n",
      "    Epoch 00058: val_loss did not improve from 0.00840\n",
      "    \n",
      "    Epoch 00059: val_loss did not improve from 0.00840\n",
      "    \n",
      "    Epoch 00060: val_loss did not improve from 0.00840\n",
      "    \n",
      "    Epoch 00061: val_loss did not improve from 0.00840\n",
      "    \n",
      "    Epoch 00062: val_loss improved from 0.00840 to 0.00837, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00063: val_loss did not improve from 0.00837\n",
      "    \n",
      "    Epoch 00064: val_loss did not improve from 0.00837\n",
      "    \n",
      "    Epoch 00065: val_loss improved from 0.00837 to 0.00746, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00066: val_loss did not improve from 0.00746\n",
      "    \n",
      "    Epoch 00067: val_loss did not improve from 0.00746\n",
      "    \n",
      "    Epoch 00068: val_loss improved from 0.00746 to 0.00710, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00069: val_loss did not improve from 0.00710\n",
      "    \n",
      "    Epoch 00070: val_loss improved from 0.00710 to 0.00710, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00071: val_loss did not improve from 0.00710\n",
      "    \n",
      "    Epoch 00072: val_loss did not improve from 0.00710\n",
      "    \n",
      "    Epoch 00073: val_loss improved from 0.00710 to 0.00697, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00074: val_loss improved from 0.00697 to 0.00479, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00075: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00076: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00077: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00078: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00079: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00080: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00081: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00082: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00083: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00084: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00085: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00086: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00087: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00088: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00089: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00090: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00091: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00092: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00093: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00094: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00095: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00096: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00097: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00098: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00099: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00100: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00101: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00102: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00103: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00104: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00105: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00106: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00107: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00108: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00109: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00110: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00111: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00112: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00113: val_loss did not improve from 0.00479\n",
      "    \n",
      "    Epoch 00114: val_loss improved from 0.00479 to 0.00463, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00115: val_loss did not improve from 0.00463\n",
      "    \n",
      "    Epoch 00116: val_loss improved from 0.00463 to 0.00419, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00117: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00118: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00119: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00120: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00121: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00122: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00123: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00124: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00125: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00126: val_loss did not improve from 0.00419\n",
      "    \n",
      "    Epoch 00127: val_loss improved from 0.00419 to 0.00365, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00128: val_loss did not improve from 0.00365\n",
      "    \n",
      "    Epoch 00129: val_loss did not improve from 0.00365\n",
      "    \n",
      "    Epoch 00130: val_loss improved from 0.00365 to 0.00360, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00131: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00132: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00133: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00134: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00135: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00136: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00137: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00138: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00139: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00140: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00141: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00142: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00143: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00144: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00145: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00146: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00147: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00148: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00149: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00150: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00151: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00152: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00153: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00154: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00155: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00156: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00157: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00158: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00159: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00160: val_loss did not improve from 0.00360\n",
      "    \n",
      "    Epoch 00161: val_loss improved from 0.00360 to 0.00289, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00162: val_loss did not improve from 0.00289\n",
      "    \n",
      "    Epoch 00163: val_loss did not improve from 0.00289\n",
      "    \n",
      "    Epoch 00164: val_loss did not improve from 0.00289\n",
      "    \n",
      "    Epoch 00165: val_loss did not improve from 0.00289\n",
      "    \n",
      "    Epoch 00166: val_loss improved from 0.00289 to 0.00273, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00167: val_loss did not improve from 0.00273\n",
      "    \n",
      "    Epoch 00168: val_loss did not improve from 0.00273\n",
      "    \n",
      "    Epoch 00169: val_loss improved from 0.00273 to 0.00248, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00170: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00171: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00172: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00173: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00174: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00175: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00176: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00177: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00178: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00179: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00180: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00181: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00182: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00183: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00184: val_loss did not improve from 0.00248\n",
      "    \n",
      "    Epoch 00185: val_loss improved from 0.00248 to 0.00240, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00186: val_loss did not improve from 0.00240\n",
      "    \n",
      "    Epoch 00187: val_loss did not improve from 0.00240\n",
      "    \n",
      "    Epoch 00188: val_loss did not improve from 0.00240\n",
      "    \n",
      "    Epoch 00189: val_loss did not improve from 0.00240\n",
      "    \n",
      "    Epoch 00190: val_loss did not improve from 0.00240\n",
      "    \n",
      "    Epoch 00191: val_loss improved from 0.00240 to 0.00181, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00192: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00193: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00194: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00195: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00196: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00197: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00198: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00199: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00200: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00201: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00202: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00203: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00204: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00205: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00206: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00207: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00208: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00209: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00210: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00211: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00212: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00213: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00214: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00215: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00216: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00217: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00218: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00219: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00220: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00221: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00222: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00223: val_loss did not improve from 0.00181\n",
      "    \n",
      "    Epoch 00224: val_loss improved from 0.00181 to 0.00159, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00225: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00226: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00227: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00228: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00229: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00230: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00231: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00232: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00233: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00234: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00235: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00236: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00237: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00238: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00239: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00240: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00241: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00242: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00243: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00244: val_loss did not improve from 0.00159\n",
      "    \n",
      "    Epoch 00245: val_loss improved from 0.00159 to 0.00147, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00246: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00247: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00248: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00249: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00250: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00251: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00252: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00253: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00254: val_loss did not improve from 0.00147\n",
      "    \n",
      "    Epoch 00255: val_loss improved from 0.00147 to 0.00142, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00256: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00257: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00258: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00259: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00260: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00261: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00262: val_loss did not improve from 0.00142\n",
      "    \n",
      "    Epoch 00263: val_loss improved from 0.00142 to 0.00139, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00264: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00265: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00266: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00267: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00268: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00269: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00270: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00271: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00272: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00273: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00274: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00275: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00276: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00277: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00278: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00279: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00280: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00281: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00282: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00283: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00284: val_loss did not improve from 0.00139\n",
      "    \n",
      "    Epoch 00285: val_loss improved from 0.00139 to 0.00115, saving model to my_best_model.hdf5\n",
      "    \n",
      "    Epoch 00286: val_loss did not improve from 0.00115\n",
      "    \n",
      "    Epoch 00287: val_loss did not improve from 0.00115\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "from tensorflow.keras.models import load_model\n",
      "model_from_saved_checkpoint = load_model(checkpoint_path)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Plot training & validation loss values\n",
      "plt.figure(figsize=(16,7))\n",
      "plt.plot(history.history['loss'], label='train')\n",
      "plt.plot(history.history['val_loss'], label='test')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "\n",
      "    ---------------------------------------------------------------------------\n",
      "\n",
      "    NameError                                 Traceback (most recent call last)\n",
      "\n",
      "    <ipython-input-18-fdc138693c48> in <module>()\n",
      "          1 # Plot training & validation loss values\n",
      "          2 plt.figure(figsize=(16,7))\n",
      "    ----> 3 plt.plot(history.history['loss'], label='train')\n",
      "          4 plt.plot(history.history['val_loss'], label='test')\n",
      "          5 plt.legend()\n",
      "\n",
      "\n",
      "    NameError: name 'history' is not defined\n",
      "\n",
      "\n",
      "\n",
      "    <Figure size 1152x504 with 0 Axes>\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "# Get the predicted values\n",
      "y_pred_scaled = model_from_saved_checkpoint.predict(x_test)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Unscale the predicted values\n",
      "y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Mean Absolute Error (MAE)\n",
      "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
      "print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
      "\n",
      "# Mean Absolute Percentage Error (MAPE)\n",
      "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
      "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
      "\n",
      "# Median Absolute Percentage Error (MDAPE)\n",
      "MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
      "print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')\n",
      "```\n",
      "\n",
      "    Median Absolute Error (MAE): 26045.48\n",
      "    Mean Absolute Percentage Error (MAPE): 18.19 %\n",
      "    Median Absolute Percentage Error (MDAPE): 14.58 %\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "plt.plot(y_test_unscaled, label='True')\n",
      "plt.plot(y_pred, label='LSTM')\n",
      "plt.title(\"LSTM's_Prediction\")\n",
      "plt.xlabel('Observation')\n",
      "plt.ylabel('Cases Prediction')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "\n",
      "    \n",
      "![png](./covid_analysis_22_0.png)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "new_df = data_clean[-sequence_length:]\n",
      "N = sequence_length\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Get the last N day closing price values and scale the data to be values between 0 and 1\n",
      "last_N_days = new_df[-sequence_length:].values\n",
      "last_N_days_scaled = scaler.transform(last_N_days)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Create an empty list and Append past N days\n",
      "X_test_new = []\n",
      "X_test_new.append(last_N_days_scaled)\n",
      "\n",
      "# Convert the X_test data set to a numpy array and reshape the data\n",
      "pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))\n",
      "pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "# Print last price and predicted price for the next day\n",
      "cases_today = np.round(new_df['new_cases_smoothed'][-1])\n",
      "predicted_cases = np.round(pred_cases_unscaled.ravel()[0])\n",
      "change_percent = np.round(100 - (cases_today * 100)/predicted_cases)\n",
      "```\n",
      "\n",
      "\n",
      "```python\n",
      "plus = '+'; minus = ''\n",
      "print(f'The close covid cases count today is  {cases_today}')\n",
      "print(f'The predicted case count for the next day is {predicted_cases} ({plus if change_percent > 0 else minus}{change_percent}%)')\n",
      "```\n",
      "\n",
      "    The close covid cases count today is  35464.0\n",
      "    The predicted case count for the next day is 2843.0 (-1147.0%)\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "!jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=.\n",
      "!cat covid_analysis.md | tee -a index.md\n",
      "!rm covid_analysis.md\n",
      "```\n",
      "\n",
      "    [NbConvertApp] Converting notebook covid_analysis.ipynb to markdown\n",
      "    ---\n",
      "    title: \"Predicting covid cases with LSTM Machine Learning Model\"\n",
      "    date: 2020-03-20\n",
      "    tags: [\"data science\", \"machine learning\", \"hugo\"]\n",
      "    draft: false\n",
      "    ---\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Import various libraries and routines needed for computation\n",
      "    import math \n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    import tensorflow as tf\n",
      "    %matplotlib inline\n",
      "    import matplotlib.pyplot as plt\n",
      "    from math import sqrt\n",
      "    from numpy import concatenate\n",
      "    from matplotlib import pyplot\n",
      "    from pandas import read_csv\n",
      "    from pandas import DataFrame\n",
      "    from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
      "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
      "    from keras.models import Sequential\n",
      "    from keras.layers import Dense, Dropout\n",
      "    import keras.backend as K\n",
      "    from keras.layers import LSTM\n",
      "    from keras.callbacks import EarlyStopping\n",
      "    from datetime import date, timedelta, datetime \n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    df = pd.read_csv('covid_final.csv')  \n",
      "    dataset = df.set_index(['date'])\n",
      "    dataset.drop(dataset.tail(10).index,\n",
      "            inplace = True)\n",
      "    values = dataset.values\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    date_index = dataset.index\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    data_clean = dataset.copy()\n",
      "    data_clean_ext = dataset.copy()\n",
      "    data_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed']\n",
      "    data_clean.tail()\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    <div>\n",
      "    <style scoped>\n",
      "        .dataframe tbody tr th:only-of-type {\n",
      "            vertical-align: middle;\n",
      "        }\n",
      "    \n",
      "        .dataframe tbody tr th {\n",
      "            vertical-align: top;\n",
      "        }\n",
      "    \n",
      "        .dataframe thead th {\n",
      "            text-align: right;\n",
      "        }\n",
      "    </style>\n",
      "    <table border=\"1\" class=\"dataframe\">\n",
      "      <thead>\n",
      "        <tr style=\"text-align: right;\">\n",
      "          <th></th>\n",
      "          <th>new_cases_smoothed</th>\n",
      "          <th>reproduction_rate</th>\n",
      "          <th>new_tests_smoothed_per_thousand</th>\n",
      "          <th>new_vaccinations_smoothed_per_million</th>\n",
      "          <th>people_fully_vaccinated_per_hundred</th>\n",
      "          <th>total_boosters_per_hundred</th>\n",
      "          <th>stringency_index</th>\n",
      "        </tr>\n",
      "        <tr>\n",
      "          <th>date</th>\n",
      "          <th></th>\n",
      "          <th></th>\n",
      "          <th></th>\n",
      "          <th></th>\n",
      "          <th></th>\n",
      "          <th></th>\n",
      "          <th></th>\n",
      "        </tr>\n",
      "      </thead>\n",
      "      <tbody>\n",
      "        <tr>\n",
      "          <th>2022-03-08</th>\n",
      "          <td>38934.286</td>\n",
      "          <td>0.65</td>\n",
      "          <td>2.748</td>\n",
      "          <td>621</td>\n",
      "          <td>65.24</td>\n",
      "          <td>28.89</td>\n",
      "          <td>53.24</td>\n",
      "        </tr>\n",
      "        <tr>\n",
      "          <th>2022-03-09</th>\n",
      "          <td>36641.429</td>\n",
      "          <td>0.66</td>\n",
      "          <td>2.699</td>\n",
      "          <td>601</td>\n",
      "          <td>65.25</td>\n",
      "          <td>28.91</td>\n",
      "          <td>53.24</td>\n",
      "        </tr>\n",
      "        <tr>\n",
      "          <th>2022-03-10</th>\n",
      "          <td>36330.429</td>\n",
      "          <td>0.69</td>\n",
      "          <td>2.613</td>\n",
      "          <td>583</td>\n",
      "          <td>65.27</td>\n",
      "          <td>28.94</td>\n",
      "          <td>53.24</td>\n",
      "        </tr>\n",
      "        <tr>\n",
      "          <th>2022-03-11</th>\n",
      "          <td>36104.714</td>\n",
      "          <td>0.71</td>\n",
      "          <td>2.580</td>\n",
      "          <td>557</td>\n",
      "          <td>65.29</td>\n",
      "          <td>28.97</td>\n",
      "          <td>53.24</td>\n",
      "        </tr>\n",
      "        <tr>\n",
      "          <th>2022-03-12</th>\n",
      "          <td>35464.143</td>\n",
      "          <td>0.71</td>\n",
      "          <td>2.561</td>\n",
      "          <td>540</td>\n",
      "          <td>65.30</td>\n",
      "          <td>28.99</td>\n",
      "          <td>53.24</td>\n",
      "        </tr>\n",
      "      </tbody>\n",
      "    </table>\n",
      "    </div>\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # number of rows in the data\n",
      "    nrows = data_clean.shape[0]\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Convert the data to numpy values\n",
      "    np_data_unscaled = np.array(data_clean)\n",
      "    np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # ensure all data is float\n",
      "    values = values.astype('float64')\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Transform the data by scaling each feature to a range between 0 and 1\n",
      "    scaler = MinMaxScaler()\n",
      "    np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Creating a separate scaler that works on a single column for scaling predictions\n",
      "    scaler_pred = MinMaxScaler()\n",
      "    df_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed'])\n",
      "    np_cases_scaled = scaler_pred.fit_transform(df_cases)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Set the sequence length - this is the timeframe used to make a single prediction\n",
      "    sequence_length = 31\n",
      "    \n",
      "    # Prediction Index\n",
      "    index_cases = dataset.columns.get_loc(\"new_cases_smoothed\")\n",
      "    \n",
      "    # Split the training data into train and train data sets\n",
      "    # As a first step, we get the number of rows to train the model on 80% of the data \n",
      "    train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
      "    \n",
      "    # Create the training and test data\n",
      "    train_data = np_data_scaled[0:train_data_len, :]\n",
      "    test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
      "    \n",
      "    # The RNN needs data with the format of [samples, time steps, features]\n",
      "    # Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
      "    def partition_dataset(sequence_length, data):\n",
      "        x, y = [], []\n",
      "        data_len = data.shape[0]\n",
      "        for i in range(sequence_length, data_len):\n",
      "            x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
      "            y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction\n",
      "        \n",
      "        # Convert the x and y to numpy arrays\n",
      "        x = np.array(x)\n",
      "        y = np.array(y)\n",
      "        return x, y\n",
      "    \n",
      "    # Generate training data and test data\n",
      "    x_train, y_train = partition_dataset(sequence_length, train_data)\n",
      "    x_test, y_test = partition_dataset(sequence_length, test_data)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    \n",
      "    # Configure the neural network model\n",
      "    model = Sequential()\n",
      "    # Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
      "    n_neurons = x_train.shape[1] * x_train.shape[2]\n",
      "    model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
      "    model.add(Dense(1))\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
      "    # Compiling the LSTM\n",
      "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    checkpoint_path = 'my_best_model.hdf5'\n",
      "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
      "                                 monitor='val_loss',\n",
      "                                 verbose=1, \n",
      "                                 save_best_only=True,\n",
      "                                 mode='min')\n",
      "    \n",
      "    earlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0)\n",
      "    callbacks = [checkpoint, earlystopping]\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Training the model\n",
      "    epochs = 300\n",
      "    batch_size = 20\n",
      "    history = model.fit(x_train, y_train,\n",
      "                         batch_size=batch_size, \n",
      "                         epochs=epochs,\n",
      "                         validation_data=(x_test, y_test),\n",
      "                         callbacks = callbacks,\n",
      "                         verbose = 0)\n",
      "    ```\n",
      "    \n",
      "        \n",
      "        Epoch 00001: val_loss improved from inf to 0.03988, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00002: val_loss improved from 0.03988 to 0.03568, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00003: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00004: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00005: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00006: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00007: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00008: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00009: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00010: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00011: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00012: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00013: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00014: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00015: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00016: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00017: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00018: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00019: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00020: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00021: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00022: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00023: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00024: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00025: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00026: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00027: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00028: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00029: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00030: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00031: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00032: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00033: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00034: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00035: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00036: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00037: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00038: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00039: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00040: val_loss did not improve from 0.03568\n",
      "        \n",
      "        Epoch 00041: val_loss improved from 0.03568 to 0.03540, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00042: val_loss improved from 0.03540 to 0.03177, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00043: val_loss improved from 0.03177 to 0.02654, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00044: val_loss did not improve from 0.02654\n",
      "        \n",
      "        Epoch 00045: val_loss did not improve from 0.02654\n",
      "        \n",
      "        Epoch 00046: val_loss improved from 0.02654 to 0.02444, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00047: val_loss did not improve from 0.02444\n",
      "        \n",
      "        Epoch 00048: val_loss improved from 0.02444 to 0.02441, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00049: val_loss improved from 0.02441 to 0.02097, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00050: val_loss did not improve from 0.02097\n",
      "        \n",
      "        Epoch 00051: val_loss did not improve from 0.02097\n",
      "        \n",
      "        Epoch 00052: val_loss did not improve from 0.02097\n",
      "        \n",
      "        Epoch 00053: val_loss did not improve from 0.02097\n",
      "        \n",
      "        Epoch 00054: val_loss did not improve from 0.02097\n",
      "        \n",
      "        Epoch 00055: val_loss did not improve from 0.02097\n",
      "        \n",
      "        Epoch 00056: val_loss improved from 0.02097 to 0.02015, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00057: val_loss did not improve from 0.02015\n",
      "        \n",
      "        Epoch 00058: val_loss did not improve from 0.02015\n",
      "        \n",
      "        Epoch 00059: val_loss did not improve from 0.02015\n",
      "        \n",
      "        Epoch 00060: val_loss improved from 0.02015 to 0.01943, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00061: val_loss improved from 0.01943 to 0.01820, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00062: val_loss improved from 0.01820 to 0.01687, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00063: val_loss improved from 0.01687 to 0.01529, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00064: val_loss did not improve from 0.01529\n",
      "        \n",
      "        Epoch 00065: val_loss did not improve from 0.01529\n",
      "        \n",
      "        Epoch 00066: val_loss did not improve from 0.01529\n",
      "        \n",
      "        Epoch 00067: val_loss did not improve from 0.01529\n",
      "        \n",
      "        Epoch 00068: val_loss did not improve from 0.01529\n",
      "        \n",
      "        Epoch 00069: val_loss did not improve from 0.01529\n",
      "        \n",
      "        Epoch 00070: val_loss improved from 0.01529 to 0.01523, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00071: val_loss improved from 0.01523 to 0.01438, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00072: val_loss improved from 0.01438 to 0.01253, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00073: val_loss did not improve from 0.01253\n",
      "        \n",
      "        Epoch 00074: val_loss did not improve from 0.01253\n",
      "        \n",
      "        Epoch 00075: val_loss did not improve from 0.01253\n",
      "        \n",
      "        Epoch 00076: val_loss did not improve from 0.01253\n",
      "        \n",
      "        Epoch 00077: val_loss did not improve from 0.01253\n",
      "        \n",
      "        Epoch 00078: val_loss did not improve from 0.01253\n",
      "        \n",
      "        Epoch 00079: val_loss improved from 0.01253 to 0.00920, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00080: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00081: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00082: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00083: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00084: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00085: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00086: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00087: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00088: val_loss did not improve from 0.00920\n",
      "        \n",
      "        Epoch 00089: val_loss improved from 0.00920 to 0.00801, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00090: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00091: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00092: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00093: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00094: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00095: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00096: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00097: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00098: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00099: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00100: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00101: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00102: val_loss did not improve from 0.00801\n",
      "        \n",
      "        Epoch 00103: val_loss improved from 0.00801 to 0.00780, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00104: val_loss did not improve from 0.00780\n",
      "        \n",
      "        Epoch 00105: val_loss did not improve from 0.00780\n",
      "        \n",
      "        Epoch 00106: val_loss did not improve from 0.00780\n",
      "        \n",
      "        Epoch 00107: val_loss did not improve from 0.00780\n",
      "        \n",
      "        Epoch 00108: val_loss did not improve from 0.00780\n",
      "        \n",
      "        Epoch 00109: val_loss did not improve from 0.00780\n",
      "        \n",
      "        Epoch 00110: val_loss improved from 0.00780 to 0.00678, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00111: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00112: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00113: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00114: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00115: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00116: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00117: val_loss did not improve from 0.00678\n",
      "        \n",
      "        Epoch 00118: val_loss improved from 0.00678 to 0.00657, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00119: val_loss did not improve from 0.00657\n",
      "        \n",
      "        Epoch 00120: val_loss did not improve from 0.00657\n",
      "        \n",
      "        Epoch 00121: val_loss did not improve from 0.00657\n",
      "        \n",
      "        Epoch 00122: val_loss did not improve from 0.00657\n",
      "        \n",
      "        Epoch 00123: val_loss did not improve from 0.00657\n",
      "        \n",
      "        Epoch 00124: val_loss did not improve from 0.00657\n",
      "        \n",
      "        Epoch 00125: val_loss improved from 0.00657 to 0.00530, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00126: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00127: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00128: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00129: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00130: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00131: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00132: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00133: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00134: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00135: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00136: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00137: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00138: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00139: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00140: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00141: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00142: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00143: val_loss did not improve from 0.00530\n",
      "        \n",
      "        Epoch 00144: val_loss improved from 0.00530 to 0.00444, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00145: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00146: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00147: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00148: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00149: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00150: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00151: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00152: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00153: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00154: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00155: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00156: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00157: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00158: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00159: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00160: val_loss did not improve from 0.00444\n",
      "        \n",
      "        Epoch 00161: val_loss improved from 0.00444 to 0.00436, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00162: val_loss did not improve from 0.00436\n",
      "        \n",
      "        Epoch 00163: val_loss did not improve from 0.00436\n",
      "        \n",
      "        Epoch 00164: val_loss did not improve from 0.00436\n",
      "        \n",
      "        Epoch 00165: val_loss improved from 0.00436 to 0.00359, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00166: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00167: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00168: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00169: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00170: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00171: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00172: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00173: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00174: val_loss did not improve from 0.00359\n",
      "        \n",
      "        Epoch 00175: val_loss improved from 0.00359 to 0.00298, saving model to my_best_model.hdf5\n",
      "        \n",
      "        Epoch 00176: val_loss did not improve from 0.00298\n",
      "        \n",
      "        Epoch 00177: val_loss did not improve from 0.00298\n",
      "        \n",
      "        Epoch 00178: val_loss did not improve from 0.00298\n",
      "        \n",
      "        Epoch 00179: val_loss did not improve from 0.00298\n",
      "        \n",
      "        Epoch 00180: val_loss did not improve from 0.00298\n",
      "    \n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    from tensorflow.keras.models import load_model\n",
      "    model_from_saved_checkpoint = load_model(checkpoint_path)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Plot training & validation loss values\n",
      "    plt.figure(figsize=(16,7))\n",
      "    plt.plot(history.history['loss'], label='train')\n",
      "    plt.plot(history.history['val_loss'], label='test')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Get the predicted values\n",
      "    y_pred_scaled = model_from_saved_checkpoint.predict(x_test)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Unscale the predicted values\n",
      "    y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Mean Absolute Error (MAE)\n",
      "    MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
      "    print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
      "    \n",
      "    # Mean Absolute Percentage Error (MAPE)\n",
      "    MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
      "    print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
      "    \n",
      "    # Median Absolute Percentage Error (MDAPE)\n",
      "    MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
      "    print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    plt.plot(y_test_unscaled, label='True')\n",
      "    plt.plot(y_pred, label='LSTM')\n",
      "    plt.title(\"LSTM's_Prediction\")\n",
      "    plt.xlabel('Observation')\n",
      "    plt.ylabel('Cases Prediction')\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    new_df = data_clean[-sequence_length:]\n",
      "    N = sequence_length\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Get the last N day closing price values and scale the data to be values between 0 and 1\n",
      "    last_N_days = new_df[-sequence_length:].values\n",
      "    last_N_days_scaled = scaler.transform(last_N_days)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Create an empty list and Append past N days\n",
      "    X_test_new = []\n",
      "    X_test_new.append(last_N_days_scaled)\n",
      "    \n",
      "    # Convert the X_test data set to a numpy array and reshape the data\n",
      "    pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))\n",
      "    pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    # Print last price and predicted price for the next day\n",
      "    cases_today = np.round(new_df['new_cases_smoothed'][-1])\n",
      "    predicted_cases = np.round(pred_cases_unscaled.ravel()[0])\n",
      "    change_percent = np.round(100 - (cases_today * 100)/predicted_cases)\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    plus = '+'; minus = ''\n",
      "    print(f'The close covid cases count today is  {cases_today}')\n",
      "    print(f'The predicted case count for the next day is {predicted_cases} ({plus if change_percent > 0 else minus}{change_percent}%)')\n",
      "    ```\n",
      "    \n",
      "    \n",
      "    ```python\n",
      "    !jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=.\n",
      "    !cat covid_analysis.md | tee -a index.md\n",
      "    !rm covid_analysis.md\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=.\n",
    "!cat covid_analysis.md | tee -a index.md\n",
    "!rm covid_analysis.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
