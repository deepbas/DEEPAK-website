---
title: "Clustering Analysis"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      collapse = TRUE, 
                      comment=NA, 
                      warning = FALSE,
                      message = FALSE,
                      fig.height = 4, fig.width = 6, fig.align='center')

library(tidyverse) 
library(parsnip)
library(ggthemes)
library(factoextra)
library(broom)
```


# Your Turn 1

```{r}
set.seed(2343)

my_df <- tibble(
  X1 = rnorm(n = 50, mean = rep(c(1, 3), each = 25)),
  X2 = rnorm(n = 50, mean = rep(c(1.3, 1), each = 25))
)

my_df %>%
  ggplot(aes(X1, X2)) +
  geom_point() + theme_tufte() 
```


a. How many clusters can you identify in the data?

*Answer:* 

b. Fit `kmeans` algorithm to the data picking the number of clusters you previously identified in part a.

```{r}
set.seed(1234)
res_kmeans <-
```

c.  Use `augment()` from the `broom` package to add the cluster association to the points in the data and plot these cluster association, color-coded.

```{r}
augment(res_kmeans, data = my_df) %>%
  ggplot(aes()) +
  geom_point()
```

d. Repeat parts b-c for identifying more number of clusters than what you picked in part a.

```{r}
set.seed(1234)
res_kmeans <- 
```


```{r}
augment(res_kmeans, data = my_df) %>%
  ggplot(aes()) +
  geom_point()
```

e. Aggregate the total within sum of squares for each k to the data table `multi_kmeans`.

```{r}
multi_kmeans <- tibble(k = ) %>%
  mutate(
    model = purrr::map(k, ~kmeans()),
    tot.withinss = purrr::map_dbl(model, ~ glance(.x)$tot.withinss)
  )
```


f. Make an elbow plot modifying the code below:

```{r}
multi_kmeans %>%
  ggplot(aes()) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:15) +
  theme_minimal()
```


g. After picking an optimal number of cluster, use the in-built function in `factoextra` package to construct the final cluster plot.


```{r}
set.seed(1234)
kmeans.final <- kmeans(my_df, 5, nstart = 25)
fviz_cluster(kmeans.final, data = my_df)
```


# Your Turn 2

Use the same dataset as your turn 1. Use the package `factoextra` to complete this activity

```{r}
#install.packages("factoextra")
library(factoextra)
```

a. Find the euclidean distance between X1 and X2 in the dataset `my_df`. This will be stored in a matrix with each entries denoting the euclidean distance between the corresponding states.

```{r}

```


```{r}
distance <- 
```


b. Set-up the hierarchical clustering object that we will use to construct a dendogram

```{r}

```



c. Cut the tree in two groups

```{r}
sub_grp <- 

```


d. Make a data tibble adding the cluster information to a new column called cluster.

```{r}
my_df %>% 
  mutate(cluster = ) 
```
e. Finally, plot the dendogram.

```{r}
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 3, border = 2:5)   # Number of clusters
```

