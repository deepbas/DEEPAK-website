<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deepak Bastola</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Deepak Bastola</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Deepak Bastola</copyright><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Deepak Bastola</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Week 10</title>
      <link>/courses/stat120/week10/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week10/</guid>
      <description>&lt;!--

Day 26 Slides (&lt;a href=&#34;/stat120/Day26.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 27 Midterm 3

--&gt;
</description>
    </item>
    
    <item>
      <title>Week 10</title>
      <link>/courses/stat220/week10/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week10/</guid>
      <description>&lt;!--

Day 26 In-class Midterm 2

Day 27 Slides (&lt;a href=&#34;/stat220/Day27.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


--&gt;</description>
    </item>
    
    <item>
      <title>Week 9</title>
      <link>/courses/stat120/week9/</link>
      <pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week9/</guid>
      <description>&lt;!--

Day 23 Slides (&lt;a href=&#34;/stat120/Day23.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 24 Slides (&lt;a href=&#34;/stat120/Day24.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 25 Slides (&lt;a href=&#34;/stat120/Day25.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 9</title>
      <link>/courses/stat220/week9/</link>
      <pubDate>Sun, 25 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week9/</guid>
      <description>&lt;!--

Day 23 Slides (&lt;a href=&#34;/stat220/Day23.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 24 Slides (&lt;a href=&#34;/stat220/Day24.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 25 Slides (&lt;a href=&#34;/stat220/Day25.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 8</title>
      <link>/courses/stat120/week8/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week8/</guid>
      <description>&lt;!--

Day 20: In-class Midterm!

Day 21 Slides (&lt;a href=&#34;/stat120/Day21.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 22 Slides (&lt;a href=&#34;/stat120/Day22.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 8</title>
      <link>/courses/stat220/week8/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week8/</guid>
      <description>&lt;!--

Day 20 Slides (&lt;a href=&#34;/stat220/Day20.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 21 Slides (&lt;a href=&#34;/stat220/Day21.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 22 Slides (&lt;a href=&#34;/stat220/Day22.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 7</title>
      <link>/courses/stat120/week7/</link>
      <pubDate>Sun, 11 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week7/</guid>
      <description>&lt;!--

Day 17 Slides (&lt;a href=&#34;/stat120/Day17.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 18 Slides (&lt;a href=&#34;/stat120/Day18.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 19 Slides (&lt;a href=&#34;/stat120/Day19.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;
</description>
    </item>
    
    <item>
      <title>Week 7</title>
      <link>/courses/stat220/week7/</link>
      <pubDate>Sun, 11 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week7/</guid>
      <description>&lt;!--

Day 17 Slides (&lt;a href=&#34;/stat220/Day17.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 18 Slides (&lt;a href=&#34;/stat220/Day18.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 19 Slides (&lt;a href=&#34;/stat220/Day19.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


--&gt;
</description>
    </item>
    
    <item>
      <title>Week 6</title>
      <link>/courses/stat120/week6/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week6/</guid>
      <description>&lt;!--

Day 15 Slides (&lt;a href=&#34;/stat120/Day15.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 16 Slides (&lt;a href=&#34;/stat120/Day16.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


--&gt;
</description>
    </item>
    
    <item>
      <title>Week 6</title>
      <link>/courses/stat220/week6/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week6/</guid>
      <description>&lt;!--

Day 15 Slides (&lt;a href=&#34;/stat220/Day15.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 16 Slides (&lt;a href=&#34;/stat220/Day16.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


--&gt;</description>
    </item>
    
    <item>
      <title>Week 5</title>
      <link>/courses/stat120/week5/</link>
      <pubDate>Sun, 28 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week5/</guid>
      <description>&lt;!--

Day 12 Slides (&lt;a href=&#34;/stat120/Day12.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 13 Slides (&lt;a href=&#34;/stat120/Day13.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 14 Slides (&lt;a href=&#34;/stat120/Day14.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 5</title>
      <link>/courses/stat220/week5/</link>
      <pubDate>Sun, 28 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week5/</guid>
      <description>&lt;!--

Day 12 Slides (&lt;a href=&#34;/stat220/Day12.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 13 Midterm

Day 14 Slides (&lt;a href=&#34;/stat220/Day14.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 4</title>
      <link>/courses/stat120/week4/</link>
      <pubDate>Sun, 21 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week4/</guid>
      <description>&lt;!--

Day 9 Slides (&lt;a href=&#34;/stat120/Day9.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 10 Midterm!

Day 11 Slides (&lt;a href=&#34;/stat120/Day11.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 4</title>
      <link>/courses/stat220/week4/</link>
      <pubDate>Sun, 21 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week4/</guid>
      <description>&lt;!--

Day 9 Slides (&lt;a href=&#34;/stat220/Day9.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


Day 10 Slides (&lt;a href=&#34;/stat220/Day10.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


Day 11 Slides (&lt;a href=&#34;/stat220/Day11.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 3</title>
      <link>/courses/stat120/week3/</link>
      <pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week3/</guid>
      <description>&lt;!--

Day 6 Slides (&lt;a href=&#34;/stat120/Day6.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


Day 7 Slides (&lt;a href=&#34;/stat120/Day7.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 8 Slides (&lt;a href=&#34;/stat120/Day8.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 3</title>
      <link>/courses/stat220/week3/</link>
      <pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week3/</guid>
      <description>&lt;!--

Day 6 Slides (&lt;a href=&#34;/stat220/Day6.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 7 Slides (&lt;a href=&#34;/stat220/Day7.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 8 Slides (&lt;a href=&#34;/stat220/Day8.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 2</title>
      <link>/courses/stat120/week2/</link>
      <pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week2/</guid>
      <description>&lt;!--

Day 3 Slides (&lt;a href=&#34;/stat120/Day3.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 4 Slides (&lt;a href=&#34;/stat120/Day4.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 5 Slides (&lt;a href=&#34;/stat120/Day5.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 2</title>
      <link>/courses/stat220/week2/</link>
      <pubDate>Sun, 07 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week2/</guid>
      <description>&lt;!--

Day 3 Slides (&lt;a href=&#34;/stat220/Day3.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 4 Slides (&lt;a href=&#34;/stat220/Day4.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 5 Slides (&lt;a href=&#34;/stat220/Day5.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 1</title>
      <link>/courses/stat120/week1/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat120/week1/</guid>
      <description>&lt;!--

Day 1 Slides (&lt;a href=&#34;/stat120/Day1.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)


Day 2 Slides (&lt;a href=&#34;/stat120/Day2.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;
</description>
    </item>
    
    <item>
      <title>Week 1</title>
      <link>/courses/stat220/week1/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/week1/</guid>
      <description>&lt;!--

Day 1 Slides (&lt;a href=&#34;/stat220/Day1.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

Day 2 Slides (&lt;a href=&#34;/stat220/Day2.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)

--&gt;</description>
    </item>
    
    <item>
      <title>Week 10</title>
      <link>/courses/stat230/week10/</link>
      <pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Week 9</title>
      <link>/courses/stat230/week9/</link>
      <pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week9/</guid>
      <description>&lt;p&gt;Day 24 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day24.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day24.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 25 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day25.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day25.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 26 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day26.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day26.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 8</title>
      <link>/courses/stat230/week8/</link>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week8/</guid>
      <description>&lt;p&gt;Day 21 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day21.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day21.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 22 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day22.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day22.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 23 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day23.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day23.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 7</title>
      <link>/courses/stat230/week7/</link>
      <pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week7/</guid>
      <description>&lt;p&gt;Day 18 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day18.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day18.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 19 Midterm II&lt;/p&gt;
&lt;p&gt;Day 20 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day20.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day20.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 6</title>
      <link>/courses/stat230/week6/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week6/</guid>
      <description>&lt;p&gt;Midterm Break on Monday 05/02!&lt;/p&gt;
&lt;p&gt;Day 16 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day16.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day16.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 17 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day17.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day17.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 5</title>
      <link>/courses/stat230/week5/</link>
      <pubDate>Mon, 25 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week5/</guid>
      <description>&lt;p&gt;Day 13 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day13.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day13.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 14 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day14.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day14.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 15 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day15.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day15.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 4</title>
      <link>/courses/stat230/week4/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week4/</guid>
      <description>&lt;p&gt;Day 10 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day10.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day10.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 11 Midterm I&lt;/p&gt;
&lt;p&gt;Day 12 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day12.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day12.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 3</title>
      <link>/courses/stat230/week3/</link>
      <pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week3/</guid>
      <description>&lt;p&gt;Day 7 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day7.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day7.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 8 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day8.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day8.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 9 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day9.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day9.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 2</title>
      <link>/courses/stat230/week2/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week2/</guid>
      <description>&lt;p&gt;Day 4 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day4.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day4.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 5 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day5.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day5.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 6 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day6.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day6.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 1</title>
      <link>/courses/stat230/week1/</link>
      <pubDate>Sat, 26 Mar 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat230/week1/</guid>
      <description>&lt;p&gt;Day 1 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day1.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day1.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 2 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day2.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day2.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Day 3 Slides (&lt;a href=&#34;/stat230/lecture_notes/Day3.pdf&#34; target=&#34;_blank&#34;&gt;pdf&lt;/a&gt;/&lt;a href=&#34;/stat230/lecture_notes/Day3.html&#34; target=&#34;_blank&#34;&gt;html&lt;/a&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Link to cheatsheets</title>
      <link>/courses/stat220/cheatsheets/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/cheatsheets/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;/cheatsheets/rmarkdown-2.0.pdf&#34; target=&#34;_blank&#34;&gt;R markdown Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/data-visualization.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/data-transformation.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/tidyr.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;tidyr&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/readr.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;readr&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/lubridate.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;lubridate&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/strings.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;stringr&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/regex.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;regex&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/shiny.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;shiny&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/caret.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;caret&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/cheatsheets/leaflet.pdf&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;leaflet&lt;/code&gt; Cheatsheet&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Link to datasets</title>
      <link>/courses/stat220/data/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/courses/stat220/data/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;/data/simple-1.dat&#34; target=&#34;_blank&#34;&gt;Simple Data file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/data/mild-1.csv&#34; target=&#34;_blank&#34;&gt;Medium Data file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/data/tricky-1.csv&#34; target=&#34;_blank&#34;&gt;Tricky Data file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/data/tricky-2.csv&#34; target=&#34;_blank&#34;&gt;More Tricky Data file&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exoplanet modeling</title>
      <link>/2023/10/07/exoplanet-modeling/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      <guid>/2023/10/07/exoplanet-modeling/</guid>
      <description>&lt;p&gt;The figure below showcases a spectrum integral to exoplanet detection, charting the transit depth against the wavelength in microns. Every observation point in the spectrum carries an inherent uncertainty, denoted by the vertical error bars. To decode and potentially minimize this uncertainty, it&amp;rsquo;s pivotal to fathom how features like &lt;code&gt;planet radius&lt;/code&gt;, &lt;code&gt;planet temperature&lt;/code&gt;, and the logarithmic concentrations of &lt;code&gt;H₂O&lt;/code&gt;, &lt;code&gt;CO₂&lt;/code&gt;, &lt;code&gt;CO&lt;/code&gt;, &lt;code&gt;CH₄&lt;/code&gt;, and &lt;code&gt;NH₃&lt;/code&gt; influence the transit depth. Leveraging interpretative tools like SHAP can provide insights into how these exoplanetary features impact the observed transit depth, refining our understanding and accuracy in exoplanet spectral analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image
Image(filename=&#39;my_spectrum.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_2_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the CSV file into a Pandas DataFrame
df = pd.read_csv(&amp;quot;small_astro.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_backup = df.copy()
df_backup.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;planet_radius&lt;/th&gt;
      &lt;th&gt;planet_temp&lt;/th&gt;
      &lt;th&gt;log_h2o&lt;/th&gt;
      &lt;th&gt;log_co2&lt;/th&gt;
      &lt;th&gt;log_co&lt;/th&gt;
      &lt;th&gt;log_ch4&lt;/th&gt;
      &lt;th&gt;log_nh3&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x43&lt;/th&gt;
      &lt;th&gt;x44&lt;/th&gt;
      &lt;th&gt;x45&lt;/th&gt;
      &lt;th&gt;x46&lt;/th&gt;
      &lt;th&gt;x47&lt;/th&gt;
      &lt;th&gt;x48&lt;/th&gt;
      &lt;th&gt;x49&lt;/th&gt;
      &lt;th&gt;x50&lt;/th&gt;
      &lt;th&gt;x51&lt;/th&gt;
      &lt;th&gt;x52&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.559620&lt;/td&gt;
      &lt;td&gt;863.394770&lt;/td&gt;
      &lt;td&gt;-8.865868&lt;/td&gt;
      &lt;td&gt;-6.700707&lt;/td&gt;
      &lt;td&gt;-5.557561&lt;/td&gt;
      &lt;td&gt;-8.957615&lt;/td&gt;
      &lt;td&gt;-3.097540&lt;/td&gt;
      &lt;td&gt;0.003836&lt;/td&gt;
      &lt;td&gt;0.003834&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.003938&lt;/td&gt;
      &lt;td&gt;0.003941&lt;/td&gt;
      &lt;td&gt;0.003903&lt;/td&gt;
      &lt;td&gt;0.003931&lt;/td&gt;
      &lt;td&gt;0.003983&lt;/td&gt;
      &lt;td&gt;0.004019&lt;/td&gt;
      &lt;td&gt;0.004046&lt;/td&gt;
      &lt;td&gt;0.004072&lt;/td&gt;
      &lt;td&gt;0.004054&lt;/td&gt;
      &lt;td&gt;0.004056&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1.118308&lt;/td&gt;
      &lt;td&gt;1201.700465&lt;/td&gt;
      &lt;td&gt;-4.510258&lt;/td&gt;
      &lt;td&gt;-8.228966&lt;/td&gt;
      &lt;td&gt;-3.565427&lt;/td&gt;
      &lt;td&gt;-7.807424&lt;/td&gt;
      &lt;td&gt;-3.633658&lt;/td&gt;
      &lt;td&gt;0.015389&lt;/td&gt;
      &lt;td&gt;0.015148&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.015450&lt;/td&gt;
      &lt;td&gt;0.015447&lt;/td&gt;
      &lt;td&gt;0.015461&lt;/td&gt;
      &lt;td&gt;0.015765&lt;/td&gt;
      &lt;td&gt;0.016099&lt;/td&gt;
      &lt;td&gt;0.016376&lt;/td&gt;
      &lt;td&gt;0.016549&lt;/td&gt;
      &lt;td&gt;0.016838&lt;/td&gt;
      &lt;td&gt;0.016781&lt;/td&gt;
      &lt;td&gt;0.016894&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.400881&lt;/td&gt;
      &lt;td&gt;1556.096477&lt;/td&gt;
      &lt;td&gt;-7.225472&lt;/td&gt;
      &lt;td&gt;-6.931472&lt;/td&gt;
      &lt;td&gt;-3.081975&lt;/td&gt;
      &lt;td&gt;-8.567854&lt;/td&gt;
      &lt;td&gt;-5.378472&lt;/td&gt;
      &lt;td&gt;0.002089&lt;/td&gt;
      &lt;td&gt;0.002073&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.001989&lt;/td&gt;
      &lt;td&gt;0.002168&lt;/td&gt;
      &lt;td&gt;0.002176&lt;/td&gt;
      &lt;td&gt;0.002123&lt;/td&gt;
      &lt;td&gt;0.002079&lt;/td&gt;
      &lt;td&gt;0.002081&lt;/td&gt;
      &lt;td&gt;0.002106&lt;/td&gt;
      &lt;td&gt;0.002167&lt;/td&gt;
      &lt;td&gt;0.002149&lt;/td&gt;
      &lt;td&gt;0.002185&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.345974&lt;/td&gt;
      &lt;td&gt;1268.624884&lt;/td&gt;
      &lt;td&gt;-7.461157&lt;/td&gt;
      &lt;td&gt;-5.853334&lt;/td&gt;
      &lt;td&gt;-3.044711&lt;/td&gt;
      &lt;td&gt;-5.149378&lt;/td&gt;
      &lt;td&gt;-3.815568&lt;/td&gt;
      &lt;td&gt;0.002523&lt;/td&gt;
      &lt;td&gt;0.002392&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.002745&lt;/td&gt;
      &lt;td&gt;0.003947&lt;/td&gt;
      &lt;td&gt;0.004296&lt;/td&gt;
      &lt;td&gt;0.003528&lt;/td&gt;
      &lt;td&gt;0.003352&lt;/td&gt;
      &lt;td&gt;0.003629&lt;/td&gt;
      &lt;td&gt;0.003929&lt;/td&gt;
      &lt;td&gt;0.004363&lt;/td&gt;
      &lt;td&gt;0.004216&lt;/td&gt;
      &lt;td&gt;0.004442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.733184&lt;/td&gt;
      &lt;td&gt;1707.323564&lt;/td&gt;
      &lt;td&gt;-4.140844&lt;/td&gt;
      &lt;td&gt;-7.460278&lt;/td&gt;
      &lt;td&gt;-3.181793&lt;/td&gt;
      &lt;td&gt;-5.996593&lt;/td&gt;
      &lt;td&gt;-4.535345&lt;/td&gt;
      &lt;td&gt;0.002957&lt;/td&gt;
      &lt;td&gt;0.002924&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.003402&lt;/td&gt;
      &lt;td&gt;0.003575&lt;/td&gt;
      &lt;td&gt;0.003667&lt;/td&gt;
      &lt;td&gt;0.003740&lt;/td&gt;
      &lt;td&gt;0.003823&lt;/td&gt;
      &lt;td&gt;0.003904&lt;/td&gt;
      &lt;td&gt;0.003897&lt;/td&gt;
      &lt;td&gt;0.004004&lt;/td&gt;
      &lt;td&gt;0.004111&lt;/td&gt;
      &lt;td&gt;0.004121&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.161165&lt;/td&gt;
      &lt;td&gt;620.185809&lt;/td&gt;
      &lt;td&gt;-4.875000&lt;/td&gt;
      &lt;td&gt;-5.074766&lt;/td&gt;
      &lt;td&gt;-3.861240&lt;/td&gt;
      &lt;td&gt;-5.388011&lt;/td&gt;
      &lt;td&gt;-8.390503&lt;/td&gt;
      &lt;td&gt;0.000444&lt;/td&gt;
      &lt;td&gt;0.000442&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000432&lt;/td&gt;
      &lt;td&gt;0.000486&lt;/td&gt;
      &lt;td&gt;0.000473&lt;/td&gt;
      &lt;td&gt;0.000462&lt;/td&gt;
      &lt;td&gt;0.000447&lt;/td&gt;
      &lt;td&gt;0.000455&lt;/td&gt;
      &lt;td&gt;0.000455&lt;/td&gt;
      &lt;td&gt;0.000457&lt;/td&gt;
      &lt;td&gt;0.000463&lt;/td&gt;
      &lt;td&gt;0.000474&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.194312&lt;/td&gt;
      &lt;td&gt;900.597575&lt;/td&gt;
      &lt;td&gt;-8.299899&lt;/td&gt;
      &lt;td&gt;-6.850709&lt;/td&gt;
      &lt;td&gt;-4.314491&lt;/td&gt;
      &lt;td&gt;-3.712038&lt;/td&gt;
      &lt;td&gt;-3.951455&lt;/td&gt;
      &lt;td&gt;0.001794&lt;/td&gt;
      &lt;td&gt;0.001721&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.001048&lt;/td&gt;
      &lt;td&gt;0.001052&lt;/td&gt;
      &lt;td&gt;0.000948&lt;/td&gt;
      &lt;td&gt;0.000976&lt;/td&gt;
      &lt;td&gt;0.001122&lt;/td&gt;
      &lt;td&gt;0.001274&lt;/td&gt;
      &lt;td&gt;0.001395&lt;/td&gt;
      &lt;td&gt;0.001522&lt;/td&gt;
      &lt;td&gt;0.001456&lt;/td&gt;
      &lt;td&gt;0.001823&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1.132685&lt;/td&gt;
      &lt;td&gt;1176.443900&lt;/td&gt;
      &lt;td&gt;-6.765865&lt;/td&gt;
      &lt;td&gt;-7.398548&lt;/td&gt;
      &lt;td&gt;-3.378307&lt;/td&gt;
      &lt;td&gt;-3.763737&lt;/td&gt;
      &lt;td&gt;-5.881384&lt;/td&gt;
      &lt;td&gt;0.012950&lt;/td&gt;
      &lt;td&gt;0.012946&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.014019&lt;/td&gt;
      &lt;td&gt;0.013871&lt;/td&gt;
      &lt;td&gt;0.013810&lt;/td&gt;
      &lt;td&gt;0.013902&lt;/td&gt;
      &lt;td&gt;0.014024&lt;/td&gt;
      &lt;td&gt;0.014150&lt;/td&gt;
      &lt;td&gt;0.014298&lt;/td&gt;
      &lt;td&gt;0.014392&lt;/td&gt;
      &lt;td&gt;0.014401&lt;/td&gt;
      &lt;td&gt;0.015042&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.158621&lt;/td&gt;
      &lt;td&gt;1189.209841&lt;/td&gt;
      &lt;td&gt;-8.376041&lt;/td&gt;
      &lt;td&gt;-6.321977&lt;/td&gt;
      &lt;td&gt;-3.243900&lt;/td&gt;
      &lt;td&gt;-8.711851&lt;/td&gt;
      &lt;td&gt;-3.449195&lt;/td&gt;
      &lt;td&gt;0.000444&lt;/td&gt;
      &lt;td&gt;0.000445&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000562&lt;/td&gt;
      &lt;td&gt;0.000595&lt;/td&gt;
      &lt;td&gt;0.000571&lt;/td&gt;
      &lt;td&gt;0.000590&lt;/td&gt;
      &lt;td&gt;0.000628&lt;/td&gt;
      &lt;td&gt;0.000663&lt;/td&gt;
      &lt;td&gt;0.000692&lt;/td&gt;
      &lt;td&gt;0.000734&lt;/td&gt;
      &lt;td&gt;0.000718&lt;/td&gt;
      &lt;td&gt;0.000736&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.660642&lt;/td&gt;
      &lt;td&gt;528.023669&lt;/td&gt;
      &lt;td&gt;-3.804286&lt;/td&gt;
      &lt;td&gt;-8.919378&lt;/td&gt;
      &lt;td&gt;-4.686964&lt;/td&gt;
      &lt;td&gt;-8.150277&lt;/td&gt;
      &lt;td&gt;-3.068319&lt;/td&gt;
      &lt;td&gt;0.008997&lt;/td&gt;
      &lt;td&gt;0.009035&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.009435&lt;/td&gt;
      &lt;td&gt;0.009375&lt;/td&gt;
      &lt;td&gt;0.009315&lt;/td&gt;
      &lt;td&gt;0.009357&lt;/td&gt;
      &lt;td&gt;0.009563&lt;/td&gt;
      &lt;td&gt;0.009739&lt;/td&gt;
      &lt;td&gt;0.009821&lt;/td&gt;
      &lt;td&gt;0.009890&lt;/td&gt;
      &lt;td&gt;0.009819&lt;/td&gt;
      &lt;td&gt;0.009734&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;10 rows × 60 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Columns of interest for planetary and chemical properties and transit depth
feature_columns = df.columns[1:8]
transit_depth_column = &#39;x1&#39; # pick the first wavelength

# Calculate mean and variance for features and transit depth
feature_mean = df[feature_columns].mean()
feature_variance = df[feature_columns].var()
transit_depth_mean = df[transit_depth_column].mean()
transit_depth_variance = df[transit_depth_column].var()

# Visualize the distributions
fig, axes = plt.subplots(1, 8, figsize=(18, 4))
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[i])
    axes[i].set_title(f&#39;{col}\nMean: {feature_mean[col]:.2f}\nVariance: {feature_variance[col]:.2f}&#39;)

# Add visualization for transit depth
sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1])
axes[-1].set_title(f&#39;{transit_depth_column}\nMean: {transit_depth_mean:.2f}\nVariance: {transit_depth_variance:.2f}&#39;)

plt.tight_layout()
plt.show()

feature_mean, feature_variance, transit_depth_mean, transit_depth_variance
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_5_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(planet_radius       0.703714
 planet_temp      1073.229674
 log_h2o            -5.934889
 log_co2            -6.873009
 log_co             -4.497141
 log_ch4            -5.799850
 log_nh3            -6.051791
 dtype: float64,
 planet_radius         0.198990
 planet_temp      154495.743225
 log_h2o               3.130868
 log_co2               1.649658
 log_co                0.738255
 log_ch4               3.208283
 log_nh3               3.050545
 dtype: float64,
 0.009442470522591322,
 0.00016172106267489707)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;exoplanet-feature-distributions&#34;&gt;Exoplanet Feature Distributions&lt;/h2&gt;
&lt;p&gt;The provided visualizations and statistics shed light on the distribution of various exoplanetary features and the observed transit depth at the &lt;code&gt;x1&lt;/code&gt; wavelength.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Radius&lt;/strong&gt;: This feature, with a mean value of approximately 0.7037 and variance of 0.1989, mostly lies between 0.5 and 1.5 as depicted in its histogram.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Temperature&lt;/strong&gt;: Exhibiting a wider spread, the temperature has a mean of approximately 1073.29 K and variance of 154495.74 K².&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logarithmic Concentrations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H₂O&lt;/strong&gt;: Mean concentration of -5.93 with a variance of 3.13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CO₂&lt;/strong&gt;: Mean concentration of -6.87 with a variance of 1.65.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CO&lt;/strong&gt;: Mean concentration of -4.50 with a variance of 0.74.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CH₄&lt;/strong&gt;: Mean concentration of -5.80 with a variance of 3.21.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NH₃&lt;/strong&gt;: Mean concentration of -6.05 with a variance of 3.05.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transit Depth at x1 Wavelength&lt;/strong&gt;: This depth, crucial for exoplanet detection, has an almost singular value near 0, with a mean of approximately 0.0094 and a negligible variance of 0.00006.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These distributions and their accompanying statistics offer invaluable insights into the data&amp;rsquo;s nature and its inherent variability, essential for accurate spectral analysis and interpretation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import required libraries for modeling and SHAP values
import xgboost as xgb
import shap

# Prepare the feature matrix (X) and the target vector (y)
X = df.iloc[:, 1:8]
y = df[&#39;x1&#39;]

# Train an XGBoost model
model = xgb.XGBRegressor(objective =&#39;reg:squarederror&#39;)
model.fit(X, y)

# Initialize the SHAP explainer
explainer = shap.Explainer(model)

# Calculate SHAP values
shap_values = explainer(X)

# Summary plot for SHAP values
shap.summary_plot(shap_values, X, title=&#39;Shapley Feature Importance&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_7_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;SHAP (SHapley Additive exPlanations) values stem from cooperative game theory and provide a way to interpret machine learning model predictions. Each feature in a model is analogous to a player in a game, and the contribution of each feature to a prediction is like the payout a player receives in the game. In SHAP, we calculate the value of each feature by considering all possible combinations (coalitions) of features, assessing the change in prediction with and without that feature. The resulting value, the Shapley value, represents the average contribution of a feature to all possible predictions.&lt;/p&gt;
&lt;p&gt;The summary plot visualizes these values. Each dot represents a SHAP value for a specific instance of a feature; positive values (right of the centerline) indicate that a feature increases the model&amp;rsquo;s output, while negative values (left of the centerline) suggest a decrease. The color depicts the actual value of the feature for the given instance, enabling a comprehensive view of feature influence across the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the Random Forest Regressor and visualization libraries
from sklearn.ensemble import RandomForestRegressor

# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Extract feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    &#39;Feature&#39;: feature_columns,
    &#39;Importance&#39;: feature_importances
}).sort_values(by=&#39;Importance&#39;, ascending=True)

# Create a horizontal bar chart for feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df[&#39;Feature&#39;], importance_df[&#39;Importance&#39;], color=&#39;dodgerblue&#39;)
plt.xlabel(&#39;Importance&#39;)
plt.ylabel(&#39;&#39;)
plt.title(&#39;Random Forest Feature Importance&#39;)
plt.xticks(fontsize=13)  # Increase the font size of the x-axis tick labels
plt.yticks(fontsize=14)  # Increase the font size of the x-axis tick labels
# Save the figure before showing it
plt.savefig(&#39;random_forest_importance_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()



feature_importances
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_9_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71850575, 0.10367982, 0.01655567, 0.07232808, 0.05603188,
       0.0146824 , 0.0182164 ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;random-forest-feature-importance-analysis&#34;&gt;Random Forest Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;To gain insights into which exoplanetary features most influence the observed transit depth, a Random Forest Regressor was utilized. Here&amp;rsquo;s an outline of the procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;: A Random Forest Regressor model was instantiated with 100 trees and a fixed random seed of 42 for reproducibility.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Training&lt;/strong&gt;: The model was trained on the feature set &lt;code&gt;X&lt;/code&gt; and target variable &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Importance Extraction&lt;/strong&gt;: After training, the importance of each feature was extracted using the &lt;code&gt;feature_importances_&lt;/code&gt; attribute of the trained model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Preparation for Visualization&lt;/strong&gt;: A DataFrame was created to house each feature alongside its respective importance. The features were then sorted in ascending order of importance for better visualization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: A horizontal bar chart was plotted to showcase the importance of each feature. The chart offers a clear visual comparison, with the y-axis representing the features and the x-axis indicating their importance. Special attention was paid to font size adjustments for better readability. Furthermore, before displaying the chart, it was saved as a PNG image with high resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resulting visualization, titled &amp;lsquo;Random Forest Feature Importance&amp;rsquo;, provides a clear understanding of the relative significance of each feature in predicting the transit depth, as discerned by the Random Forest model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image
Image(filename=&#39;PME.png&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_11_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;pme-feature-importance-analysis&#34;&gt;PME Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;Using a modeling technique, the impact of different exoplanetary features on the observed transit depth was assessed, and their importance was visualized in the attached figure titled &amp;lsquo;PME Feature Importance&amp;rsquo;. Here&amp;rsquo;s a breakdown of the visual representation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Most Influential Feature&lt;/strong&gt;: The &lt;code&gt;planet_radius&lt;/code&gt; stands out as the most influential feature with the highest PME (Predictive Modeling Estimate) value. This suggests that the radius of the planet plays a pivotal role in determining the observed transit depth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Other Features&lt;/strong&gt;: Logarithmic concentrations of gases, such as &lt;code&gt;log_co2&lt;/code&gt;, &lt;code&gt;log_co&lt;/code&gt;, &lt;code&gt;log_nh3&lt;/code&gt;, &lt;code&gt;log_h2o&lt;/code&gt;, and &lt;code&gt;log_ch4&lt;/code&gt;, also exhibit varying degrees of importance. Among these, &lt;code&gt;log_co2&lt;/code&gt; and &lt;code&gt;log_co&lt;/code&gt; are the more significant contributors compared to others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Least Influential Feature&lt;/strong&gt;: The &lt;code&gt;planet_temp&lt;/code&gt;, representing the temperature of the planet, has the least importance in this analysis, suggesting its minimal role in influencing the transit depth, at least according to the PME metric.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visual Clarity&lt;/strong&gt;: The horizontal bar chart offers a lucid comparison of feature importances. Each bar&amp;rsquo;s length represents the PME value of a feature, providing a direct visual cue to its significance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;: This visualization aids in discerning which exoplanetary characteristics are most relevant when predicting the transit depth using the given model. It can guide future analyses by highlighting key features to focus on or, conversely, those that might be less consequential.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By examining the &amp;lsquo;PME Feature Importance&amp;rsquo; chart, one gains a deeper understanding of the relative significance of each feature in predicting the transit depth within this specific modeling context.&lt;/p&gt;
&lt;h2 id=&#34;uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis&#34;&gt;Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;When delving deep into the realms of uncertainty quantification and feature reduction in predictive modeling, it&amp;rsquo;s crucial to evaluate feature importance metrics critically. The provided PME (Predictive Modeling Estimate) Feature Importance Analysis offers a valuable lens for this task. Below is a nuanced exploration of its significance in the described contexts:&lt;/p&gt;
&lt;h3 id=&#34;uncertainty-quantification&#34;&gt;&lt;strong&gt;Uncertainty Quantification&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Origin of Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In exoplanetary spectral analysis, uncertainty can arise from observational noise, instrumental errors, or intrinsic variability of the observed phenomena. This uncertainty often manifests in the form of error bars in spectra, like the ones shown in transit depth against wavelengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Impact on Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The degree of influence a feature has on the predicted outcome can be a proxy for how that feature might contribute to the overall predictive uncertainty. If a feature like &lt;code&gt;planet_radius&lt;/code&gt; has a high PME value, it might be a critical determinant of transit depth. Any uncertainty in measuring or estimating the &lt;code&gt;planet_radius&lt;/code&gt; could propagate and significantly affect the prediction&amp;rsquo;s reliability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME as a Measure of Stochastic Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The PME values themselves might be obtained by analyzing a model&amp;rsquo;s sensitivity to perturbations in input features. A high PME value indicates that slight changes in the feature can lead to notable changes in the output, thereby implying a greater inherent stochastic uncertainty tied to that feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;feature-reduction&#34;&gt;&lt;strong&gt;Feature Reduction&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identifying Critical Features&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When dealing with a multitude of features, not all may be equally relevant. The PME analysis provides a hierarchy of feature importance. In this case, while &lt;code&gt;planet_radius&lt;/code&gt; emerges as crucial, &lt;code&gt;planet_temp&lt;/code&gt; appears less consequential. This differentiation is fundamental for feature reduction, guiding us on which features to prioritize in modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reducing Dimensionality &amp;amp; Complexity&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In data-driven modeling, especially with limited data points, overfitting is a genuine concern. By understanding which features significantly influence the predictions (like &lt;code&gt;planet_radius&lt;/code&gt; or &lt;code&gt;log_co2&lt;/code&gt;), one can potentially reduce the model&amp;rsquo;s complexity and the risk of overfitting by focusing only on these paramount features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Informing Experimental Design&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If further observational or experimental data is required, knowing feature importances can guide where resources are channeled. For instance, more precise measurements might be sought for features with high PME values, as their accurate estimation is vital for reliable predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trade-off with Predictive Performance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s essential to understand that while feature reduction can simplify models and make them more interpretable, there&amp;rsquo;s always a trade-off with predictive performance. Removing features based on their PME values should be done judiciously, ensuring that the model&amp;rsquo;s predictive capability isn&amp;rsquo;t unduly compromised.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In summary, the &amp;lsquo;PME Feature Importance&amp;rsquo; chart isn&amp;rsquo;t merely a representation of feature significance but serves as a cornerstone for rigorous analytical decisions in uncertainty quantification and feature reduction. Analyzing such importance metrics within the broader context of the problem at hand ensures that models are both robust and interpretable, catering effectively to the dual objectives of predictive accuracy and analytical clarity.&lt;/p&gt;
&lt;h3 id=&#34;weighted-principal-component-analysis-pca-on-exoplanet-data&#34;&gt;Weighted Principal Component Analysis (PCA) on Exoplanet Data&lt;/h3&gt;
&lt;h4 id=&#34;conceptual-overview&#34;&gt;&lt;strong&gt;Conceptual Overview&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;PCA is a method used to emphasize variation and capture strong patterns in a dataset. The &amp;ldquo;weighted PCA&amp;rdquo; approach fine-tunes this by considering the importance of different features, effectively giving more attention to features deemed vital.&lt;/p&gt;
&lt;h4 id=&#34;detailed-breakdown&#34;&gt;&lt;strong&gt;Detailed Breakdown&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Standardization&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before applying PCA, the dataset is standardized to give each feature a mean of 0 and a variance of 1. This is essential because PCA is influenced by the scale of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Weighted Features&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Features are weighted according to their importance, as identified by the Random Forest model. Taking the square root of the weights ensures proper scaling during matrix multiplication in PCA.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PCA Application&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA projects the data into a new space defined by its principal components. The first two components often hold most of the dataset&amp;rsquo;s variance, making them crucial for visualization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The scatter plot visualizes the data in the space of the first two principal components. The color indicates &lt;code&gt;Transit Depth (x1)&lt;/code&gt;, shedding light on how this parameter varies across the main patterns in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explained Variance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This provides an understanding of how much original variance the first two components capture. A high percentage indicates that the PCA representation retains much of the data&amp;rsquo;s original structure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;significance&#34;&gt;&lt;strong&gt;Significance&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Compression&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA offers a simplified yet rich representation of the data, which can be invaluable for visualization and pattern recognition.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Emphasis&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using feature importances ensures the PCA representation highlights the most critical patterns related to influential features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Framework for Further Exploration&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observing patterns or groupings in the PCA plot can guide subsequent investigations, pinpointing areas of interest or potential clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient Data Overview&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The visualization provides a comprehensive but digestible overview of the data, suitable for a wide range of audiences.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In essence, weighted PCA melds the dimensionality reduction capabilities of PCA with the interpretative power of feature importances, offering a profound view into the dataset&amp;rsquo;s intricate structures and relationships.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
# The square root is used because each feature contributes to both rows and columns in the dot product calculation
X_weighted = X_scaled * np.sqrt(feature_importances)

# Perform PCA on the weighted data
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_weighted)

# Plotting the first two principal components
plt.figure(figsize=(10, 7))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
plt.colorbar().set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
plt.xlabel(&#39;Weighted Principal Component 1&#39;)
plt.ylabel(&#39;Weighted Principal Component 2&#39;)
plt.title(&#39;Weighted PCA: First Two Principal Components&#39;)
plt.show()

# Variance explained by the first two principal components
variance_explained = pca.explained_variance_ratio_
variance_explained

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_15_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71985328, 0.10370239])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Your data initialization (this part is assumed, as you haven&#39;t provided it in the original code)
# X, y, feature_importances = ...

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Perform PCA on the weighted data with three components
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_weighted)

# Plotting the first three principal components
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
plt.colorbar(scatter, ax=ax, pad=0.2).set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
ax.set_xlabel(&#39;Weighted Principal Component 1&#39;)
ax.set_ylabel(&#39;Weighted Principal Component 2&#39;)
ax.set_zlabel(&#39;Weighted Principal Component 3&#39;)
plt.title(&#39;Weighted PCA: First Three Principal Components&#39;)
plt.show()

# Variance explained by the first three principal components
variance_explained = pca.explained_variance_ratio_
variance_explained

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_16_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71985328, 0.10370239, 0.07160805])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# side by side
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Your data initialization (this part is assumed, as you haven&#39;t provided it in the original code)
# X, y, feature_importances = ...

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Create a combined figure with subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 7))

# Perform PCA on the weighted data with two components and plot
pca2 = PCA(n_components=2)
X_pca2 = pca2.fit_transform(X_weighted)
scatter_2d = axes[0].scatter(X_pca2[:, 0], X_pca2[:, 1], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
fig.colorbar(scatter_2d, ax=axes[0], orientation=&#39;vertical&#39;).set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
axes[0].set_xlabel(&#39;Weighted Principal Component 1&#39;)
axes[0].set_ylabel(&#39;Weighted Principal Component 2&#39;)
axes[0].set_title(&#39;Weighted PCA: First Two Principal Components&#39;)

# Perform PCA on the weighted data with three components and plot
pca3 = PCA(n_components=3)
X_pca3 = pca3.fit_transform(X_weighted)
ax3d = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;)
scatter_3d = ax3d.scatter(X_pca3[:, 0], X_pca3[:, 1], X_pca3[:, 2], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
fig.colorbar(scatter_3d, ax=ax3d, pad=0.2).set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
ax3d.set_xlabel(&#39;Weighted Principal Component 1&#39;)
ax3d.set_ylabel(&#39;Weighted Principal Component 2&#39;)
ax3d.set_zlabel(&#39;Weighted Principal Component 3&#39;)
ax3d.set_title(&#39;Weighted PCA: First Three Principal Components&#39;)

# Display the combined plot
plt.tight_layout()
plt.savefig(&#39;wPCA_2D_3D.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_17_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import seaborn as sns

# Your data initialization (this part is assumed, as you haven&#39;t provided it in the original code)
# df, feature_columns, transit_depth_column = ...

# Reconstruct the approximate original data using the first three principal components
reconstructed_data = np.dot(X_pca, pca.components_[:3, :]) + np.mean(X_scaled, axis=0)

# Create a DataFrame for the reconstructed data
reconstructed_df = pd.DataFrame(reconstructed_data, columns=feature_columns)

# Visualize the approximate original histograms
fig, axes = plt.subplots(1, len(feature_columns) + 1, figsize=(18, 4))  # Adjusted for the number of features
for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[i], color=&#39;orange&#39;)
    axes[i].set_title(f&#39;Approx of {col}&#39;)

# Add visualization for actual transit depth (since it was not part of the PCA)
sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1], color=&#39;green&#39;)
axes[-1].set_title(f&#39;Actual {transit_depth_column}&#39;)

plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_18_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
fig, axes = plt.subplots(2, 8, figsize=(18, 8))

# Original Data
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])
    axes[0, i].set_title(f&#39;{col}\nMean: {feature_mean[col]:.2f}\nVariance: {feature_variance[col]:.2f}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])
axes[0, -1].set_title(f&#39;{transit_depth_column}\nMean: {transit_depth_mean:.2f}\nVariance: {transit_depth_variance:.2f}&#39;)

# Reconstructed Data
for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[1, i], color=&#39;orange&#39;)
    axes[1, i].set_title(f&#39;Surrogate of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color=&#39;green&#39;)
axes[1, -1].set_title(f&#39;Actual {transit_depth_column}&#39;)

plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_19_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming data initialization and feature_importances, feature_columns, and transit_depth_column definitions exist

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Initialize the figure and axes
fig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))

# Original Data histograms
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])
    axes[0, i].set_title(f&#39;Original {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])
axes[0, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Reconstruction using 2 PCs
pca2 = PCA(n_components=2)
X_pca2 = pca2.fit_transform(X_weighted)
reconstructed_data_2PCs = np.dot(X_pca2, pca2.components_[:2, :]) + np.mean(X_scaled, axis=0)
reconstructed_df_2PCs = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_2PCs[col], bins=20, kde=True, ax=axes[1, i], color=&#39;orange&#39;)
    axes[1, i].set_title(f&#39;2 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color=&#39;green&#39;)
axes[1, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Reconstruction using 3 PCs
pca3 = PCA(n_components=3)
X_pca3 = pca3.fit_transform(X_weighted)
reconstructed_data_3PCs = np.dot(X_pca3, pca3.components_[:3, :]) + np.mean(X_scaled, axis=0)
reconstructed_df_3PCs = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_3PCs[col], bins=20, kde=True, ax=axes[2, i], color=&#39;purple&#39;)
    axes[2, i].set_title(f&#39;3 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color=&#39;green&#39;)
axes[2, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

plt.tight_layout()
plt.savefig(&#39;wPCA_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_20_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;reconstruction-of-exoplanet-data-using-principal-component-analysis-pca&#34;&gt;Reconstruction of Exoplanet Data Using Principal Component Analysis (PCA)&lt;/h3&gt;
&lt;p&gt;The code segment aims to reconstruct exoplanet data using different numbers of principal components (PCs) from PCA, offering insights into how much information retention occurs as we vary the number of PCs.&lt;/p&gt;
&lt;h4 id=&#34;data-standardization&#34;&gt;&lt;strong&gt;Data Standardization&lt;/strong&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The features undergo standardization, ensuring each feature has a mean of 0 and variance of 1. This normalization is crucial for PCA, as the algorithm is sensitive to varying scales across features.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;weighted-features&#34;&gt;&lt;strong&gt;Weighted Features&lt;/strong&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Features are adjusted based on their significance as ascertained by a prior model, specifically the Random Forest. Weighting the features adjusts the emphasis the PCA places on each feature during the dimensionality reduction process.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data-reconstruction&#34;&gt;&lt;strong&gt;Data Reconstruction&lt;/strong&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After performing PCA, the algorithm seeks to transform the reduced data back to the original high-dimensional space. This &amp;ldquo;reconstructed&amp;rdquo; data is an approximation of the original but is built using fewer dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;visualization-of-reconstructions&#34;&gt;&lt;strong&gt;Visualization of Reconstructions&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Original Data Histograms&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The initial histograms show the distributions of the original features and the transit depth.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reconstruction Using 2 Principal Components&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using only the first two PCs, the data is reconstructed and its histograms visualized. This illustrates the patterns and distributions captured when only the first two components are considered.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reconstruction Using 3 Principal Components&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analogous reconstruction is done with three PCs. The addition of a third dimension might capture more nuanced variations in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;significance-1&#34;&gt;&lt;strong&gt;Significance&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Compression vs. Retention&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The visualizations enable us to compare the reconstructions against the original data. We discern how much information is retained and what is lost as we reduce dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Guide for Dimensionality Decision&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By juxtaposing the original with the reconstructions, we gain insights into the optimal number of PCs to use for specific tasks, striking a balance between compression and information retention.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Empirical Understanding&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These histograms and visual representations offer a tangible way to grasp the abstract notion of dimensionality reduction. They elucidate how PCA captures the essence of the data while diminishing dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In conclusion, this analysis, coupled with the visualizations, equips us with a robust understanding of how PCA reconstructs data using varying numbers of components. It underscores the trade-offs involved and the implications of choosing specific dimensionality levels in data-driven tasks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.gridspec as gridspec

# Load images
img_shap = mpimg.imread(&#39;shapley.png&#39;)
img_rf = mpimg.imread(&#39;random_forest_importance_plot.png&#39;)
img_pme = mpimg.imread(&#39;PME.png&#39;)

# Create a grid for the subplots
fig_combined = plt.figure(figsize=(20, 12))
gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])

# Display SHAP plot
ax0 = plt.subplot(gs[0])
ax0.imshow(img_shap)
ax0.axis(&#39;off&#39;)

# Display RF Importance plot
ax1 = plt.subplot(gs[1])
ax1.imshow(img_rf)
ax1.axis(&#39;off&#39;)

# Display PME image in the middle of the 2nd row
ax2 = plt.subplot(gs[2:4])  # This makes the PME plot span both columns on the second row
ax2.imshow(img_pme)
ax2.axis(&#39;off&#39;)

plt.tight_layout()
plt.savefig(&#39;sensitivity_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_22_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This code snippet consolidates and displays three distinct plots—SHAP values, Random Forest Feature Importance, and PME Feature Importance—into a single visualization. The images are loaded and arranged in a 2x2 grid, with the SHAP and Random Forest plots on the top row, and the PME plot spanning both columns on the bottom row. After layout adjustments, the combined visualization is saved as a high-resolution PNG image titled &amp;lsquo;sensitivity_combined_plot.png&amp;rsquo; and then displayed.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;Changeat, Q., &amp;amp; Yip, K. H. (2023). ESA-Ariel Data Challenge NeurIPS 2022: Introduction to exo-atmospheric studies and presentation of the Atmospheric Big Challenge (ABC) Database. &lt;em&gt;arXiv preprint arXiv:2206.14633&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Herin, M., Il Idrissi, M., Chabridon, V., &amp;amp; Iooss, B. (2022). Proportional marginal effects for global sensitivity analysis. arXiv preprint arXiv:2210.13065.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to predict AAPL Stick price using an Informer model to capture long term dependencies?</title>
      <link>/2023/10/07/how-to-predict-aapl-stick-price-using-an-informer-model-to-capture-long-term-dependencies/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      <guid>/2023/10/07/how-to-predict-aapl-stick-price-using-an-informer-model-to-capture-long-term-dependencies/</guid>
      <description>&lt;h1 id=&#34;informer&#34;&gt;Informer&lt;/h1&gt;
&lt;p&gt;The Informer model variant is designed for multivariate prediction. Let&amp;rsquo;s consider &amp;lsquo;Open&amp;rsquo;, &amp;lsquo;High&amp;rsquo;, &amp;lsquo;Low&amp;rsquo;, and &amp;lsquo;Close&amp;rsquo; prices for simplicity. The provided code is designed to fetch and preprocess historical stock prices for Apple Inc. for the purpose of multivariate time series forecasting using an LSTM model. Initially, the code downloads Apple&amp;rsquo;s stock data, specifically capturing four significant features: Open, High, Low, and Close prices. To make the data suitable for deep learning models, it is normalized to fit within a range of 0 to 1. The sequential data is then transformed into a format suitable for supervised learning, where the data from the past &lt;code&gt;look_back&lt;/code&gt; days is used to predict the next day&amp;rsquo;s features. Finally, the data is partitioned into training (67%) and test sets, ensuring separate datasets for model training and evaluation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install yfinance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
import yfinance as yf
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Download historical data as dataframe
data = yf.download(&amp;quot;AAPL&amp;quot;, start=&amp;quot;2018-01-01&amp;quot;, end=&amp;quot;2023-09-01&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Using multiple columns for multivariate prediction
df = data[[&amp;quot;Open&amp;quot;, &amp;quot;High&amp;quot;, &amp;quot;Low&amp;quot;, &amp;quot;Close&amp;quot;]]

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
df_scaled = scaler.fit_transform(df)

# Prepare data for LSTM
look_back = 10  # Number of previous time steps to use as input variables
n_features = df.shape[1]  # number of features

# Convert to supervised learning problem
X, y = [], []
for i in range(len(df_scaled) - look_back):
    X.append(df_scaled[i:i+look_back, :])
    y.append(df_scaled[i + look_back, :])
X, y = np.array(X), np.array(y)

# Reshape input to be [samples, time steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], n_features))

# Train-test split
train_size = int(len(X) * 0.67)
test_size = len(X) - train_size
X_train, X_test = X[0:train_size], X[train_size:]
y_train, y_test = y[0:train_size], y[train_size:]

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;probsparse-self-attention&#34;&gt;ProbSparse Self-Attention:&lt;/h1&gt;
&lt;p&gt;Self-attention involves computing a weighted sum of all values in the sequence, based on the dot product between the query and key. In ProbSparse, we don&amp;rsquo;t compute this for all query-key pairs, but rather select dominant ones, thus making the computation more efficient. The given code defines a custom Keras layer, &lt;code&gt;ProbSparseSelfAttention&lt;/code&gt;, which implements the multi-head self-attention mechanism, a critical component of Transformer models. This layer initializes three dense networks for the Query, Key, and Value matrices, and splits the input data into multiple heads to enable parallel processing. During the forward pass (&lt;code&gt;call&lt;/code&gt; method), the Query, Key, and Value matrices are calculated, scaled, and then used to compute attention scores. These scores indicate the importance of each element in the sequence when predicting another element. The output is a weighted sum of the input values, which is then passed through another dense layer to produce the final result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ProbSparseSelfAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, **kwargs):
        super(ProbSparseSelfAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.d_model = d_model

        # Assert that d_model is divisible by num_heads
        assert self.d_model % self.num_heads == 0, f&amp;quot;d_model ({d_model}) must be divisible by num_heads ({num_heads})&amp;quot;

        self.depth = d_model // self.num_heads

        # Defining the dense layers for Query, Key and Value
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)

        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        # Fixing matrix multiplication
        matmul_qk = tf.matmul(q, k, transpose_b=True)

        d_k = tf.cast(self.depth, tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)

        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)

        output = tf.transpose(output, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))

        return self.dense(concat_attention)

&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;informer-encoder&#34;&gt;Informer Encoder:&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;InformerEncoder&lt;/code&gt; is a custom Keras layer designed to process sequential data using a combination of attention and convolutional mechanisms. Within the encoder, the input data undergoes multi-head self-attention, utilizing the &lt;code&gt;ProbSparseSelfAttention&lt;/code&gt; mechanism, to capture relationships in the data regardless of their distance. Post attention, the data is transformed and normalized, then further processed using two 1D convolutional layers, emphasizing local features in the data. After another normalization step, the processed data is pooled to a lower dimensionality, ensuring the model captures global context, and then passed through a dense layer to produce the final output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class InformerEncoder(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, conv_filters, **kwargs):
        super(InformerEncoder, self).__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads

        # Assert that d_model is divisible by num_heads
        assert self.d_model % self.num_heads == 0, f&amp;quot;d_model ({d_model}) must be divisible by num_heads ({num_heads})&amp;quot;

        self.self_attention = ProbSparseSelfAttention(d_model=d_model, num_heads=num_heads)

        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        # This dense layer will transform the input &#39;x&#39; to have the dimensionality &#39;d_model&#39;
        self.dense_transform = tf.keras.layers.Dense(d_model)

        self.conv1 = tf.keras.layers.Conv1D(conv_filters, 3, padding=&#39;same&#39;)
        self.conv2 = tf.keras.layers.Conv1D(d_model, 3, padding=&#39;same&#39;)
        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()
        self.dense = tf.keras.layers.Dense(d_model)

    def call(self, x):
        attn_output = self.self_attention(x, x, x)

        # Transform &#39;x&#39; to have the desired dimensionality
        x_transformed = self.dense_transform(x)
        attn_output = self.norm1(attn_output + x_transformed)

        conv_output = self.conv1(attn_output)
        conv_output = tf.nn.relu(conv_output)
        conv_output = self.conv2(conv_output)

        encoded_output = self.norm2(conv_output + attn_output)

        pooled_output = self.global_avg_pooling(encoded_output)
        return self.dense(pooled_output)[:, -4:]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_layer = tf.keras.layers.Input(shape=(look_back, n_features))

# Encoder
encoder_output = InformerEncoder(d_model=360, num_heads=8, conv_filters=64)(input_layer)

# Decoder (with attention)
decoder_lstm = tf.keras.layers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;InformerModel&lt;/code&gt; function is designed to create a deep learning architecture tailored for sequential data prediction. It takes an input sequence and processes it using the &lt;code&gt;InformerEncoder&lt;/code&gt;, a custom encoder layer, which captures both local and global patterns in the data. Following the encoding step, a decoder structure unravels the encoded data by first repeating the encoder&amp;rsquo;s output, then passing it through an LSTM layer to retain sequential dependencies, and finally making predictions using a dense layer. The resulting architecture is then compiled with the Adam optimizer and Mean Squared Error loss, ready for training on time series data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.layers import RepeatVector
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam


def InformerModel(input_shape, d_model=64, num_heads=2, conv_filters=256, learning_rate= 1e-3):
    # Input
    input_layer = Input(shape=input_shape)

    # Encoder
    encoder_output = InformerEncoder(d_model=d_model, num_heads=num_heads, conv_filters=conv_filters)(input_layer)

    # Decoder
    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder&#39;s output
    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)
    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value

    # Model
    model = Model(inputs=input_layer, outputs=decoder_output)
    # Compile the model with the specified learning rate
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)



    return model

model = InformerModel(input_shape=(look_back, n_features))
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
model.summary()



&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model_10&amp;quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 10, 4)]           0         
                                                                 
 informer_encoder_11 (Infor  (None, 4)                 108480    
 merEncoder)                                                     
                                                                 
 repeat_vector_10 (RepeatVe  (None, 4, 4)              0         
 ctor)                                                           
                                                                 
 lstm_10 (LSTM)              (None, 4, 312)            395616    
                                                                 
 tf.__operators__.getitem_1  (None, 312)               0         
 0 (SlicingOpLambda)                                             
                                                                 
 dense_82 (Dense)            (None, 4)                 1252      
                                                                 
=================================================================
Total params: 505348 (1.93 MB)
Trainable params: 505348 (1.93 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;model.fit&lt;/code&gt; method trains the neural network using the provided training data (&lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt;) for a total of 50 epochs with mini-batches of 32 samples. During training, 20% of the training data is set aside for validation to monitor and prevent overfitting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_split=0.3,
    shuffle=True
)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code displays a visual representation of the model&amp;rsquo;s training and validation loss over the epochs using a line chart. The x-axis represents the number of epochs, while the y-axis indicates the mean squared error, allowing users to observe how the model&amp;rsquo;s performance evolves over time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(12, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;Training Loss&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)
plt.title(&#39;Training and Validation Loss&#39;)
plt.xlabel(&#39;Epoch&#39;)
plt.ylabel(&#39;Mean Squared Error&#39;)
plt.legend()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_15_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_predictions = model.predict(X_test)
test_predictions = scaler.inverse_transform(test_predictions)
true_values = scaler.inverse_transform(y_test)

mse = mean_squared_error(true_values, test_predictions)
print(f&amp;quot;Test MSE: {mse}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;15/15 [==============================] - 1s 3ms/step
Test MSE: 462.6375895467179
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(12, 6))
plt.plot(true_values, label=&#39;True Values&#39;)
plt.plot(test_predictions, label=&#39;Predictions&#39;, alpha=0.6)
plt.title(&#39;Test Set Predictions vs. True Values&#39;)
plt.legend()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_17_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install keras-tuner

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code defines a function to construct a neural network model using varying hyperparameters, aiming to optimize its architecture. Subsequently, the RandomSearch method from Keras Tuner is employed to explore 200 different model configurations, assessing their performance to determine the best hyperparameters that minimize the validation loss.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.layers import Input, RepeatVector, LSTM, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import kerastuner as kt

def build_model(hp):
    # Input
    input_layer = Input(shape=(look_back, n_features))

    # Encoder
    encoder_output = InformerEncoder(d_model=hp.Int(&#39;d_model&#39;, min_value=32, max_value=512, step=16),
                                     num_heads=hp.Int(&#39;num_heads&#39;, 2, 8, step=2),
                                     conv_filters=hp.Int(&#39;conv_filters&#39;, min_value=16, max_value=256, step=16))(input_layer)

    # Decoder
    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder&#39;s output
    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)
    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value

    # Model
    model = Model(inputs=input_layer, outputs=decoder_output)

    # Compile the model with the specified learning rate
    optimizer = Adam(learning_rate=hp.Choice(&#39;learning_rate&#39;, [1e-3, 1e-2, 1e-1]))
    model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)

    return model

# Define the tuner
tuner = kt.RandomSearch(
    build_model,
    objective=&#39;val_loss&#39;,
    max_trials=30,
    executions_per_trial=5,
    directory=&#39;hyperparam_search&#39;,
    project_name=&#39;informer_model&#39;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code sets up two training callbacks: one for early stopping if validation loss doesn&amp;rsquo;t improve after 10 epochs, and another to save the model weights at their best performance. With these callbacks, the tuner conducts a search over the hyperparameter space using the training data, and evaluates model configurations over 100 epochs, saving the most optimal weights and potentially halting early if improvements stagnate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10, verbose=1, restore_best_weights=True)
model_checkpoint = ModelCheckpoint(filepath=&#39;trial_best.h5&#39;, monitor=&#39;val_loss&#39;, verbose=1, save_best_only=True)

tuner.search(X_train, y_train,
             epochs=100,
             validation_split=0.2,
             callbacks=[early_stopping, model_checkpoint])

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Trial 30 Complete [00h 00m 01s]

Best val_loss So Far: 0.0009468139498494566
Total elapsed time: 00h 30m 43s
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the best hyperparameters
best_hp = tuner.get_best_hyperparameters()[0]

# Retrieve the best model
best_model = tuner.get_best_models()[0]
best_model.summary()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 10, 4)]           0         
                                                                 
 informer_encoder (Informer  (None, 4)                 108480    
 Encoder)                                                        
                                                                 
 repeat_vector (RepeatVecto  (None, 4, 4)              0         
 r)                                                              
                                                                 
 lstm (LSTM)                 (None, 4, 312)            395616    
                                                                 
 tf.__operators__.getitem (  (None, 312)               0         
 SlicingOpLambda)                                                
                                                                 
 dense_6 (Dense)             (None, 4)                 1252      
                                                                 
=================================================================
Total params: 505348 (1.93 MB)
Trainable params: 505348 (1.93 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_loss = best_model.evaluate(X_test, y_test)
print(f&amp;quot;Test MSE: {test_loss}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;15/15 [==============================] - 0s 3ms/step - loss: 0.0086
Test MSE: 0.008609036915004253
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(20, 12))
for i in range(true_values.shape[1]):
    plt.subplot(2, 2, i+1)
    plt.plot(true_values[:, i], label=&#39;True Values&#39;, color=&#39;blue&#39;)
    plt.plot(test_predictions[:, i], label=&#39;Predictions&#39;, color=&#39;red&#39;, linestyle=&#39;--&#39;)
    plt.title(f&amp;quot;Feature {i+1}&amp;quot;)
    plt.legend()
plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_25_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(true_values, test_predictions)
rmse = np.sqrt(test_loss)  # since loss is MSE
print(f&amp;quot;MAE: {mae}, RMSE: {rmse}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MAE: 19.377517553476185, RMSE: 0.09278489594219662
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&amp;quot;Best d_model: {best_hp.get(&#39;d_model&#39;)}&amp;quot;)
print(f&amp;quot;Best num_heads: {best_hp.get(&#39;num_heads&#39;)}&amp;quot;)
print(f&amp;quot;Best conv_filters: {best_hp.get(&#39;conv_filters&#39;)}&amp;quot;)
print(f&amp;quot;Best learning_rate: {best_hp.get(&#39;learning_rate&#39;)}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best d_model: 64
Best num_heads: 2
Best conv_filters: 256
Best learning_rate: 0.001
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install shap
import shap

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The reference can be a dataset or just random data
background = X_train[np.random.choice(X_train.shape[0], 300, replace=False)]  # Taking a random sample of the training data as background
explainer = shap.GradientExplainer(best_model, background)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shap_values = explainer.shap_values(X_test[:300])  # Computing for a subset for performance reasons

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for timestep in range(10):
    print(f&amp;quot;Summary plot for timestep {timestep + 1}&amp;quot;)
    shap.summary_plot(shap_values[0][:, timestep, :], X_test[:300, timestep, :])

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_3.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_5.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_7.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_9.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_11.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_13.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_15.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_17.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_19.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;shapley-values&#34;&gt;Shapley values&lt;/h1&gt;
&lt;p&gt;SHAP values are indicating by how much the presence of a particular feature influenced the model&amp;rsquo;s prediction, compared to if that feature was absent. The color represents the actual value of the feature itself. In the context of the present work, Shapley values are employed as a method to enhance the interpretability of the model. Shapley values provide insights into the contribution of each feature to the model&amp;rsquo;s predictions. Specifically, they quantify how each feature influences the prediction by evaluating its impact when combined with all other features. By calculating Shapley values, we gain a clear understanding of the relative importance of each feature in multivariate time series prediction.&lt;/p&gt;
&lt;p&gt;Overall, Shapley values enhance model interpretability by offering a systematic and quantitative way to dissect complex models and understand their decision-making processes. They enable us to identify which features are the key drivers of predictions, helping us make more informed decisions and potentially improving model performance. This interpretability is crucial in various applications, from finance to healthcare, where understanding the factors influencing predictions is paramount for trust and decision-making.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting AAPL Stock Price with Informer Model</title>
      <link>/project/internal-project/informer-timeseries/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/informer-timeseries/</guid>
      <description>&lt;h1 id=&#34;informer&#34;&gt;Informer&lt;/h1&gt;
&lt;p&gt;We aim to predict stock prices for Apple Inc. using multivariate time series data, focusing on &amp;lsquo;Open&amp;rsquo;, &amp;lsquo;High&amp;rsquo;, &amp;lsquo;Low&amp;rsquo;, and &amp;lsquo;Close&amp;rsquo; prices. The process begins by acquiring historical stock data from Yahoo Finance. Once acquired, we extract the four essential columns for prediction. To ensure optimal LSTM model performance, the data is normalized to fit between 0 and 1. We then transform this sequential data into a format suitable for supervised learning, where a sequence of 10 days&amp;rsquo; data predicts the following day&amp;rsquo;s features. This structured data is divided into training and test sets, with 67% dedicated to training and the remaining 33% for testing. This pre-processed dataset is now primed for feeding into an LSTM model for forecasting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install yfinance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
import yfinance as yf
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Download historical data as dataframe
data = yf.download(&amp;quot;AAPL&amp;quot;, start=&amp;quot;2018-01-01&amp;quot;, end=&amp;quot;2023-09-01&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Using multiple columns for multivariate prediction
df = data[[&amp;quot;Open&amp;quot;, &amp;quot;High&amp;quot;, &amp;quot;Low&amp;quot;, &amp;quot;Close&amp;quot;]]

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
df_scaled = scaler.fit_transform(df)

# Prepare data for LSTM
look_back = 10  # Number of previous time steps to use as input variables
n_features = df.shape[1]  # number of features

# Convert to supervised learning problem
X, y = [], []
for i in range(len(df_scaled) - look_back):
    X.append(df_scaled[i:i+look_back, :])
    y.append(df_scaled[i + look_back, :])
X, y = np.array(X), np.array(y)

# Reshape input to be [samples, time steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], n_features))

# Train-test split
train_size = int(len(X) * 0.67)
test_size = len(X) - train_size
X_train, X_test = X[0:train_size], X[train_size:]
y_train, y_test = y[0:train_size], y[train_size:]

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;probsparse-self-attention&#34;&gt;ProbSparse Self-Attention:&lt;/h1&gt;
&lt;p&gt;Self-attention involves computing a weighted sum of all values in the sequence, based on the dot product between the query and key. In ProbSparse, we don&amp;rsquo;t compute this for all query-key pairs, but rather select dominant ones, thus making the computation more efficient. The given code defines a custom Keras layer, &lt;code&gt;ProbSparseSelfAttention&lt;/code&gt;, which implements the multi-head self-attention mechanism, a critical component of Transformer models. This layer initializes three dense networks for the Query, Key, and Value matrices, and splits the input data into multiple heads to enable parallel processing. During the forward pass (&lt;code&gt;call&lt;/code&gt; method), the Query, Key, and Value matrices are calculated, scaled, and then used to compute attention scores. These scores indicate the importance of each element in the sequence when predicting another element. The output is a weighted sum of the input values, which is then passed through another dense layer to produce the final result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class ProbSparseSelfAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, **kwargs):
        super(ProbSparseSelfAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.d_model = d_model

        # Assert that d_model is divisible by num_heads
        assert self.d_model % self.num_heads == 0, f&amp;quot;d_model ({d_model}) must be divisible by num_heads ({num_heads})&amp;quot;

        self.depth = d_model // self.num_heads

        # Defining the dense layers for Query, Key and Value
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)

        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        # Fixing matrix multiplication
        matmul_qk = tf.matmul(q, k, transpose_b=True)

        d_k = tf.cast(self.depth, tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)

        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)

        output = tf.transpose(output, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))

        return self.dense(concat_attention)

&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;informer-encoder&#34;&gt;Informer Encoder:&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;InformerEncoder&lt;/code&gt; is a custom Keras layer designed to process sequential data using a combination of attention and convolutional mechanisms. Within the encoder, the input data undergoes multi-head self-attention, utilizing the &lt;code&gt;ProbSparseSelfAttention&lt;/code&gt; mechanism, to capture relationships in the data regardless of their distance. Post attention, the data is transformed and normalized, then further processed using two 1D convolutional layers, emphasizing local features in the data. After another normalization step, the processed data is pooled to a lower dimensionality, ensuring the model captures global context, and then passed through a dense layer to produce the final output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class InformerEncoder(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, conv_filters, **kwargs):
        super(InformerEncoder, self).__init__(**kwargs)
        self.d_model = d_model
        self.num_heads = num_heads

        # Assert that d_model is divisible by num_heads
        assert self.d_model % self.num_heads == 0, f&amp;quot;d_model ({d_model}) must be divisible by num_heads ({num_heads})&amp;quot;

        self.self_attention = ProbSparseSelfAttention(d_model=d_model, num_heads=num_heads)

        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        # This dense layer will transform the input &#39;x&#39; to have the dimensionality &#39;d_model&#39;
        self.dense_transform = tf.keras.layers.Dense(d_model)

        self.conv1 = tf.keras.layers.Conv1D(conv_filters, 3, padding=&#39;same&#39;)
        self.conv2 = tf.keras.layers.Conv1D(d_model, 3, padding=&#39;same&#39;)
        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()
        self.dense = tf.keras.layers.Dense(d_model)

    def call(self, x):
        attn_output = self.self_attention(x, x, x)

        # Transform &#39;x&#39; to have the desired dimensionality
        x_transformed = self.dense_transform(x)
        attn_output = self.norm1(attn_output + x_transformed)

        conv_output = self.conv1(attn_output)
        conv_output = tf.nn.relu(conv_output)
        conv_output = self.conv2(conv_output)

        encoded_output = self.norm2(conv_output + attn_output)

        pooled_output = self.global_avg_pooling(encoded_output)
        return self.dense(pooled_output)[:, -4:]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_layer = tf.keras.layers.Input(shape=(look_back, n_features))

# Encoder
encoder_output = InformerEncoder(d_model=360, num_heads=8, conv_filters=64)(input_layer)

# Decoder (with attention)
decoder_lstm = tf.keras.layers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;InformerModel&lt;/code&gt; function is designed to create a deep learning architecture tailored for sequential data prediction. It takes an input sequence and processes it using the &lt;code&gt;InformerEncoder&lt;/code&gt;, a custom encoder layer, which captures both local and global patterns in the data. Following the encoding step, a decoder structure unravels the encoded data by first repeating the encoder&amp;rsquo;s output, then passing it through an LSTM layer to retain sequential dependencies, and finally making predictions using a dense layer. The resulting architecture is then compiled with the Adam optimizer and Mean Squared Error loss, ready for training on time series data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.layers import RepeatVector
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam


def InformerModel(input_shape, d_model=64, num_heads=2, conv_filters=256, learning_rate= 1e-3):
    # Input
    input_layer = Input(shape=input_shape)

    # Encoder
    encoder_output = InformerEncoder(d_model=d_model, num_heads=num_heads, conv_filters=conv_filters)(input_layer)

    # Decoder
    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder&#39;s output
    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)
    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value

    # Model
    model = Model(inputs=input_layer, outputs=decoder_output)
    # Compile the model with the specified learning rate
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)



    return model

model = InformerModel(input_shape=(look_back, n_features))
model.compile(optimizer=&#39;adam&#39;, loss=&#39;mean_squared_error&#39;)
model.summary()



&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model_10&amp;quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 10, 4)]           0         
                                                                 
 informer_encoder_11 (Infor  (None, 4)                 108480    
 merEncoder)                                                     
                                                                 
 repeat_vector_10 (RepeatVe  (None, 4, 4)              0         
 ctor)                                                           
                                                                 
 lstm_10 (LSTM)              (None, 4, 312)            395616    
                                                                 
 tf.__operators__.getitem_1  (None, 312)               0         
 0 (SlicingOpLambda)                                             
                                                                 
 dense_82 (Dense)            (None, 4)                 1252      
                                                                 
=================================================================
Total params: 505348 (1.93 MB)
Trainable params: 505348 (1.93 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;model.fit&lt;/code&gt; method trains the neural network using the provided training data (&lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;y_train&lt;/code&gt;) for a total of 50 epochs with mini-batches of 32 samples. During training, 20% of the training data is set aside for validation to monitor and prevent overfitting.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_split=0.3,
    shuffle=True
)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code displays a visual representation of the model&amp;rsquo;s training and validation loss over the epochs using a line chart. The x-axis represents the number of epochs, while the y-axis indicates the mean squared error, allowing users to observe how the model&amp;rsquo;s performance evolves over time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(12, 6))
plt.plot(history.history[&#39;loss&#39;], label=&#39;Training Loss&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)
plt.title(&#39;Training and Validation Loss&#39;)
plt.xlabel(&#39;Epoch&#39;)
plt.ylabel(&#39;Mean Squared Error&#39;)
plt.legend()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_15_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_predictions = model.predict(X_test)
test_predictions = scaler.inverse_transform(test_predictions)
true_values = scaler.inverse_transform(y_test)

mse = mean_squared_error(true_values, test_predictions)
print(f&amp;quot;Test MSE: {mse}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;15/15 [==============================] - 1s 3ms/step
Test MSE: 462.6375895467179
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(12, 6))
plt.plot(true_values, label=&#39;True Values&#39;)
plt.plot(test_predictions, label=&#39;Predictions&#39;, alpha=0.6)
plt.title(&#39;Test Set Predictions vs. True Values&#39;)
plt.legend()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_17_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install keras-tuner

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code defines a function to construct a neural network model using varying hyperparameters, aiming to optimize its architecture. Subsequently, the RandomSearch method from Keras Tuner is employed to explore 200 different model configurations, assessing their performance to determine the best hyperparameters that minimize the validation loss.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.layers import Input, RepeatVector, LSTM, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import kerastuner as kt

def build_model(hp):
    # Input
    input_layer = Input(shape=(look_back, n_features))

    # Encoder
    encoder_output = InformerEncoder(d_model=hp.Int(&#39;d_model&#39;, min_value=32, max_value=512, step=16),
                                     num_heads=hp.Int(&#39;num_heads&#39;, 2, 8, step=2),
                                     conv_filters=hp.Int(&#39;conv_filters&#39;, min_value=16, max_value=256, step=16))(input_layer)

    # Decoder
    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder&#39;s output
    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)
    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value

    # Model
    model = Model(inputs=input_layer, outputs=decoder_output)

    # Compile the model with the specified learning rate
    optimizer = Adam(learning_rate=hp.Choice(&#39;learning_rate&#39;, [1e-3, 1e-2, 1e-1]))
    model.compile(optimizer=optimizer, loss=&#39;mean_squared_error&#39;)

    return model

# Define the tuner
tuner = kt.RandomSearch(
    build_model,
    objective=&#39;val_loss&#39;,
    max_trials=30,
    executions_per_trial=5,
    directory=&#39;hyperparam_search&#39;,
    project_name=&#39;informer_model&#39;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code sets up two training callbacks: one for early stopping if validation loss doesn&amp;rsquo;t improve after 10 epochs, and another to save the model weights at their best performance. With these callbacks, the tuner conducts a search over the hyperparameter space using the training data, and evaluates model configurations over 100 epochs, saving the most optimal weights and potentially halting early if improvements stagnate.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10, verbose=1, restore_best_weights=True)
model_checkpoint = ModelCheckpoint(filepath=&#39;trial_best.h5&#39;, monitor=&#39;val_loss&#39;, verbose=1, save_best_only=True)

tuner.search(X_train, y_train,
             epochs=100,
             validation_split=0.2,
             callbacks=[early_stopping, model_checkpoint])

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Trial 30 Complete [00h 00m 01s]

Best val_loss So Far: 0.0009468139498494566
Total elapsed time: 00h 30m 43s
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the best hyperparameters
best_hp = tuner.get_best_hyperparameters()[0]

# Retrieve the best model
best_model = tuner.get_best_models()[0]
best_model.summary()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model: &amp;quot;model&amp;quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 10, 4)]           0         
                                                                 
 informer_encoder (Informer  (None, 4)                 108480    
 Encoder)                                                        
                                                                 
 repeat_vector (RepeatVecto  (None, 4, 4)              0         
 r)                                                              
                                                                 
 lstm (LSTM)                 (None, 4, 312)            395616    
                                                                 
 tf.__operators__.getitem (  (None, 312)               0         
 SlicingOpLambda)                                                
                                                                 
 dense_6 (Dense)             (None, 4)                 1252      
                                                                 
=================================================================
Total params: 505348 (1.93 MB)
Trainable params: 505348 (1.93 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_loss = best_model.evaluate(X_test, y_test)
print(f&amp;quot;Test MSE: {test_loss}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;15/15 [==============================] - 0s 3ms/step - loss: 0.0086
Test MSE: 0.008609036915004253
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(20, 12))
for i in range(true_values.shape[1]):
    plt.subplot(2, 2, i+1)
    plt.plot(true_values[:, i], label=&#39;True Values&#39;, color=&#39;blue&#39;)
    plt.plot(test_predictions[:, i], label=&#39;Predictions&#39;, color=&#39;red&#39;, linestyle=&#39;--&#39;)
    plt.title(f&amp;quot;Feature {i+1}&amp;quot;)
    plt.legend()
plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_25_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(true_values, test_predictions)
rmse = np.sqrt(test_loss)  # since loss is MSE
print(f&amp;quot;MAE: {mae}, RMSE: {rmse}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MAE: 19.377517553476185, RMSE: 0.09278489594219662
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(f&amp;quot;Best d_model: {best_hp.get(&#39;d_model&#39;)}&amp;quot;)
print(f&amp;quot;Best num_heads: {best_hp.get(&#39;num_heads&#39;)}&amp;quot;)
print(f&amp;quot;Best conv_filters: {best_hp.get(&#39;conv_filters&#39;)}&amp;quot;)
print(f&amp;quot;Best learning_rate: {best_hp.get(&#39;learning_rate&#39;)}&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Best d_model: 64
Best num_heads: 2
Best conv_filters: 256
Best learning_rate: 0.001
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!pip install shap
import shap

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The reference can be a dataset or just random data
background = X_train[np.random.choice(X_train.shape[0], 300, replace=False)]  # Taking a random sample of the training data as background
explainer = shap.GradientExplainer(best_model, background)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;shap_values = explainer.shap_values(X_test[:300])  # Computing for a subset for performance reasons

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for timestep in range(10):
    print(f&amp;quot;Summary plot for timestep {timestep + 1}&amp;quot;)
    shap.summary_plot(shap_values[0][:, timestep, :], X_test[:300, timestep, :])

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_3.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_5.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_7.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_9.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_11.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 7
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_13.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_15.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 9
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_17.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Summary plot for timestep 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./Multivariate_multistep_Informer_31_19.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;shapley-values&#34;&gt;Shapley values&lt;/h1&gt;
&lt;p&gt;SHAP values are indicating by how much the presence of a particular feature influenced the model&amp;rsquo;s prediction, compared to if that feature was absent. The color represents the actual value of the feature itself. In the context of the present work, Shapley values are employed as a method to enhance the interpretability of the model. Shapley values provide insights into the contribution of each feature to the model&amp;rsquo;s predictions. Specifically, they quantify how each feature influences the prediction by evaluating its impact when combined with all other features. By calculating Shapley values, we gain a clear understanding of the relative importance of each feature in multivariate time series prediction.&lt;/p&gt;
&lt;p&gt;Overall, Shapley values enhance model interpretability by offering a systematic and quantitative way to dissect complex models and understand their decision-making processes. They enable us to identify which features are the key drivers of predictions, helping us make more informed decisions and potentially improving model performance. This interpretability is crucial in various applications, from finance to healthcare, where understanding the factors influencing predictions is paramount for trust and decision-making.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exoplanet Detection Using Reduced Dimensions</title>
      <link>/project/internal-project/wpca-exoplanet/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/wpca-exoplanet/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The figure below showcases a spectrum integral to exoplanet detection, charting the transit depth against the wavelength in microns. Every observation point in the spectrum carries an inherent uncertainty, denoted by the vertical error bars. To decode and potentially minimize this uncertainty, it&amp;rsquo;s pivotal to fathom how features like &lt;code&gt;planet radius&lt;/code&gt;, &lt;code&gt;planet temperature&lt;/code&gt;, and the logarithmic concentrations of &lt;code&gt;H₂O&lt;/code&gt;, &lt;code&gt;CO₂&lt;/code&gt;, &lt;code&gt;CO&lt;/code&gt;, &lt;code&gt;CH₄&lt;/code&gt;, and &lt;code&gt;NH₃&lt;/code&gt; influence the transit depth. Leveraging interpretative tools like SHAP can provide insights into how these exoplanetary features impact the observed transit depth, refining our understanding and accuracy in exoplanet spectral analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image
Image(filename=&#39;my_spectrum.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_2_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the CSV file into a Pandas DataFrame
df = pd.read_csv(&amp;quot;small_astro.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_backup = df.copy()
df_backup.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;planet_radius&lt;/th&gt;
      &lt;th&gt;planet_temp&lt;/th&gt;
      &lt;th&gt;log_h2o&lt;/th&gt;
      &lt;th&gt;log_co2&lt;/th&gt;
      &lt;th&gt;log_co&lt;/th&gt;
      &lt;th&gt;log_ch4&lt;/th&gt;
      &lt;th&gt;log_nh3&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x43&lt;/th&gt;
      &lt;th&gt;x44&lt;/th&gt;
      &lt;th&gt;x45&lt;/th&gt;
      &lt;th&gt;x46&lt;/th&gt;
      &lt;th&gt;x47&lt;/th&gt;
      &lt;th&gt;x48&lt;/th&gt;
      &lt;th&gt;x49&lt;/th&gt;
      &lt;th&gt;x50&lt;/th&gt;
      &lt;th&gt;x51&lt;/th&gt;
      &lt;th&gt;x52&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.559620&lt;/td&gt;
      &lt;td&gt;863.394770&lt;/td&gt;
      &lt;td&gt;-8.865868&lt;/td&gt;
      &lt;td&gt;-6.700707&lt;/td&gt;
      &lt;td&gt;-5.557561&lt;/td&gt;
      &lt;td&gt;-8.957615&lt;/td&gt;
      &lt;td&gt;-3.097540&lt;/td&gt;
      &lt;td&gt;0.003836&lt;/td&gt;
      &lt;td&gt;0.003834&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.003938&lt;/td&gt;
      &lt;td&gt;0.003941&lt;/td&gt;
      &lt;td&gt;0.003903&lt;/td&gt;
      &lt;td&gt;0.003931&lt;/td&gt;
      &lt;td&gt;0.003983&lt;/td&gt;
      &lt;td&gt;0.004019&lt;/td&gt;
      &lt;td&gt;0.004046&lt;/td&gt;
      &lt;td&gt;0.004072&lt;/td&gt;
      &lt;td&gt;0.004054&lt;/td&gt;
      &lt;td&gt;0.004056&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1.118308&lt;/td&gt;
      &lt;td&gt;1201.700465&lt;/td&gt;
      &lt;td&gt;-4.510258&lt;/td&gt;
      &lt;td&gt;-8.228966&lt;/td&gt;
      &lt;td&gt;-3.565427&lt;/td&gt;
      &lt;td&gt;-7.807424&lt;/td&gt;
      &lt;td&gt;-3.633658&lt;/td&gt;
      &lt;td&gt;0.015389&lt;/td&gt;
      &lt;td&gt;0.015148&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.015450&lt;/td&gt;
      &lt;td&gt;0.015447&lt;/td&gt;
      &lt;td&gt;0.015461&lt;/td&gt;
      &lt;td&gt;0.015765&lt;/td&gt;
      &lt;td&gt;0.016099&lt;/td&gt;
      &lt;td&gt;0.016376&lt;/td&gt;
      &lt;td&gt;0.016549&lt;/td&gt;
      &lt;td&gt;0.016838&lt;/td&gt;
      &lt;td&gt;0.016781&lt;/td&gt;
      &lt;td&gt;0.016894&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.400881&lt;/td&gt;
      &lt;td&gt;1556.096477&lt;/td&gt;
      &lt;td&gt;-7.225472&lt;/td&gt;
      &lt;td&gt;-6.931472&lt;/td&gt;
      &lt;td&gt;-3.081975&lt;/td&gt;
      &lt;td&gt;-8.567854&lt;/td&gt;
      &lt;td&gt;-5.378472&lt;/td&gt;
      &lt;td&gt;0.002089&lt;/td&gt;
      &lt;td&gt;0.002073&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.001989&lt;/td&gt;
      &lt;td&gt;0.002168&lt;/td&gt;
      &lt;td&gt;0.002176&lt;/td&gt;
      &lt;td&gt;0.002123&lt;/td&gt;
      &lt;td&gt;0.002079&lt;/td&gt;
      &lt;td&gt;0.002081&lt;/td&gt;
      &lt;td&gt;0.002106&lt;/td&gt;
      &lt;td&gt;0.002167&lt;/td&gt;
      &lt;td&gt;0.002149&lt;/td&gt;
      &lt;td&gt;0.002185&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.345974&lt;/td&gt;
      &lt;td&gt;1268.624884&lt;/td&gt;
      &lt;td&gt;-7.461157&lt;/td&gt;
      &lt;td&gt;-5.853334&lt;/td&gt;
      &lt;td&gt;-3.044711&lt;/td&gt;
      &lt;td&gt;-5.149378&lt;/td&gt;
      &lt;td&gt;-3.815568&lt;/td&gt;
      &lt;td&gt;0.002523&lt;/td&gt;
      &lt;td&gt;0.002392&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.002745&lt;/td&gt;
      &lt;td&gt;0.003947&lt;/td&gt;
      &lt;td&gt;0.004296&lt;/td&gt;
      &lt;td&gt;0.003528&lt;/td&gt;
      &lt;td&gt;0.003352&lt;/td&gt;
      &lt;td&gt;0.003629&lt;/td&gt;
      &lt;td&gt;0.003929&lt;/td&gt;
      &lt;td&gt;0.004363&lt;/td&gt;
      &lt;td&gt;0.004216&lt;/td&gt;
      &lt;td&gt;0.004442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.733184&lt;/td&gt;
      &lt;td&gt;1707.323564&lt;/td&gt;
      &lt;td&gt;-4.140844&lt;/td&gt;
      &lt;td&gt;-7.460278&lt;/td&gt;
      &lt;td&gt;-3.181793&lt;/td&gt;
      &lt;td&gt;-5.996593&lt;/td&gt;
      &lt;td&gt;-4.535345&lt;/td&gt;
      &lt;td&gt;0.002957&lt;/td&gt;
      &lt;td&gt;0.002924&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.003402&lt;/td&gt;
      &lt;td&gt;0.003575&lt;/td&gt;
      &lt;td&gt;0.003667&lt;/td&gt;
      &lt;td&gt;0.003740&lt;/td&gt;
      &lt;td&gt;0.003823&lt;/td&gt;
      &lt;td&gt;0.003904&lt;/td&gt;
      &lt;td&gt;0.003897&lt;/td&gt;
      &lt;td&gt;0.004004&lt;/td&gt;
      &lt;td&gt;0.004111&lt;/td&gt;
      &lt;td&gt;0.004121&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.161165&lt;/td&gt;
      &lt;td&gt;620.185809&lt;/td&gt;
      &lt;td&gt;-4.875000&lt;/td&gt;
      &lt;td&gt;-5.074766&lt;/td&gt;
      &lt;td&gt;-3.861240&lt;/td&gt;
      &lt;td&gt;-5.388011&lt;/td&gt;
      &lt;td&gt;-8.390503&lt;/td&gt;
      &lt;td&gt;0.000444&lt;/td&gt;
      &lt;td&gt;0.000442&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000432&lt;/td&gt;
      &lt;td&gt;0.000486&lt;/td&gt;
      &lt;td&gt;0.000473&lt;/td&gt;
      &lt;td&gt;0.000462&lt;/td&gt;
      &lt;td&gt;0.000447&lt;/td&gt;
      &lt;td&gt;0.000455&lt;/td&gt;
      &lt;td&gt;0.000455&lt;/td&gt;
      &lt;td&gt;0.000457&lt;/td&gt;
      &lt;td&gt;0.000463&lt;/td&gt;
      &lt;td&gt;0.000474&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.194312&lt;/td&gt;
      &lt;td&gt;900.597575&lt;/td&gt;
      &lt;td&gt;-8.299899&lt;/td&gt;
      &lt;td&gt;-6.850709&lt;/td&gt;
      &lt;td&gt;-4.314491&lt;/td&gt;
      &lt;td&gt;-3.712038&lt;/td&gt;
      &lt;td&gt;-3.951455&lt;/td&gt;
      &lt;td&gt;0.001794&lt;/td&gt;
      &lt;td&gt;0.001721&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.001048&lt;/td&gt;
      &lt;td&gt;0.001052&lt;/td&gt;
      &lt;td&gt;0.000948&lt;/td&gt;
      &lt;td&gt;0.000976&lt;/td&gt;
      &lt;td&gt;0.001122&lt;/td&gt;
      &lt;td&gt;0.001274&lt;/td&gt;
      &lt;td&gt;0.001395&lt;/td&gt;
      &lt;td&gt;0.001522&lt;/td&gt;
      &lt;td&gt;0.001456&lt;/td&gt;
      &lt;td&gt;0.001823&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1.132685&lt;/td&gt;
      &lt;td&gt;1176.443900&lt;/td&gt;
      &lt;td&gt;-6.765865&lt;/td&gt;
      &lt;td&gt;-7.398548&lt;/td&gt;
      &lt;td&gt;-3.378307&lt;/td&gt;
      &lt;td&gt;-3.763737&lt;/td&gt;
      &lt;td&gt;-5.881384&lt;/td&gt;
      &lt;td&gt;0.012950&lt;/td&gt;
      &lt;td&gt;0.012946&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.014019&lt;/td&gt;
      &lt;td&gt;0.013871&lt;/td&gt;
      &lt;td&gt;0.013810&lt;/td&gt;
      &lt;td&gt;0.013902&lt;/td&gt;
      &lt;td&gt;0.014024&lt;/td&gt;
      &lt;td&gt;0.014150&lt;/td&gt;
      &lt;td&gt;0.014298&lt;/td&gt;
      &lt;td&gt;0.014392&lt;/td&gt;
      &lt;td&gt;0.014401&lt;/td&gt;
      &lt;td&gt;0.015042&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.158621&lt;/td&gt;
      &lt;td&gt;1189.209841&lt;/td&gt;
      &lt;td&gt;-8.376041&lt;/td&gt;
      &lt;td&gt;-6.321977&lt;/td&gt;
      &lt;td&gt;-3.243900&lt;/td&gt;
      &lt;td&gt;-8.711851&lt;/td&gt;
      &lt;td&gt;-3.449195&lt;/td&gt;
      &lt;td&gt;0.000444&lt;/td&gt;
      &lt;td&gt;0.000445&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000562&lt;/td&gt;
      &lt;td&gt;0.000595&lt;/td&gt;
      &lt;td&gt;0.000571&lt;/td&gt;
      &lt;td&gt;0.000590&lt;/td&gt;
      &lt;td&gt;0.000628&lt;/td&gt;
      &lt;td&gt;0.000663&lt;/td&gt;
      &lt;td&gt;0.000692&lt;/td&gt;
      &lt;td&gt;0.000734&lt;/td&gt;
      &lt;td&gt;0.000718&lt;/td&gt;
      &lt;td&gt;0.000736&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.660642&lt;/td&gt;
      &lt;td&gt;528.023669&lt;/td&gt;
      &lt;td&gt;-3.804286&lt;/td&gt;
      &lt;td&gt;-8.919378&lt;/td&gt;
      &lt;td&gt;-4.686964&lt;/td&gt;
      &lt;td&gt;-8.150277&lt;/td&gt;
      &lt;td&gt;-3.068319&lt;/td&gt;
      &lt;td&gt;0.008997&lt;/td&gt;
      &lt;td&gt;0.009035&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.009435&lt;/td&gt;
      &lt;td&gt;0.009375&lt;/td&gt;
      &lt;td&gt;0.009315&lt;/td&gt;
      &lt;td&gt;0.009357&lt;/td&gt;
      &lt;td&gt;0.009563&lt;/td&gt;
      &lt;td&gt;0.009739&lt;/td&gt;
      &lt;td&gt;0.009821&lt;/td&gt;
      &lt;td&gt;0.009890&lt;/td&gt;
      &lt;td&gt;0.009819&lt;/td&gt;
      &lt;td&gt;0.009734&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;10 rows × 60 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Columns of interest for planetary and chemical properties and transit depth
feature_columns = df.columns[1:8]
transit_depth_column = &#39;x1&#39; # pick the first wavelength

# Calculate mean and variance for features and transit depth
feature_mean = df[feature_columns].mean()
feature_variance = df[feature_columns].var()
transit_depth_mean = df[transit_depth_column].mean()
transit_depth_variance = df[transit_depth_column].var()

# Visualize the distributions
fig, axes = plt.subplots(1, 8, figsize=(18, 4))
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[i])
    axes[i].set_title(f&#39;{col}\nMean: {feature_mean[col]:.2f}\nVariance: {feature_variance[col]:.2f}&#39;)

# Add visualization for transit depth
sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1])
axes[-1].set_title(f&#39;{transit_depth_column}\nMean: {transit_depth_mean:.2f}\nVariance: {transit_depth_variance:.2f}&#39;)

plt.tight_layout()
plt.show()

feature_mean, feature_variance, transit_depth_mean, transit_depth_variance
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_5_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(planet_radius       0.703714
 planet_temp      1073.229674
 log_h2o            -5.934889
 log_co2            -6.873009
 log_co             -4.497141
 log_ch4            -5.799850
 log_nh3            -6.051791
 dtype: float64,
 planet_radius         0.198990
 planet_temp      154495.743225
 log_h2o               3.130868
 log_co2               1.649658
 log_co                0.738255
 log_ch4               3.208283
 log_nh3               3.050545
 dtype: float64,
 0.009442470522591322,
 0.00016172106267489707)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;exoplanet-feature-distributions&#34;&gt;Exoplanet Feature Distributions&lt;/h2&gt;
&lt;p&gt;The provided visualizations and statistics shed light on the distribution of various exoplanetary features and the observed transit depth at the &lt;code&gt;x1&lt;/code&gt; wavelength.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Radius&lt;/strong&gt;: This feature, with a mean value of approximately 0.7037 and variance of 0.1989, mostly lies between 0.5 and 1.5 as depicted in its histogram.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Temperature&lt;/strong&gt;: Exhibiting a wider spread, the temperature has a mean of approximately 1073.29 K and variance of 154495.74 K².&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logarithmic Concentrations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H₂O&lt;/strong&gt;: Mean concentration of -5.93 with a variance of 3.13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CO₂&lt;/strong&gt;: Mean concentration of -6.87 with a variance of 1.65.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CO&lt;/strong&gt;: Mean concentration of -4.50 with a variance of 0.74.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CH₄&lt;/strong&gt;: Mean concentration of -5.80 with a variance of 3.21.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NH₃&lt;/strong&gt;: Mean concentration of -6.05 with a variance of 3.05.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transit Depth at x1 Wavelength&lt;/strong&gt;: This depth, crucial for exoplanet detection, has an almost singular value near 0, with a mean of approximately 0.0094 and a negligible variance of 0.00006.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These distributions and their accompanying statistics offer invaluable insights into the data&amp;rsquo;s nature and its inherent variability, essential for accurate spectral analysis and interpretation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import required libraries for modeling and SHAP values
import xgboost as xgb
import shap

# Prepare the feature matrix (X) and the target vector (y)
X = df.iloc[:, 1:8]
y = df[&#39;x1&#39;]

# Train an XGBoost model
model = xgb.XGBRegressor(objective =&#39;reg:squarederror&#39;)
model.fit(X, y)

# Initialize the SHAP explainer
explainer = shap.Explainer(model)

# Calculate SHAP values
shap_values = explainer(X)

# Summary plot for SHAP values
shap.summary_plot(shap_values, X, title=&#39;Shapley Feature Importance&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_7_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;SHAP (SHapley Additive exPlanations) values stem from cooperative game theory and provide a way to interpret machine learning model predictions. Each feature in a model is analogous to a player in a game, and the contribution of each feature to a prediction is like the payout a player receives in the game. In SHAP, we calculate the value of each feature by considering all possible combinations (coalitions) of features, assessing the change in prediction with and without that feature. The resulting value, the Shapley value, represents the average contribution of a feature to all possible predictions.&lt;/p&gt;
&lt;p&gt;The summary plot visualizes these values. Each dot represents a SHAP value for a specific instance of a feature; positive values (right of the centerline) indicate that a feature increases the model&amp;rsquo;s output, while negative values (left of the centerline) suggest a decrease. The color depicts the actual value of the feature for the given instance, enabling a comprehensive view of feature influence across the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the Random Forest Regressor and visualization libraries
from sklearn.ensemble import RandomForestRegressor

# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Extract feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    &#39;Feature&#39;: feature_columns,
    &#39;Importance&#39;: feature_importances
}).sort_values(by=&#39;Importance&#39;, ascending=True)

# Create a horizontal bar chart for feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df[&#39;Feature&#39;], importance_df[&#39;Importance&#39;], color=&#39;dodgerblue&#39;)
plt.xlabel(&#39;Importance&#39;)
plt.ylabel(&#39;&#39;)
plt.title(&#39;Random Forest Feature Importance&#39;)
plt.xticks(fontsize=13)  # Increase the font size of the x-axis tick labels
plt.yticks(fontsize=14)  # Increase the font size of the x-axis tick labels
# Save the figure before showing it
plt.savefig(&#39;random_forest_importance_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()



feature_importances
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_9_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71850575, 0.10367982, 0.01655567, 0.07232808, 0.05603188,
       0.0146824 , 0.0182164 ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;random-forest-feature-importance-analysis&#34;&gt;Random Forest Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;To gain insights into which exoplanetary features most influence the observed transit depth, a Random Forest Regressor was utilized. Here&amp;rsquo;s an outline of the procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;: A Random Forest Regressor model was instantiated with 100 trees and a fixed random seed of 42 for reproducibility.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Training&lt;/strong&gt;: The model was trained on the feature set &lt;code&gt;X&lt;/code&gt; and target variable &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Importance Extraction&lt;/strong&gt;: After training, the importance of each feature was extracted using the &lt;code&gt;feature_importances_&lt;/code&gt; attribute of the trained model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Preparation for Visualization&lt;/strong&gt;: A DataFrame was created to house each feature alongside its respective importance. The features were then sorted in ascending order of importance for better visualization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: A horizontal bar chart was plotted to showcase the importance of each feature. The chart offers a clear visual comparison, with the y-axis representing the features and the x-axis indicating their importance. Special attention was paid to font size adjustments for better readability. Furthermore, before displaying the chart, it was saved as a PNG image with high resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resulting visualization, titled &amp;lsquo;Random Forest Feature Importance&amp;rsquo;, provides a clear understanding of the relative significance of each feature in predicting the transit depth, as discerned by the Random Forest model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image
Image(filename=&#39;PME.png&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_11_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;pme-feature-importance-analysis&#34;&gt;PME Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;Using a modeling technique, the impact of different exoplanetary features on the observed transit depth was assessed, and their importance was visualized in the attached figure titled &amp;lsquo;PME Feature Importance&amp;rsquo;. Here&amp;rsquo;s a breakdown of the visual representation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Most Influential Feature&lt;/strong&gt;: The &lt;code&gt;planet_radius&lt;/code&gt; stands out as the most influential feature with the highest PME (Predictive Modeling Estimate) value. This suggests that the radius of the planet plays a pivotal role in determining the observed transit depth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Other Features&lt;/strong&gt;: Logarithmic concentrations of gases, such as &lt;code&gt;log_co2&lt;/code&gt;, &lt;code&gt;log_co&lt;/code&gt;, &lt;code&gt;log_nh3&lt;/code&gt;, &lt;code&gt;log_h2o&lt;/code&gt;, and &lt;code&gt;log_ch4&lt;/code&gt;, also exhibit varying degrees of importance. Among these, &lt;code&gt;log_co2&lt;/code&gt; and &lt;code&gt;log_co&lt;/code&gt; are the more significant contributors compared to others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Least Influential Feature&lt;/strong&gt;: The &lt;code&gt;planet_temp&lt;/code&gt;, representing the temperature of the planet, has the least importance in this analysis, suggesting its minimal role in influencing the transit depth, at least according to the PME metric.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visual Clarity&lt;/strong&gt;: The horizontal bar chart offers a lucid comparison of feature importances. Each bar&amp;rsquo;s length represents the PME value of a feature, providing a direct visual cue to its significance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;: This visualization aids in discerning which exoplanetary characteristics are most relevant when predicting the transit depth using the given model. It can guide future analyses by highlighting key features to focus on or, conversely, those that might be less consequential.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By examining the &amp;lsquo;PME Feature Importance&amp;rsquo; chart, one gains a deeper understanding of the relative significance of each feature in predicting the transit depth within this specific modeling context.&lt;/p&gt;
&lt;h2 id=&#34;uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis&#34;&gt;Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;When delving deep into the realms of uncertainty quantification and feature reduction in predictive modeling, it&amp;rsquo;s crucial to evaluate feature importance metrics critically. The provided PME (Predictive Modeling Estimate) Feature Importance Analysis offers a valuable lens for this task. Below is a nuanced exploration of its significance in the described contexts:&lt;/p&gt;
&lt;h3 id=&#34;uncertainty-quantification&#34;&gt;&lt;strong&gt;Uncertainty Quantification&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Origin of Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In exoplanetary spectral analysis, uncertainty can arise from observational noise, instrumental errors, or intrinsic variability of the observed phenomena. This uncertainty often manifests in the form of error bars in spectra, like the ones shown in transit depth against wavelengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Impact on Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The degree of influence a feature has on the predicted outcome can be a proxy for how that feature might contribute to the overall predictive uncertainty. If a feature like &lt;code&gt;planet_radius&lt;/code&gt; has a high PME value, it might be a critical determinant of transit depth. Any uncertainty in measuring or estimating the &lt;code&gt;planet_radius&lt;/code&gt; could propagate and significantly affect the prediction&amp;rsquo;s reliability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME as a Measure of Stochastic Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The PME values themselves might be obtained by analyzing a model&amp;rsquo;s sensitivity to perturbations in input features. A high PME value indicates that slight changes in the feature can lead to notable changes in the output, thereby implying a greater inherent stochastic uncertainty tied to that feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;feature-reduction&#34;&gt;&lt;strong&gt;Feature Reduction&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identifying Critical Features&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When dealing with a multitude of features, not all may be equally relevant. The PME analysis provides a hierarchy of feature importance. In this case, while &lt;code&gt;planet_radius&lt;/code&gt; emerges as crucial, &lt;code&gt;planet_temp&lt;/code&gt; appears less consequential. This differentiation is fundamental for feature reduction, guiding us on which features to prioritize in modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reducing Dimensionality &amp;amp; Complexity&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In data-driven modeling, especially with limited data points, overfitting is a genuine concern. By understanding which features significantly influence the predictions (like &lt;code&gt;planet_radius&lt;/code&gt; or &lt;code&gt;log_co2&lt;/code&gt;), one can potentially reduce the model&amp;rsquo;s complexity and the risk of overfitting by focusing only on these paramount features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Informing Experimental Design&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If further observational or experimental data is required, knowing feature importances can guide where resources are channeled. For instance, more precise measurements might be sought for features with high PME values, as their accurate estimation is vital for reliable predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trade-off with Predictive Performance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s essential to understand that while feature reduction can simplify models and make them more interpretable, there&amp;rsquo;s always a trade-off with predictive performance. Removing features based on their PME values should be done judiciously, ensuring that the model&amp;rsquo;s predictive capability isn&amp;rsquo;t unduly compromised.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In summary, the &amp;lsquo;PME Feature Importance&amp;rsquo; chart isn&amp;rsquo;t merely a representation of feature significance but serves as a cornerstone for rigorous analytical decisions in uncertainty quantification and feature reduction. Analyzing such importance metrics within the broader context of the problem at hand ensures that models are both robust and interpretable, catering effectively to the dual objectives of predictive accuracy and analytical clarity.&lt;/p&gt;
&lt;h3 id=&#34;weighted-principal-component-analysis-pca-on-exoplanet-data&#34;&gt;Weighted Principal Component Analysis (PCA) on Exoplanet Data&lt;/h3&gt;
&lt;h4 id=&#34;conceptual-overview&#34;&gt;&lt;strong&gt;Conceptual Overview&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;PCA is a method used to emphasize variation and capture strong patterns in a dataset. The &amp;ldquo;weighted PCA&amp;rdquo; approach fine-tunes this by considering the importance of different features, effectively giving more attention to features deemed vital.&lt;/p&gt;
&lt;h4 id=&#34;detailed-breakdown&#34;&gt;&lt;strong&gt;Detailed Breakdown&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Standardization&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before applying PCA, the dataset is standardized to give each feature a mean of 0 and a variance of 1. This is essential because PCA is influenced by the scale of the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Weighted Features&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Features are weighted according to their importance, as identified by the Random Forest model. Taking the square root of the weights ensures proper scaling during matrix multiplication in PCA.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PCA Application&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA projects the data into a new space defined by its principal components. The first two components often hold most of the dataset&amp;rsquo;s variance, making them crucial for visualization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The scatter plot visualizes the data in the space of the first two principal components. The color indicates &lt;code&gt;Transit Depth (x1)&lt;/code&gt;, shedding light on how this parameter varies across the main patterns in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explained Variance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This provides an understanding of how much original variance the first two components capture. A high percentage indicates that the PCA representation retains much of the data&amp;rsquo;s original structure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;significance&#34;&gt;&lt;strong&gt;Significance&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Compression&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA offers a simplified yet rich representation of the data, which can be invaluable for visualization and pattern recognition.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Emphasis&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using feature importances ensures the PCA representation highlights the most critical patterns related to influential features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Framework for Further Exploration&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observing patterns or groupings in the PCA plot can guide subsequent investigations, pinpointing areas of interest or potential clusters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient Data Overview&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The visualization provides a comprehensive but digestible overview of the data, suitable for a wide range of audiences.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In essence, weighted PCA melds the dimensionality reduction capabilities of PCA with the interpretative power of feature importances, offering a profound view into the dataset&amp;rsquo;s intricate structures and relationships.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
# The square root is used because each feature contributes to both rows and columns in the dot product calculation
X_weighted = X_scaled * np.sqrt(feature_importances)

# Perform PCA on the weighted data
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_weighted)

# Plotting the first two principal components
plt.figure(figsize=(10, 7))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
plt.colorbar().set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
plt.xlabel(&#39;Weighted Principal Component 1&#39;)
plt.ylabel(&#39;Weighted Principal Component 2&#39;)
plt.title(&#39;Weighted PCA: First Two Principal Components&#39;)
plt.show()

# Variance explained by the first two principal components
variance_explained = pca.explained_variance_ratio_
variance_explained

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_15_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71985328, 0.10370239])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Your data initialization (this part is assumed, as you haven&#39;t provided it in the original code)
# X, y, feature_importances = ...

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Perform PCA on the weighted data with three components
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_weighted)

# Plotting the first three principal components
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
plt.colorbar(scatter, ax=ax, pad=0.2).set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
ax.set_xlabel(&#39;Weighted Principal Component 1&#39;)
ax.set_ylabel(&#39;Weighted Principal Component 2&#39;)
ax.set_zlabel(&#39;Weighted Principal Component 3&#39;)
plt.title(&#39;Weighted PCA: First Three Principal Components&#39;)
plt.show()

# Variance explained by the first three principal components
variance_explained = pca.explained_variance_ratio_
variance_explained

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_16_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71985328, 0.10370239, 0.07160805])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# side by side
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Your data initialization (this part is assumed, as you haven&#39;t provided it in the original code)
# X, y, feature_importances = ...

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Create a combined figure with subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 7))

# Perform PCA on the weighted data with two components and plot
pca2 = PCA(n_components=2)
X_pca2 = pca2.fit_transform(X_weighted)
scatter_2d = axes[0].scatter(X_pca2[:, 0], X_pca2[:, 1], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
fig.colorbar(scatter_2d, ax=axes[0], orientation=&#39;vertical&#39;).set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
axes[0].set_xlabel(&#39;Weighted Principal Component 1&#39;)
axes[0].set_ylabel(&#39;Weighted Principal Component 2&#39;)
axes[0].set_title(&#39;Weighted PCA: First Two Principal Components&#39;)

# Perform PCA on the weighted data with three components and plot
pca3 = PCA(n_components=3)
X_pca3 = pca3.fit_transform(X_weighted)
ax3d = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;)
scatter_3d = ax3d.scatter(X_pca3[:, 0], X_pca3[:, 1], X_pca3[:, 2], c=y, cmap=&#39;viridis&#39;, edgecolor=&#39;k&#39;, s=40)
fig.colorbar(scatter_3d, ax=ax3d, pad=0.2).set_label(&#39;Transit Depth (x1)&#39;, rotation=270, labelpad=15)
ax3d.set_xlabel(&#39;Weighted Principal Component 1&#39;)
ax3d.set_ylabel(&#39;Weighted Principal Component 2&#39;)
ax3d.set_zlabel(&#39;Weighted Principal Component 3&#39;)
ax3d.set_title(&#39;Weighted PCA: First Three Principal Components&#39;)

# Display the combined plot
plt.tight_layout()
plt.savefig(&#39;wPCA_2D_3D.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_17_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import seaborn as sns

# Your data initialization (this part is assumed, as you haven&#39;t provided it in the original code)
# df, feature_columns, transit_depth_column = ...

# Reconstruct the approximate original data using the first three principal components
reconstructed_data = np.dot(X_pca, pca.components_[:3, :]) + np.mean(X_scaled, axis=0)

# Create a DataFrame for the reconstructed data
reconstructed_df = pd.DataFrame(reconstructed_data, columns=feature_columns)

# Visualize the approximate original histograms
fig, axes = plt.subplots(1, len(feature_columns) + 1, figsize=(18, 4))  # Adjusted for the number of features
for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[i], color=&#39;orange&#39;)
    axes[i].set_title(f&#39;Approx of {col}&#39;)

# Add visualization for actual transit depth (since it was not part of the PCA)
sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1], color=&#39;green&#39;)
axes[-1].set_title(f&#39;Actual {transit_depth_column}&#39;)

plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_18_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
fig, axes = plt.subplots(2, 8, figsize=(18, 8))

# Original Data
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])
    axes[0, i].set_title(f&#39;{col}\nMean: {feature_mean[col]:.2f}\nVariance: {feature_variance[col]:.2f}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])
axes[0, -1].set_title(f&#39;{transit_depth_column}\nMean: {transit_depth_mean:.2f}\nVariance: {transit_depth_variance:.2f}&#39;)

# Reconstructed Data
for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[1, i], color=&#39;orange&#39;)
    axes[1, i].set_title(f&#39;Surrogate of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color=&#39;green&#39;)
axes[1, -1].set_title(f&#39;Actual {transit_depth_column}&#39;)

plt.tight_layout()
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_19_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Assuming data initialization and feature_importances, feature_columns, and transit_depth_column definitions exist

# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Initialize the figure and axes
fig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))

# Original Data histograms
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])
    axes[0, i].set_title(f&#39;Original {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])
axes[0, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Reconstruction using 2 PCs
pca2 = PCA(n_components=2)
X_pca2 = pca2.fit_transform(X_weighted)
reconstructed_data_2PCs = np.dot(X_pca2, pca2.components_[:2, :]) + np.mean(X_scaled, axis=0)
reconstructed_df_2PCs_wPCA = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_2PCs_wPCA[col], bins=20, kde=True, ax=axes[1, i], color=&#39;orange&#39;)
    axes[1, i].set_title(f&#39;2 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color=&#39;green&#39;)
axes[1, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Reconstruction using 3 PCs
pca3 = PCA(n_components=3)
X_pca3 = pca3.fit_transform(X_weighted)
reconstructed_data_3PCs = np.dot(X_pca3, pca3.components_[:3, :]) + np.mean(X_scaled, axis=0)
reconstructed_df_3PCs_wPCA = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_3PCs_wPCA[col], bins=20, kde=True, ax=axes[2, i], color=&#39;purple&#39;)
    axes[2, i].set_title(f&#39;3 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color=&#39;green&#39;)
axes[2, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

plt.tight_layout()
plt.savefig(&#39;wPCA_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_20_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;reconstruction-of-exoplanet-data-using-principal-component-analysis-pca&#34;&gt;Reconstruction of Exoplanet Data Using Principal Component Analysis (PCA)&lt;/h3&gt;
&lt;p&gt;The code segment aims to reconstruct exoplanet data using different numbers of principal components (PCs) from PCA, offering insights into how much information retention occurs as we vary the number of PCs.&lt;/p&gt;
&lt;h4 id=&#34;data-standardization&#34;&gt;&lt;strong&gt;Data Standardization&lt;/strong&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The features undergo standardization, ensuring each feature has a mean of 0 and variance of 1. This normalization is crucial for PCA, as the algorithm is sensitive to varying scales across features.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;weighted-features&#34;&gt;&lt;strong&gt;Weighted Features&lt;/strong&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Features are adjusted based on their significance as ascertained by a prior model, specifically the Random Forest. Weighting the features adjusts the emphasis the PCA places on each feature during the dimensionality reduction process.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data-reconstruction&#34;&gt;&lt;strong&gt;Data Reconstruction&lt;/strong&gt;:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;After performing PCA, the algorithm seeks to transform the reduced data back to the original high-dimensional space. This &amp;ldquo;reconstructed&amp;rdquo; data is an approximation of the original but is built using fewer dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;visualization-of-reconstructions&#34;&gt;&lt;strong&gt;Visualization of Reconstructions&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Original Data Histograms&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The initial histograms show the distributions of the original features and the transit depth.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reconstruction Using 2 Principal Components&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using only the first two PCs, the data is reconstructed and its histograms visualized. This illustrates the patterns and distributions captured when only the first two components are considered.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reconstruction Using 3 Principal Components&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An analogous reconstruction is done with three PCs. The addition of a third dimension might capture more nuanced variations in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;significance-1&#34;&gt;&lt;strong&gt;Significance&lt;/strong&gt;:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Compression vs. Retention&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The visualizations enable us to compare the reconstructions against the original data. We discern how much information is retained and what is lost as we reduce dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Guide for Dimensionality Decision&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By juxtaposing the original with the reconstructions, we gain insights into the optimal number of PCs to use for specific tasks, striking a balance between compression and information retention.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Empirical Understanding&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These histograms and visual representations offer a tangible way to grasp the abstract notion of dimensionality reduction. They elucidate how PCA captures the essence of the data while diminishing dimensions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In conclusion, this analysis, coupled with the visualizations, equips us with a robust understanding of how PCA reconstructs data using varying numbers of components. It underscores the trade-offs involved and the implications of choosing specific dimensionality levels in data-driven tasks.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.decomposition import KernelPCA

# Apply Kernel PCA on the weighted data
kpca = KernelPCA(n_components=2, kernel=&#39;rbf&#39;)  
X_kpca = kpca.fit_transform(X_weighted)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;choice-of-kernels&#34;&gt;Choice of Kernels:&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;kernel&amp;rsquo; parameter in &amp;lsquo;KernelPCA&amp;rsquo; allows you to specify the desired kernel. Available options include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;rsquo; linear&amp;rsquo; : Linear kernel&lt;/li&gt;
&lt;li&gt;&amp;lsquo;poly&amp;rsquo; : Polynomial kernel&lt;/li&gt;
&lt;li&gt;&amp;rsquo; rbf &amp;lsquo;: Radial basis function (RBF) or Gaussian kernel&lt;/li&gt;
&lt;li&gt;&amp;rsquo; sigmoid &amp;lsquo;: Sigmoid kernel&lt;/li&gt;
&lt;li&gt;&amp;lsquo;cosine&amp;rsquo; : Cosine similarity&lt;/li&gt;
&lt;li&gt;&amp;rsquo; precomputed &amp;lsquo;: If you have precomputed the kernel matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import KernelPCA
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Initialize the figure and axes
fig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))

# Original Data histograms
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])
    axes[0, i].set_title(f&#39;Original {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])
axes[0, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Kernel PCA with 2 components
kpca2 = KernelPCA(n_components=2, kernel=&#39;rbf&#39;, gamma=0.1, fit_inverse_transform=True)  

X_kpca2 = kpca2.fit_transform(X_weighted)
reconstructed_data_2PCs = kpca2.inverse_transform(X_kpca2)
reconstructed_df_2PCs_kPCA_rbf = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_2PCs_kPCA_rbf[col], bins=20, kde=True, ax=axes[1, i], color=&#39;orange&#39;)
    axes[1, i].set_title(f&#39;2 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color=&#39;green&#39;)
axes[1, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Kernel PCA with 3 components
kpca3 = KernelPCA(n_components=3, kernel=&#39;rbf&#39;, gamma=0.1, fit_inverse_transform=True) 

X_kpca3 = kpca3.fit_transform(X_weighted)
reconstructed_data_3PCs = kpca3.inverse_transform(X_kpca3)
reconstructed_df_3PCs_kPCA_rbf = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_3PCs_kPCA_rbf[col], bins=20, kde=True, ax=axes[2, i], color=&#39;purple&#39;)
    axes[2, i].set_title(f&#39;3 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color=&#39;green&#39;)
axes[2, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

plt.tight_layout()
plt.savefig(&#39;wKPCA_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_25_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Standardize the feature matrix
X_scaled = StandardScaler().fit_transform(X)

# Multiply each feature by its square root of importance weight for weighted PCA
X_weighted = X_scaled * np.sqrt(feature_importances)

# Initialize the figure and axes
fig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))

# Original Data histograms
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])
    axes[0, i].set_title(f&#39;Original {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])
axes[0, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Kernel PCA with 2 components
kpca2 = KernelPCA(n_components=2, kernel=&#39;poly&#39;, degree=2, coef0=1, fit_inverse_transform=True)  

X_kpca2 = kpca2.fit_transform(X_weighted)
reconstructed_data_2PCs = kpca2.inverse_transform(X_kpca2)
reconstructed_df_2PCs_kPCA_poly = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_2PCs_kPCA_poly[col], bins=20, kde=True, ax=axes[1, i], color=&#39;orange&#39;)
    axes[1, i].set_title(f&#39;2 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color=&#39;green&#39;)
axes[1, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

# Kernel PCA with 3 components
kpca3 = KernelPCA(n_components=3, kernel=&#39;poly&#39;, degree=2, coef0=1, fit_inverse_transform=True) 

X_kpca3 = kpca3.fit_transform(X_weighted)
reconstructed_data_3PCs = kpca3.inverse_transform(X_kpca3)
reconstructed_df_3PCs_kPCA_poly = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)

for i, col in enumerate(feature_columns):
    sns.histplot(reconstructed_df_3PCs_kPCA_poly[col], bins=20, kde=True, ax=axes[2, i], color=&#39;purple&#39;)
    axes[2, i].set_title(f&#39;3 PCs of {col}&#39;)

sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color=&#39;green&#39;)
axes[2, -1].set_title(f&#39;Original {transit_depth_column}&#39;)

plt.tight_layout()
plt.savefig(&#39;wKPCA_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=500)
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_26_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error

mse_wPCA = mean_squared_error(X_scaled, reconstructed_df_3PCs_wPCA)
mse_rbf = mean_squared_error(X_scaled, reconstructed_df_3PCs_kPCA_rbf)
mse_poly = mean_squared_error(X_scaled, reconstructed_df_3PCs_kPCA_poly)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

# Creating a dictionary with the methods and their respective MSEs
data = {
    &#39;Method&#39;: [&#39;Weighted PCA&#39;, &#39;Kernel PCA (RBF)&#39;, &#39;Kernel PCA (Poly)&#39;],
    &#39;MSE (3 PCs)&#39;: [
        mean_squared_error(X_scaled, reconstructed_df_3PCs_wPCA),
        mean_squared_error(X_scaled, reconstructed_df_3PCs_kPCA_rbf),
        mean_squared_error(X_scaled, reconstructed_df_3PCs_kPCA_poly)
    ]
}

# Converting the dictionary to a pandas DataFrame
mse_table = pd.DataFrame(data)

# Displaying the table
print(mse_table)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;              Method  MSE (3 PCs)
0       Weighted PCA     0.715436
1   Kernel PCA (RBF)     0.795583
2  Kernel PCA (Poly)     0.736338
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

df_original = pd.DataFrame(X_scaled, columns=feature_columns)
correlation_original = df_original.corr()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;correlation_wPCA_3PCs = reconstructed_df_3PCs_wPCA.corr()
correlation_kPCA_rbf_3PCs = reconstructed_df_3PCs_kPCA_rbf.corr()
correlation_kPCA_poly_3PCs = reconstructed_df_3PCs_kPCA_poly.corr()

correlation_wPCA_2PCs = reconstructed_df_2PCs_wPCA.corr()
correlation_kPCA_rbf_2PCs = reconstructed_df_2PCs_kPCA_rbf.corr()
correlation_kPCA_poly_2PCs = reconstructed_df_2PCs_kPCA_poly.corr()


&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import seaborn as sns
import matplotlib.pyplot as plt

# Function to visualize correlation matrix
def visualize_corr(corr_matrix, title):
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, cmap=&#39;coolwarm&#39;, annot=True, fmt=&amp;quot;.2f&amp;quot;, linewidths=0.5)
    plt.title(title)
    plt.show()

# Visualize original data&#39;s correlation matrix
visualize_corr(correlation_original, &amp;quot;Original Data Correlation Matrix&amp;quot;)

# Visualize reconstructed data&#39;s correlation matrices for 3 PCs
visualize_corr(correlation_wPCA_3PCs, &amp;quot;3 PCs Weighted PCA Correlation Matrix&amp;quot;)
visualize_corr(correlation_kPCA_rbf_3PCs, &amp;quot;3 PCs Kernel PCA (RBF) Correlation Matrix&amp;quot;)
visualize_corr(correlation_kPCA_poly_3PCs, &amp;quot;3 PCs Kernel PCA (Poly) Correlation Matrix&amp;quot;)



&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_31_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_31_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_31_2.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_31_3.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.gridspec as gridspec

# Load images
img_shap = mpimg.imread(&#39;shapley.png&#39;)
img_rf = mpimg.imread(&#39;random_forest_importance_plot.png&#39;)
img_pme = mpimg.imread(&#39;PME.png&#39;)

# Create a grid for the subplots
fig_combined = plt.figure(figsize=(20, 12))
gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])

# Display SHAP plot
ax0 = plt.subplot(gs[0])
ax0.imshow(img_shap)
ax0.axis(&#39;off&#39;)

# Display RF Importance plot
ax1 = plt.subplot(gs[1])
ax1.imshow(img_rf)
ax1.axis(&#39;off&#39;)

# Display PME image in the middle of the 2nd row
ax2 = plt.subplot(gs[2:4])  # This makes the PME plot span both columns on the second row
ax2.imshow(img_pme)
ax2.axis(&#39;off&#39;)

plt.tight_layout()
plt.savefig(&#39;sensitivity_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_32_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This code snippet consolidates and displays three distinct plots—SHAP values, Random Forest Feature Importance, and PME Feature Importance—into a single visualization. The images are loaded and arranged in a 2x2 grid, with the SHAP and Random Forest plots on the top row, and the PME plot spanning both columns on the bottom row. After layout adjustments, the combined visualization is saved as a high-resolution PNG image titled &amp;lsquo;sensitivity_combined_plot.png&amp;rsquo; and then displayed.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;Changeat, Q., &amp;amp; Yip, K. H. (2023). ESA-Ariel Data Challenge NeurIPS 2022: Introduction to exo-atmospheric studies and presentation of the Atmospheric Big Challenge (ABC) Database. &lt;em&gt;arXiv preprint arXiv:2206.14633&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Herin, M., Il Idrissi, M., Chabridon, V., &amp;amp; Iooss, B. (2022). Proportional marginal effects for global sensitivity analysis. arXiv preprint arXiv:2210.13065.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exoplanet Detection: Informed Parameters &amp; Spectral Error Minimization</title>
      <link>/project/internal-project/exoplanet-modeling/</link>
      <pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/exoplanet-modeling/</guid>
      <description>&lt;p&gt;The figure below showcases a spectrum integral to exoplanet detection, charting the transit depth against the wavelength in microns. Every observation point in the spectrum carries an inherent uncertainty, denoted by the vertical error bars. To decode and potentially minimize this uncertainty, it&amp;rsquo;s pivotal to fathom how features like &lt;code&gt;planet radius&lt;/code&gt;, &lt;code&gt;planet temperature&lt;/code&gt;, and the logarithmic concentrations of &lt;code&gt;H₂O&lt;/code&gt;, &lt;code&gt;CO₂&lt;/code&gt;, &lt;code&gt;CO&lt;/code&gt;, &lt;code&gt;CH₄&lt;/code&gt;, and &lt;code&gt;NH₃&lt;/code&gt; influence the transit depth. Leveraging interpretative tools like SHAP can provide insights into how these exoplanetary features impact the observed transit depth, refining our understanding and accuracy in exoplanet spectral analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image
Image(filename=&#39;my_spectrum.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_2_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the CSV file into a Pandas DataFrame
df = pd.read_csv(&amp;quot;small_astro.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_backup = df.copy()
df_backup.head(10)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;planet_radius&lt;/th&gt;
      &lt;th&gt;planet_temp&lt;/th&gt;
      &lt;th&gt;log_h2o&lt;/th&gt;
      &lt;th&gt;log_co2&lt;/th&gt;
      &lt;th&gt;log_co&lt;/th&gt;
      &lt;th&gt;log_ch4&lt;/th&gt;
      &lt;th&gt;log_nh3&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;x43&lt;/th&gt;
      &lt;th&gt;x44&lt;/th&gt;
      &lt;th&gt;x45&lt;/th&gt;
      &lt;th&gt;x46&lt;/th&gt;
      &lt;th&gt;x47&lt;/th&gt;
      &lt;th&gt;x48&lt;/th&gt;
      &lt;th&gt;x49&lt;/th&gt;
      &lt;th&gt;x50&lt;/th&gt;
      &lt;th&gt;x51&lt;/th&gt;
      &lt;th&gt;x52&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.559620&lt;/td&gt;
      &lt;td&gt;863.394770&lt;/td&gt;
      &lt;td&gt;-8.865868&lt;/td&gt;
      &lt;td&gt;-6.700707&lt;/td&gt;
      &lt;td&gt;-5.557561&lt;/td&gt;
      &lt;td&gt;-8.957615&lt;/td&gt;
      &lt;td&gt;-3.097540&lt;/td&gt;
      &lt;td&gt;0.003836&lt;/td&gt;
      &lt;td&gt;0.003834&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.003938&lt;/td&gt;
      &lt;td&gt;0.003941&lt;/td&gt;
      &lt;td&gt;0.003903&lt;/td&gt;
      &lt;td&gt;0.003931&lt;/td&gt;
      &lt;td&gt;0.003983&lt;/td&gt;
      &lt;td&gt;0.004019&lt;/td&gt;
      &lt;td&gt;0.004046&lt;/td&gt;
      &lt;td&gt;0.004072&lt;/td&gt;
      &lt;td&gt;0.004054&lt;/td&gt;
      &lt;td&gt;0.004056&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1.118308&lt;/td&gt;
      &lt;td&gt;1201.700465&lt;/td&gt;
      &lt;td&gt;-4.510258&lt;/td&gt;
      &lt;td&gt;-8.228966&lt;/td&gt;
      &lt;td&gt;-3.565427&lt;/td&gt;
      &lt;td&gt;-7.807424&lt;/td&gt;
      &lt;td&gt;-3.633658&lt;/td&gt;
      &lt;td&gt;0.015389&lt;/td&gt;
      &lt;td&gt;0.015148&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.015450&lt;/td&gt;
      &lt;td&gt;0.015447&lt;/td&gt;
      &lt;td&gt;0.015461&lt;/td&gt;
      &lt;td&gt;0.015765&lt;/td&gt;
      &lt;td&gt;0.016099&lt;/td&gt;
      &lt;td&gt;0.016376&lt;/td&gt;
      &lt;td&gt;0.016549&lt;/td&gt;
      &lt;td&gt;0.016838&lt;/td&gt;
      &lt;td&gt;0.016781&lt;/td&gt;
      &lt;td&gt;0.016894&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.400881&lt;/td&gt;
      &lt;td&gt;1556.096477&lt;/td&gt;
      &lt;td&gt;-7.225472&lt;/td&gt;
      &lt;td&gt;-6.931472&lt;/td&gt;
      &lt;td&gt;-3.081975&lt;/td&gt;
      &lt;td&gt;-8.567854&lt;/td&gt;
      &lt;td&gt;-5.378472&lt;/td&gt;
      &lt;td&gt;0.002089&lt;/td&gt;
      &lt;td&gt;0.002073&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.001989&lt;/td&gt;
      &lt;td&gt;0.002168&lt;/td&gt;
      &lt;td&gt;0.002176&lt;/td&gt;
      &lt;td&gt;0.002123&lt;/td&gt;
      &lt;td&gt;0.002079&lt;/td&gt;
      &lt;td&gt;0.002081&lt;/td&gt;
      &lt;td&gt;0.002106&lt;/td&gt;
      &lt;td&gt;0.002167&lt;/td&gt;
      &lt;td&gt;0.002149&lt;/td&gt;
      &lt;td&gt;0.002185&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.345974&lt;/td&gt;
      &lt;td&gt;1268.624884&lt;/td&gt;
      &lt;td&gt;-7.461157&lt;/td&gt;
      &lt;td&gt;-5.853334&lt;/td&gt;
      &lt;td&gt;-3.044711&lt;/td&gt;
      &lt;td&gt;-5.149378&lt;/td&gt;
      &lt;td&gt;-3.815568&lt;/td&gt;
      &lt;td&gt;0.002523&lt;/td&gt;
      &lt;td&gt;0.002392&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.002745&lt;/td&gt;
      &lt;td&gt;0.003947&lt;/td&gt;
      &lt;td&gt;0.004296&lt;/td&gt;
      &lt;td&gt;0.003528&lt;/td&gt;
      &lt;td&gt;0.003352&lt;/td&gt;
      &lt;td&gt;0.003629&lt;/td&gt;
      &lt;td&gt;0.003929&lt;/td&gt;
      &lt;td&gt;0.004363&lt;/td&gt;
      &lt;td&gt;0.004216&lt;/td&gt;
      &lt;td&gt;0.004442&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.733184&lt;/td&gt;
      &lt;td&gt;1707.323564&lt;/td&gt;
      &lt;td&gt;-4.140844&lt;/td&gt;
      &lt;td&gt;-7.460278&lt;/td&gt;
      &lt;td&gt;-3.181793&lt;/td&gt;
      &lt;td&gt;-5.996593&lt;/td&gt;
      &lt;td&gt;-4.535345&lt;/td&gt;
      &lt;td&gt;0.002957&lt;/td&gt;
      &lt;td&gt;0.002924&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.003402&lt;/td&gt;
      &lt;td&gt;0.003575&lt;/td&gt;
      &lt;td&gt;0.003667&lt;/td&gt;
      &lt;td&gt;0.003740&lt;/td&gt;
      &lt;td&gt;0.003823&lt;/td&gt;
      &lt;td&gt;0.003904&lt;/td&gt;
      &lt;td&gt;0.003897&lt;/td&gt;
      &lt;td&gt;0.004004&lt;/td&gt;
      &lt;td&gt;0.004111&lt;/td&gt;
      &lt;td&gt;0.004121&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.161165&lt;/td&gt;
      &lt;td&gt;620.185809&lt;/td&gt;
      &lt;td&gt;-4.875000&lt;/td&gt;
      &lt;td&gt;-5.074766&lt;/td&gt;
      &lt;td&gt;-3.861240&lt;/td&gt;
      &lt;td&gt;-5.388011&lt;/td&gt;
      &lt;td&gt;-8.390503&lt;/td&gt;
      &lt;td&gt;0.000444&lt;/td&gt;
      &lt;td&gt;0.000442&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000432&lt;/td&gt;
      &lt;td&gt;0.000486&lt;/td&gt;
      &lt;td&gt;0.000473&lt;/td&gt;
      &lt;td&gt;0.000462&lt;/td&gt;
      &lt;td&gt;0.000447&lt;/td&gt;
      &lt;td&gt;0.000455&lt;/td&gt;
      &lt;td&gt;0.000455&lt;/td&gt;
      &lt;td&gt;0.000457&lt;/td&gt;
      &lt;td&gt;0.000463&lt;/td&gt;
      &lt;td&gt;0.000474&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.194312&lt;/td&gt;
      &lt;td&gt;900.597575&lt;/td&gt;
      &lt;td&gt;-8.299899&lt;/td&gt;
      &lt;td&gt;-6.850709&lt;/td&gt;
      &lt;td&gt;-4.314491&lt;/td&gt;
      &lt;td&gt;-3.712038&lt;/td&gt;
      &lt;td&gt;-3.951455&lt;/td&gt;
      &lt;td&gt;0.001794&lt;/td&gt;
      &lt;td&gt;0.001721&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.001048&lt;/td&gt;
      &lt;td&gt;0.001052&lt;/td&gt;
      &lt;td&gt;0.000948&lt;/td&gt;
      &lt;td&gt;0.000976&lt;/td&gt;
      &lt;td&gt;0.001122&lt;/td&gt;
      &lt;td&gt;0.001274&lt;/td&gt;
      &lt;td&gt;0.001395&lt;/td&gt;
      &lt;td&gt;0.001522&lt;/td&gt;
      &lt;td&gt;0.001456&lt;/td&gt;
      &lt;td&gt;0.001823&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;1.132685&lt;/td&gt;
      &lt;td&gt;1176.443900&lt;/td&gt;
      &lt;td&gt;-6.765865&lt;/td&gt;
      &lt;td&gt;-7.398548&lt;/td&gt;
      &lt;td&gt;-3.378307&lt;/td&gt;
      &lt;td&gt;-3.763737&lt;/td&gt;
      &lt;td&gt;-5.881384&lt;/td&gt;
      &lt;td&gt;0.012950&lt;/td&gt;
      &lt;td&gt;0.012946&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.014019&lt;/td&gt;
      &lt;td&gt;0.013871&lt;/td&gt;
      &lt;td&gt;0.013810&lt;/td&gt;
      &lt;td&gt;0.013902&lt;/td&gt;
      &lt;td&gt;0.014024&lt;/td&gt;
      &lt;td&gt;0.014150&lt;/td&gt;
      &lt;td&gt;0.014298&lt;/td&gt;
      &lt;td&gt;0.014392&lt;/td&gt;
      &lt;td&gt;0.014401&lt;/td&gt;
      &lt;td&gt;0.015042&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.158621&lt;/td&gt;
      &lt;td&gt;1189.209841&lt;/td&gt;
      &lt;td&gt;-8.376041&lt;/td&gt;
      &lt;td&gt;-6.321977&lt;/td&gt;
      &lt;td&gt;-3.243900&lt;/td&gt;
      &lt;td&gt;-8.711851&lt;/td&gt;
      &lt;td&gt;-3.449195&lt;/td&gt;
      &lt;td&gt;0.000444&lt;/td&gt;
      &lt;td&gt;0.000445&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.000562&lt;/td&gt;
      &lt;td&gt;0.000595&lt;/td&gt;
      &lt;td&gt;0.000571&lt;/td&gt;
      &lt;td&gt;0.000590&lt;/td&gt;
      &lt;td&gt;0.000628&lt;/td&gt;
      &lt;td&gt;0.000663&lt;/td&gt;
      &lt;td&gt;0.000692&lt;/td&gt;
      &lt;td&gt;0.000734&lt;/td&gt;
      &lt;td&gt;0.000718&lt;/td&gt;
      &lt;td&gt;0.000736&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.660642&lt;/td&gt;
      &lt;td&gt;528.023669&lt;/td&gt;
      &lt;td&gt;-3.804286&lt;/td&gt;
      &lt;td&gt;-8.919378&lt;/td&gt;
      &lt;td&gt;-4.686964&lt;/td&gt;
      &lt;td&gt;-8.150277&lt;/td&gt;
      &lt;td&gt;-3.068319&lt;/td&gt;
      &lt;td&gt;0.008997&lt;/td&gt;
      &lt;td&gt;0.009035&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.009435&lt;/td&gt;
      &lt;td&gt;0.009375&lt;/td&gt;
      &lt;td&gt;0.009315&lt;/td&gt;
      &lt;td&gt;0.009357&lt;/td&gt;
      &lt;td&gt;0.009563&lt;/td&gt;
      &lt;td&gt;0.009739&lt;/td&gt;
      &lt;td&gt;0.009821&lt;/td&gt;
      &lt;td&gt;0.009890&lt;/td&gt;
      &lt;td&gt;0.009819&lt;/td&gt;
      &lt;td&gt;0.009734&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;10 rows × 60 columns&lt;/p&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Columns of interest for planetary and chemical properties and transit depth
feature_columns = df.columns[1:8]
transit_depth_column = &#39;x1&#39; # pick the first wavelength

# Calculate mean and variance for features and transit depth
feature_mean = df[feature_columns].mean()
feature_variance = df[feature_columns].var()
transit_depth_mean = df[transit_depth_column].mean()
transit_depth_variance = df[transit_depth_column].var()

# Visualize the distributions
fig, axes = plt.subplots(1, 8, figsize=(18, 4))
for i, col in enumerate(feature_columns):
    sns.histplot(df[col], bins=20, kde=True, ax=axes[i])
    axes[i].set_title(f&#39;{col}\nMean: {feature_mean[col]:.2f}\nVariance: {feature_variance[col]:.2f}&#39;)

# Add visualization for transit depth
sns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1])
axes[-1].set_title(f&#39;{transit_depth_column}\nMean: {transit_depth_mean:.2f}\nVariance: {transit_depth_variance:.2f}&#39;)

plt.tight_layout()
plt.show()

feature_mean, feature_variance, transit_depth_mean, transit_depth_variance
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_5_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(planet_radius       0.703714
 planet_temp      1073.229674
 log_h2o            -5.934889
 log_co2            -6.873009
 log_co             -4.497141
 log_ch4            -5.799850
 log_nh3            -6.051791
 dtype: float64,
 planet_radius         0.198990
 planet_temp      154495.743225
 log_h2o               3.130868
 log_co2               1.649658
 log_co                0.738255
 log_ch4               3.208283
 log_nh3               3.050545
 dtype: float64,
 0.009442470522591322,
 0.00016172106267489707)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;exoplanet-feature-distributions&#34;&gt;Exoplanet Feature Distributions&lt;/h2&gt;
&lt;p&gt;The provided visualizations and statistics shed light on the distribution of various exoplanetary features and the observed transit depth at the &lt;code&gt;x1&lt;/code&gt; wavelength.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Radius&lt;/strong&gt;: This feature, with a mean value of approximately 0.7037 and variance of 0.1989, mostly lies between 0.5 and 1.5 as depicted in its histogram.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Planet Temperature&lt;/strong&gt;: Exhibiting a wider spread, the temperature has a mean of approximately 1073.29 K and variance of 154495.74 K².&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logarithmic Concentrations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H₂O&lt;/strong&gt;: Mean concentration of -5.93 with a variance of 3.13.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CO₂&lt;/strong&gt;: Mean concentration of -6.87 with a variance of 1.65.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CO&lt;/strong&gt;: Mean concentration of -4.50 with a variance of 0.74.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CH₄&lt;/strong&gt;: Mean concentration of -5.80 with a variance of 3.21.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NH₃&lt;/strong&gt;: Mean concentration of -6.05 with a variance of 3.05.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transit Depth at x1 Wavelength&lt;/strong&gt;: This depth, crucial for exoplanet detection, has an almost singular value near 0, with a mean of approximately 0.0094 and a negligible variance of 0.00006.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These distributions and their accompanying statistics offer invaluable insights into the data&amp;rsquo;s nature and its inherent variability, essential for accurate spectral analysis and interpretation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import required libraries for modeling and SHAP values
import xgboost as xgb
import shap

# Prepare the feature matrix (X) and the target vector (y)
X = df.iloc[:, 1:8]
y = df[&#39;x1&#39;]

# Train an XGBoost model
model = xgb.XGBRegressor(objective =&#39;reg:squarederror&#39;)
model.fit(X, y)

# Initialize the SHAP explainer
explainer = shap.Explainer(model)

# Calculate SHAP values
shap_values = explainer(X)

# Summary plot for SHAP values
shap.summary_plot(shap_values, X, title=&#39;Shapley Feature Importance&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_7_1.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;SHAP (SHapley Additive exPlanations) values stem from cooperative game theory and provide a way to interpret machine learning model predictions. Each feature in a model is analogous to a player in a game, and the contribution of each feature to a prediction is like the payout a player receives in the game. In SHAP, we calculate the value of each feature by considering all possible combinations (coalitions) of features, assessing the change in prediction with and without that feature. The resulting value, the Shapley value, represents the average contribution of a feature to all possible predictions.&lt;/p&gt;
&lt;p&gt;The summary plot visualizes these values. Each dot represents a SHAP value for a specific instance of a feature; positive values (right of the centerline) indicate that a feature increases the model&amp;rsquo;s output, while negative values (left of the centerline) suggest a decrease. The color depicts the actual value of the feature for the given instance, enabling a comprehensive view of feature influence across the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import the Random Forest Regressor and visualization libraries
from sklearn.ensemble import RandomForestRegressor

# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Extract feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    &#39;Feature&#39;: feature_columns,
    &#39;Importance&#39;: feature_importances
}).sort_values(by=&#39;Importance&#39;, ascending=True)

# Create a horizontal bar chart for feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df[&#39;Feature&#39;], importance_df[&#39;Importance&#39;], color=&#39;dodgerblue&#39;)
plt.xlabel(&#39;Importance&#39;)
plt.ylabel(&#39;&#39;)
plt.title(&#39;Random Forest Feature Importance&#39;)
plt.xticks(fontsize=13)  # Increase the font size of the x-axis tick labels
plt.yticks(fontsize=14)  # Increase the font size of the x-axis tick labels
# Save the figure before showing it
plt.savefig(&#39;random_forest_importance_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()



feature_importances
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_9_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([0.71850575, 0.10367982, 0.01655567, 0.07232808, 0.05603188,
       0.0146824 , 0.0182164 ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;random-forest-feature-importance-analysis&#34;&gt;Random Forest Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;To gain insights into which exoplanetary features most influence the observed transit depth, a Random Forest Regressor was utilized. Here&amp;rsquo;s an outline of the procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;: A Random Forest Regressor model was instantiated with 100 trees and a fixed random seed of 42 for reproducibility.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Training&lt;/strong&gt;: The model was trained on the feature set &lt;code&gt;X&lt;/code&gt; and target variable &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Importance Extraction&lt;/strong&gt;: After training, the importance of each feature was extracted using the &lt;code&gt;feature_importances_&lt;/code&gt; attribute of the trained model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Preparation for Visualization&lt;/strong&gt;: A DataFrame was created to house each feature alongside its respective importance. The features were then sorted in ascending order of importance for better visualization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: A horizontal bar chart was plotted to showcase the importance of each feature. The chart offers a clear visual comparison, with the y-axis representing the features and the x-axis indicating their importance. Special attention was paid to font size adjustments for better readability. Furthermore, before displaying the chart, it was saved as a PNG image with high resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The resulting visualization, titled &amp;lsquo;Random Forest Feature Importance&amp;rsquo;, provides a clear understanding of the relative significance of each feature in predicting the transit depth, as discerned by the Random Forest model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.display import Image
Image(filename=&#39;PME.png&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_11_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;pme-feature-importance-analysis&#34;&gt;PME Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;Using a modeling technique, the impact of different exoplanetary features on the observed transit depth was assessed, and their importance was visualized in the attached figure titled &amp;lsquo;PME Feature Importance&amp;rsquo;. Here&amp;rsquo;s a breakdown of the visual representation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Most Influential Feature&lt;/strong&gt;: The &lt;code&gt;planet_radius&lt;/code&gt; stands out as the most influential feature with the highest PME (Proportional Marginal Effects) value. This suggests that the radius of the planet plays a pivotal role in determining the observed transit depth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Other Features&lt;/strong&gt;: Logarithmic concentrations of gases, such as &lt;code&gt;log_co2&lt;/code&gt;, &lt;code&gt;log_co&lt;/code&gt;, &lt;code&gt;log_nh3&lt;/code&gt;, &lt;code&gt;log_h2o&lt;/code&gt;, and &lt;code&gt;log_ch4&lt;/code&gt;, also exhibit varying degrees of importance. Among these, &lt;code&gt;log_co2&lt;/code&gt; and &lt;code&gt;log_co&lt;/code&gt; are the more significant contributors compared to others.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Least Influential Feature&lt;/strong&gt;: The &lt;code&gt;planet_temp&lt;/code&gt;, representing the temperature of the planet, has the least importance in this analysis, suggesting its minimal role in influencing the transit depth, at least according to the PME metric.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Visual Clarity&lt;/strong&gt;: The horizontal bar chart offers a lucid comparison of feature importances. Each bar&amp;rsquo;s length represents the PME value of a feature, providing a direct visual cue to its significance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interpretation&lt;/strong&gt;: This visualization aids in discerning which exoplanetary characteristics are most relevant when predicting the transit depth using the given model. It can guide future analyses by highlighting key features to focus on or, conversely, those that might be less consequential.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By examining the &amp;lsquo;PME Feature Importance&amp;rsquo; chart, one gains a deeper understanding of the relative significance of each feature in predicting the transit depth within this specific modeling context.&lt;/p&gt;
&lt;h2 id=&#34;uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis&#34;&gt;Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis&lt;/h2&gt;
&lt;p&gt;When delving deep into the realms of uncertainty quantification and feature reduction in predictive modeling, it&amp;rsquo;s crucial to evaluate feature importance metrics critically. The provided PME (Proportional Marginal Effects) Feature Importance Analysis offers a valuable lens for this task. Below is a nuanced exploration of its significance in the described contexts:&lt;/p&gt;
&lt;h3 id=&#34;uncertainty-quantification&#34;&gt;&lt;strong&gt;Uncertainty Quantification&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Origin of Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In exoplanetary spectral analysis, uncertainty can arise from observational noise, instrumental errors, or intrinsic variability of the observed phenomena. This uncertainty often manifests in the form of error bars in spectra, like the ones shown in transit depth against wavelengths.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Impact on Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The degree of influence a feature has on the predicted outcome can be a proxy for how that feature might contribute to the overall predictive uncertainty. If a feature like &lt;code&gt;planet_radius&lt;/code&gt; has a high PME value, it might be a critical determinant of transit depth. Any uncertainty in measuring or estimating the &lt;code&gt;planet_radius&lt;/code&gt; could propagate and significantly affect the prediction&amp;rsquo;s reliability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;PME as a Measure of Stochastic Uncertainty&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The PME values themselves might be obtained by analyzing a model&amp;rsquo;s sensitivity to perturbations in input features. A high PME value indicates that slight changes in the feature can lead to notable changes in the output, thereby implying a greater inherent stochastic uncertainty tied to that feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;feature-reduction&#34;&gt;&lt;strong&gt;Feature Reduction&lt;/strong&gt;:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identifying Critical Features&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When dealing with a multitude of features, not all may be equally relevant. The PME analysis provides a hierarchy of feature importance. In this case, while &lt;code&gt;planet_radius&lt;/code&gt; emerges as crucial, &lt;code&gt;planet_temp&lt;/code&gt; appears less consequential. This differentiation is fundamental for feature reduction, guiding us on which features to prioritize in modeling.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reducing Dimensionality &amp;amp; Complexity&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In data-driven modeling, especially with limited data points, overfitting is a genuine concern. By understanding which features significantly influence the predictions (like &lt;code&gt;planet_radius&lt;/code&gt; or &lt;code&gt;log_co2&lt;/code&gt;), one can potentially reduce the model&amp;rsquo;s complexity and the risk of overfitting by focusing only on these paramount features.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Informing Experimental Design&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If further observational or experimental data is required, knowing feature importances can guide where resources are channeled. For instance, more precise measurements might be sought for features with high PME values, as their accurate estimation is vital for reliable predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trade-off with Predictive Performance&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s essential to understand that while feature reduction can simplify models and make them more interpretable, there&amp;rsquo;s always a trade-off with predictive performance. Removing features based on their PME values should be done judiciously, ensuring that the model&amp;rsquo;s predictive capability isn&amp;rsquo;t unduly compromised.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In summary, the &amp;lsquo;PME Feature Importance&amp;rsquo; chart isn&amp;rsquo;t merely a representation of feature significance but serves as a cornerstone for rigorous analytical decisions in uncertainty quantification and feature reduction. Analyzing such importance metrics within the broader context of the problem at hand ensures that models are both robust and interpretable, catering effectively to the dual objectives of predictive accuracy and analytical clarity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.gridspec as gridspec

# Load images
img_shap = mpimg.imread(&#39;shapley.png&#39;)
img_rf = mpimg.imread(&#39;random_forest_importance_plot.png&#39;)
img_pme = mpimg.imread(&#39;PME.png&#39;)

# Create a grid for the subplots
fig_combined = plt.figure(figsize=(20, 12))
gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])

# Display SHAP plot
ax0 = plt.subplot(gs[0])
ax0.imshow(img_shap)
ax0.axis(&#39;off&#39;)

# Display RF Importance plot
ax1 = plt.subplot(gs[1])
ax1.imshow(img_rf)
ax1.axis(&#39;off&#39;)

# Display PME image in the middle of the 2nd row
ax2 = plt.subplot(gs[2:4])  # This makes the PME plot span both columns on the second row
ax2.imshow(img_pme)
ax2.axis(&#39;off&#39;)

plt.tight_layout()
plt.savefig(&#39;sensitivity_combined_plot.png&#39;, bbox_inches=&#39;tight&#39;, dpi=300)  # &#39;bbox_inches&#39; ensures the entire plot is saved

plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./research_statement_22_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This code snippet consolidates and displays three distinct plots—SHAP values, Random Forest Feature Importance, and PME Feature Importance—into a single visualization. The images are loaded and arranged in a 2x2 grid, with the SHAP and Random Forest plots on the top row, and the PME plot spanning both columns on the bottom row. After layout adjustments, the combined visualization is saved as a high-resolution PNG image titled &amp;lsquo;sensitivity_combined_plot.png&amp;rsquo; and then displayed.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;p&gt;Changeat, Q., &amp;amp; Yip, K. H. (2023). ESA-Ariel Data Challenge NeurIPS 2022: Introduction to exo-atmospheric studies and presentation of the Atmospheric Big Challenge (ABC) Database. &lt;em&gt;arXiv preprint arXiv:2206.14633&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Herin, M., Il Idrissi, M., Chabridon, V., &amp;amp; Iooss, B. (2022). Proportional marginal effects for global sensitivity analysis. arXiv preprint arXiv:2210.13065.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to predict covid case counts using machine learning models?</title>
      <link>/2022/03/23/how-to-predict-covid-case-counts-using-machine-learning-models/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate>
      <guid>/2022/03/23/how-to-predict-covid-case-counts-using-machine-learning-models/</guid>
      <description>&lt;p&gt;It would be nice to predict the number of positive covid cases depending on past cases evolution. Regression models based on recurrent neural networks (RNNs) are proven to identify patterns in time series data and this allows us to make accurate short-term predictions.&lt;/p&gt;
&lt;p&gt;The model used in the following example is based on long-term short-term memory (LSTM) model that uses more than one features to make informed predictions. LSTMs are recurrent neural networks that avoid the vanishing gradient problem prevalent in feed-forward type of algorithms by imposing filtering mechanisms in the gates using a technique known as back-propagation.&lt;/p&gt;
&lt;p&gt;The following set of codes loads all the required Python libraries, packages, and subroutines required for LSTM modeling. This blog post is just intended to give a high level summary of how to realize a covid case count prediction in the United States using some convenient features readily available.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import various libraries and routines needed for computation
import math 
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import keras.backend as K
from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv, DataFrame
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.callbacks import EarlyStopping
from datetime import date, timedelta, datetime 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read in the data file that has relevant features 
df = pd.read_csv(&#39;covid_final.csv&#39;)  
dataset = df.set_index([&#39;date&#39;])
# Drop the last 10 row as they are incomplete
dataset.drop(dataset.tail(10).index,
        inplace = True)
values = dataset.values
# Store the indexes (i.e., dates)
date_index = dataset.index
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Clean up the dataset more for predictions and inverse transformations (Re-scaling)
data_clean = dataset.copy()
data_clean_ext = dataset.copy()
data_clean_ext[&#39;new_cases_predictions&#39;] = data_clean_ext[&#39;new_cases_smoothed&#39;]
data_clean.tail()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new_cases_smoothed&lt;/th&gt;
      &lt;th&gt;reproduction_rate&lt;/th&gt;
      &lt;th&gt;new_tests_smoothed_per_thousand&lt;/th&gt;
      &lt;th&gt;new_vaccinations_smoothed_per_million&lt;/th&gt;
      &lt;th&gt;people_fully_vaccinated_per_hundred&lt;/th&gt;
      &lt;th&gt;total_boosters_per_hundred&lt;/th&gt;
      &lt;th&gt;stringency_index&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-08&lt;/th&gt;
      &lt;td&gt;38934.286&lt;/td&gt;
      &lt;td&gt;0.65&lt;/td&gt;
      &lt;td&gt;2.748&lt;/td&gt;
      &lt;td&gt;621&lt;/td&gt;
      &lt;td&gt;65.24&lt;/td&gt;
      &lt;td&gt;28.89&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-09&lt;/th&gt;
      &lt;td&gt;36641.429&lt;/td&gt;
      &lt;td&gt;0.66&lt;/td&gt;
      &lt;td&gt;2.699&lt;/td&gt;
      &lt;td&gt;601&lt;/td&gt;
      &lt;td&gt;65.25&lt;/td&gt;
      &lt;td&gt;28.91&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-10&lt;/th&gt;
      &lt;td&gt;36330.429&lt;/td&gt;
      &lt;td&gt;0.69&lt;/td&gt;
      &lt;td&gt;2.613&lt;/td&gt;
      &lt;td&gt;583&lt;/td&gt;
      &lt;td&gt;65.27&lt;/td&gt;
      &lt;td&gt;28.94&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-11&lt;/th&gt;
      &lt;td&gt;36104.714&lt;/td&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;2.580&lt;/td&gt;
      &lt;td&gt;557&lt;/td&gt;
      &lt;td&gt;65.29&lt;/td&gt;
      &lt;td&gt;28.97&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-12&lt;/th&gt;
      &lt;td&gt;35464.143&lt;/td&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;2.561&lt;/td&gt;
      &lt;td&gt;540&lt;/td&gt;
      &lt;td&gt;65.30&lt;/td&gt;
      &lt;td&gt;28.99&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# number of rows in the data
nrows = data_clean.shape[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The day-to-day case counts can be regarded as a time series and the data needs to be prepared before training a supervised learning model. For LSTM, the data is composed of inputs and outputs, and the inputs can be seen as a moving window blocks consisting of the feature values to predict the outcome. The size of the window is a free parameter that the user must optimize.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert the data to numpy values
np_data_unscaled = np.array(data_clean)
np_data = np.reshape(np_data_unscaled, (nrows, -1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ensure all data is float
values = values.astype(&#39;float64&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform the data by scaling each feature to a range between 0 and 1
scaler = MinMaxScaler()
np_data_scaled = scaler.fit_transform(np_data_unscaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Creating a separate scaler that works on a single column for scaling predictions
scaler_pred = MinMaxScaler()
df_cases = pd.DataFrame(data_clean_ext[&#39;new_cases_smoothed&#39;])
np_cases_scaled = scaler_pred.fit_transform(df_cases)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In LSTM methodology, it is required to reshape the input to be a 3D tensor of samples, time steps, and features. This is more important when we are fitting the model later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set the sequence length - this is the timeframe used to make a single prediction
sequence_length = 31  # rolling window size

# Prediction Index
index_cases = dataset.columns.get_loc(&amp;quot;new_cases_smoothed&amp;quot;)

# Split the training data into train and train data sets
# As a first step, we get the number of rows to train the model on 80% of the data 
train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)

# Create the training and test data
train_data = np_data_scaled[0:train_data_len, :]
test_data = np_data_scaled[train_data_len - sequence_length:, :]

# The RNN needs data with the format of [samples, time steps, features]
# Here, we create N samples, sequence_length time steps per sample, and 6 features
def partition_dataset(sequence_length, data):
    x, y = [], []
    data_len = data.shape[0]
    for i in range(sequence_length, data_len):
        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn
        y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction
    
    # Convert the x and y to numpy arrays
    x = np.array(x)
    y = np.array(y)
    return x, y

# Generate training data and test data
x_train, y_train = partition_dataset(sequence_length, train_data)
x_test, y_test = partition_dataset(sequence_length, test_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Configure the neural network model
model = Sequential()
# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables
n_neurons = x_train.shape[1] * x_train.shape[2]
model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check-points and early stopping parameters make our modeling easier
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
# Compiling the LSTM
model.compile(optimizer = &#39;adam&#39;, loss = &#39;mean_squared_error&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specfy the file and file path for the best model
checkpoint_path = &#39;my_best_model.hdf5&#39;
checkpoint = ModelCheckpoint(filepath=checkpoint_path, 
                             monitor=&#39;val_loss&#39;,
                             verbose=1, 
                             save_best_only=True,
                             mode=&#39;min&#39;)

earlystopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=50, restore_best_weights=True, verbose =0)
callbacks = [checkpoint, earlystopping]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Training the model
epochs = 300
batch_size = 20
history = model.fit(x_train, y_train,
                     batch_size=batch_size, 
                     epochs=epochs,
                     validation_data=(x_test, y_test),
                     callbacks = callbacks,
                     verbose = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the best model
from tensorflow.keras.models import load_model
model_from_saved_checkpoint = load_model(checkpoint_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot training &amp;amp; validation loss values
plt.figure(figsize=(16,7))
plt.plot(history.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;test&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./covid_analysis_17_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the predicted values
y_pred_scaled = model_from_saved_checkpoint.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Unscale the predicted values
y_pred = scaler_pred.inverse_transform(y_pred_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# reshape 
y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Mean Absolute Error (MAE)
MAE = mean_absolute_error(y_test_unscaled, y_pred)
print(f&#39;Median Absolute Error (MAE): {np.round(MAE, 2)}&#39;)

# Mean Absolute Percentage Error (MAPE)
MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100
print(f&#39;Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %&#39;)

# Median Absolute Percentage Error (MDAPE)
MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100
print(f&#39;Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot of the true and predicted case counts
plt.plot(y_test_unscaled, label=&#39;True&#39;)
plt.plot(y_pred, label=&#39;LSTM&#39;)
plt.title(&amp;quot;LSTM&#39;s_Prediction&amp;quot;)
plt.xlabel(&#39;Time steps&#39;)
plt.ylabel(&#39;Cases&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./covid_analysis_22_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New data frame for predicting the next day count
new_df = data_clean[-sequence_length:] # gets the last N days
N = sequence_length
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the values of the last N day cases counts 
# scale the data to be values between 0 and 1
last_N_days = new_df[-sequence_length:].values
last_N_days_scaled = scaler.transform(last_N_days)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create an empty list and Append past N days
X_test_new = []
X_test_new.append(last_N_days_scaled)

# Convert the X_test data set to a numpy array and reshape the data
pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))
pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print last price, predicted price, and change percent for the next day
cases_today = np.round(new_df[&#39;new_cases_smoothed&#39;][-1])
predicted_cases = np.round(pred_cases_unscaled.ravel()[0])
change_percent = np.round(100 - (cases_today * 100)/predicted_cases)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Code used to produce this article in jupyter notebook in hugo academic blog post
!jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=.
!cat covid_analysis.md | tee -a index.md
!rm covid_analysis.md
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Predicting COVID Case Counts with Machine Learning</title>
      <link>/project/internal-project/jupyter-lstm/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate>
      <guid>/project/internal-project/jupyter-lstm/</guid>
      <description>&lt;p&gt;It would be nice to predict the number of positive covid cases depending on past cases evolution. Regression models based on recurrent neural networks (RNNs) are proven to identify patterns in time series data and this allows us to make accurate short-term predictions.&lt;/p&gt;
&lt;p&gt;The model used in the following example is based on long-term short-term memory (LSTM) model that uses more than one features to make informed predictions. LSTMs are recurrent neural networks that avoid the vanishing gradient problem prevalent in feed-forward type of algorithms by imposing filtering mechanisms in the gates using a technique known as back-propagation.&lt;/p&gt;
&lt;p&gt;The following set of codes loads all the required Python libraries, packages, and subroutines required for LSTM modeling. This blog post is just intended to give a high level summary of how to realize a covid case count prediction in the United States using some convenient features readily available.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import various libraries and routines needed for computation
import math 
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import keras.backend as K
from math import sqrt
from numpy import concatenate
from matplotlib import pyplot
from pandas import read_csv, DataFrame
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.callbacks import EarlyStopping
from datetime import date, timedelta, datetime 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Read in the data file that has relevant features 
df = pd.read_csv(&#39;covid_final.csv&#39;)  
dataset = df.set_index([&#39;date&#39;])
# Drop the last 10 row as they are incomplete
dataset.drop(dataset.tail(10).index,
        inplace = True)
values = dataset.values
# Store the indexes (i.e., dates)
date_index = dataset.index
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Clean up the dataset more for predictions and inverse transformations (Re-scaling)
data_clean = dataset.copy()
data_clean_ext = dataset.copy()
data_clean_ext[&#39;new_cases_predictions&#39;] = data_clean_ext[&#39;new_cases_smoothed&#39;]
data_clean.tail()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;new_cases_smoothed&lt;/th&gt;
      &lt;th&gt;reproduction_rate&lt;/th&gt;
      &lt;th&gt;new_tests_smoothed_per_thousand&lt;/th&gt;
      &lt;th&gt;new_vaccinations_smoothed_per_million&lt;/th&gt;
      &lt;th&gt;people_fully_vaccinated_per_hundred&lt;/th&gt;
      &lt;th&gt;total_boosters_per_hundred&lt;/th&gt;
      &lt;th&gt;stringency_index&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-08&lt;/th&gt;
      &lt;td&gt;38934.286&lt;/td&gt;
      &lt;td&gt;0.65&lt;/td&gt;
      &lt;td&gt;2.748&lt;/td&gt;
      &lt;td&gt;621&lt;/td&gt;
      &lt;td&gt;65.24&lt;/td&gt;
      &lt;td&gt;28.89&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-09&lt;/th&gt;
      &lt;td&gt;36641.429&lt;/td&gt;
      &lt;td&gt;0.66&lt;/td&gt;
      &lt;td&gt;2.699&lt;/td&gt;
      &lt;td&gt;601&lt;/td&gt;
      &lt;td&gt;65.25&lt;/td&gt;
      &lt;td&gt;28.91&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-10&lt;/th&gt;
      &lt;td&gt;36330.429&lt;/td&gt;
      &lt;td&gt;0.69&lt;/td&gt;
      &lt;td&gt;2.613&lt;/td&gt;
      &lt;td&gt;583&lt;/td&gt;
      &lt;td&gt;65.27&lt;/td&gt;
      &lt;td&gt;28.94&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-11&lt;/th&gt;
      &lt;td&gt;36104.714&lt;/td&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;2.580&lt;/td&gt;
      &lt;td&gt;557&lt;/td&gt;
      &lt;td&gt;65.29&lt;/td&gt;
      &lt;td&gt;28.97&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2022-03-12&lt;/th&gt;
      &lt;td&gt;35464.143&lt;/td&gt;
      &lt;td&gt;0.71&lt;/td&gt;
      &lt;td&gt;2.561&lt;/td&gt;
      &lt;td&gt;540&lt;/td&gt;
      &lt;td&gt;65.30&lt;/td&gt;
      &lt;td&gt;28.99&lt;/td&gt;
      &lt;td&gt;53.24&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# number of rows in the data
nrows = data_clean.shape[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The day-to-day case counts can be regarded as a time series and the data needs to be prepared before training a supervised learning model. For LSTM, the data is composed of inputs and outputs, and the inputs can be seen as a moving window blocks consisting of the feature values to predict the outcome. The size of the window is a free parameter that the user must optimize.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Convert the data to numpy values
np_data_unscaled = np.array(data_clean)
np_data = np.reshape(np_data_unscaled, (nrows, -1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ensure all data is float
values = values.astype(&#39;float64&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transform the data by scaling each feature to a range between 0 and 1
scaler = MinMaxScaler()
np_data_scaled = scaler.fit_transform(np_data_unscaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Creating a separate scaler that works on a single column for scaling predictions
scaler_pred = MinMaxScaler()
df_cases = pd.DataFrame(data_clean_ext[&#39;new_cases_smoothed&#39;])
np_cases_scaled = scaler_pred.fit_transform(df_cases)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In LSTM methodology, it is required to reshape the input to be a 3D tensor of samples, time steps, and features. This is more important when we are fitting the model later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set the sequence length - this is the timeframe used to make a single prediction
sequence_length = 31  # rolling window size

# Prediction Index
index_cases = dataset.columns.get_loc(&amp;quot;new_cases_smoothed&amp;quot;)

# Split the training data into train and train data sets
# As a first step, we get the number of rows to train the model on 80% of the data 
train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)

# Create the training and test data
train_data = np_data_scaled[0:train_data_len, :]
test_data = np_data_scaled[train_data_len - sequence_length:, :]

# The RNN needs data with the format of [samples, time steps, features]
# Here, we create N samples, sequence_length time steps per sample, and 6 features
def partition_dataset(sequence_length, data):
    x, y = [], []
    data_len = data.shape[0]
    for i in range(sequence_length, data_len):
        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn
        y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction
    
    # Convert the x and y to numpy arrays
    x = np.array(x)
    y = np.array(y)
    return x, y

# Generate training data and test data
x_train, y_train = partition_dataset(sequence_length, train_data)
x_test, y_test = partition_dataset(sequence_length, test_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Configure the neural network model
model = Sequential()
# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables
n_neurons = x_train.shape[1] * x_train.shape[2]
model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check-points and early stopping parameters make our modeling easier
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
# Compiling the LSTM
model.compile(optimizer = &#39;adam&#39;, loss = &#39;mean_squared_error&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Specfy the file and file path for the best model
checkpoint_path = &#39;my_best_model.hdf5&#39;
checkpoint = ModelCheckpoint(filepath=checkpoint_path, 
                             monitor=&#39;val_loss&#39;,
                             verbose=1, 
                             save_best_only=True,
                             mode=&#39;min&#39;)

earlystopping = EarlyStopping(monitor=&#39;val_loss&#39;, patience=50, restore_best_weights=True, verbose =0)
callbacks = [checkpoint, earlystopping]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Training the model
epochs = 300
batch_size = 20
history = model.fit(x_train, y_train,
                     batch_size=batch_size, 
                     epochs=epochs,
                     validation_data=(x_test, y_test),
                     callbacks = callbacks,
                     verbose = 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load the best model
from tensorflow.keras.models import load_model
model_from_saved_checkpoint = load_model(checkpoint_path)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot training &amp;amp; validation loss values
plt.figure(figsize=(16,7))
plt.plot(history.history[&#39;loss&#39;], label=&#39;train&#39;)
plt.plot(history.history[&#39;val_loss&#39;], label=&#39;test&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./covid_analysis_17_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the predicted values
y_pred_scaled = model_from_saved_checkpoint.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Unscale the predicted values
y_pred = scaler_pred.inverse_transform(y_pred_scaled)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# reshape 
y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Mean Absolute Error (MAE)
MAE = mean_absolute_error(y_test_unscaled, y_pred)
print(f&#39;Median Absolute Error (MAE): {np.round(MAE, 2)}&#39;)

# Mean Absolute Percentage Error (MAPE)
MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100
print(f&#39;Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %&#39;)

# Median Absolute Percentage Error (MDAPE)
MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100
print(f&#39;Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot of the true and predicted case counts
plt.plot(y_test_unscaled, label=&#39;True&#39;)
plt.plot(y_pred, label=&#39;LSTM&#39;)
plt.title(&amp;quot;LSTM&#39;s_Prediction&amp;quot;)
plt.xlabel(&#39;Time steps&#39;)
plt.ylabel(&#39;Cases&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./covid_analysis_22_0.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# New data frame for predicting the next day count
new_df = data_clean[-sequence_length:] # gets the last N days
N = sequence_length
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get the values of the last N day cases counts 
# scale the data to be values between 0 and 1
last_N_days = new_df[-sequence_length:].values
last_N_days_scaled = scaler.transform(last_N_days)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create an empty list and Append past N days
X_test_new = []
X_test_new.append(last_N_days_scaled)

# Convert the X_test data set to a numpy array and reshape the data
pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))
pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print last price, predicted price, and change percent for the next day
cases_today = np.round(new_df[&#39;new_cases_smoothed&#39;][-1])
predicted_cases = np.round(pred_cases_unscaled.ravel()[0])
change_percent = np.round(100 - (cases_today * 100)/predicted_cases)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Code used to produce this article in jupyter notebook in hugo academic blog post
!jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=.
!cat covid_analysis.md | tee -a index.md
!rm covid_analysis.md
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/preprint/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/journal-article/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Supplementary notes can be added here, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code, math, and images&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
