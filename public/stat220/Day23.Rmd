---
title: "Cross Validation and Logistic Regression"
subtitle: "<br/> Spring 2023"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", css/xaringan-themer-solns.css, css/my-theme.css, css/my-font.css]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    seal: false
    nature:
      highlightStyle: googlecode  #http://arm.rbind.io/slides/xaringan.html#77 # idea, magula
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
      titleSlideClass: ["left", "middle", "inverse"]
      ratio: "16:9"
    includes:
      in_header: header.html
editor_options: 
  chunk_output_type: console
header-includes:
    - \usepackage{caption}
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(htmltools.preserve.raw = FALSE)
options(ggrepel.max.overlaps = Inf)

knitr::opts_chunk$set(echo = TRUE, 
                      dev = 'svg',
                      collapse = TRUE, 
                      comment = NA,  # PRINTS IN FRONT OF OUTPUT, default is '##' which comments out output
                      prompt = FALSE, # IF TRUE adds a > before each code input
                      warning = FALSE, 
                      message = FALSE,
                      fig.height = 3, 
                      fig.width = 4,
                      out.width = "100%",
                      prompt = FALSE,
                      rows.print=7
                      )



# load necessary packages
library(tidyr)
library(dplyr)
library(ggplot2)
library(countdown)
library(ggthemes)
library(tidyverse)
library(stringr)
library(xaringanExtra)
xaringanExtra::use_panelset()
xaringanExtra::use_tachyons()
library(flipbookr)
library(htmlwidgets)
library(lubridate)
library(palmerpenguins)
library(fontawesome)
library(class)
library(patchwork)
library(tidymodels)
library(mlbench)     # for PimaIndiansDiabetes2 dataset
library(janitor)
library(parsnip)
library(kknn)
library(paletteer)
library(corrr)
library(scico)
library(gridExtra)


select <- dplyr::select

# Set ggplot theme
# theme_set(theme_stata(base_size = 10))

yt <- 0

standardize <- function(x, na.rm = FALSE) {
  (x - mean(x, na.rm = na.rm)) / sd(x, na.rm = na.rm)
}

# Load the fire data
fire <- read_csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/Algeriafires.csv")
fire <- fire %>% clean_names() %>% na.omit() %>% mutate_at(c(10,13), as.numeric)
fire1 <- fire %>% mutate(across(where(is.numeric), standardize))

fire_raw <- fire %>% select(temperature, isi, classes)


fire1 <- fire %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  mutate(classes = as.factor(classes))


fire_recipe <- recipe(classes ~ ., data = fire_raw) %>%
 step_scale(all_predictors()) %>%
 step_center(all_predictors()) %>%
 prep()

fire_scaled <- bake(fire_recipe, fire_raw)

fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_fit <- fire_knn_spec %>%
 fit(classes ~ ., data = fire_scaled)


data(PimaIndiansDiabetes2)
db <- PimaIndiansDiabetes2
db <- db %>% drop_na() %>% mutate(diabetes = fct_rev(factor(diabetes))) 

```



```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


layout: true
  
---

class: title-slide, middle

# .fancy[Cross Validation and Logistic Regression]

### .fancy[Spring 2023]

`r format(Sys.Date(), ' %B %d %Y')`


---


# Resampling methods

<center>
<img src="images/resampling.svg" width="60%" height="60%"> <br>
</center>


.yellow-h[Create a series of data sets similar to the training/testing split, always used with the training set]


.footnote[[Kuhn and Johnson (2019)](https://bookdown.org/max/FES/resampling.html)]


---


```{r echo=FALSE, comment=NULL}
set.seed(12345)
fire1 <- fire %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  mutate(classes = as.factor(classes))

fire_split <- initial_split(fire1, prop = 0.5)

# Create training data
fire_train <- fire_split %>%
                    training()


# Create testing data
fire_test <- fire_split %>%
                    testing()
```


```{r, echo=FALSE}
fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_wkflow <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(fire_knn_spec)


fire_knn_fit <- fit(fire_knn_wkflow, data = fire_train)

test_features <- fire_test %>% select(temperature, isi) %>% data.frame()

nn1_pred <- predict(fire_knn_fit, test_features, type = "raw")

fire_results <- fire_test %>% 
  select(classes) %>% 
  bind_cols(predicted = nn1_pred)
  

#create a prediction pt grid

temp_grid <- seq(min(fire1$temperature), max(fire1$temperature), length.out = 100)
isi_grid <- seq(min(fire1$isi), max(fire1$isi), length.out = 100)
plot_grid <- expand.grid(temperature = temp_grid, isi = isi_grid)

knn_pred_grid <- predict(fire_knn_fit, plot_grid, type = "raw")

prediction_table <- bind_cols(plot_grid, data.frame(classes = knn_pred_grid))

```



## Why not to use single (training) test set

```{r, echo=FALSE, fig.width=8, fig.height=4.5, fig.align='center', out.width = "90%"}
ts1 <- ggplot(data = fire_train) +
  geom_point(data = prediction_table, aes(x = temperature, y = isi, color = classes), alpha = 0.05) +
  geom_point(aes(x = temperature, y = isi, color = classes), alpha = 0.5) +
  labs(x = "Temperature", y = "Initian Spread Index (ISI)") +
  scale_fill_wsj() +
  scale_color_wsj() +
  theme_light() +
  theme(legend.position = "none")  +
  ggtitle("Training set #1") + theme(plot.title = element_text(hjust = 0.5))



set.seed(143)
fire_split1 <- initial_split(fire1, prop = 0.75)

# Create training data
fire_train1 <- fire_split1 %>%
                    training()


# Create testing data
fire_test1 <- fire_split1 %>%
                    testing()


fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_wkflow <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(fire_knn_spec)


fire_knn_fit1 <- fit(fire_knn_wkflow, data = fire_train1)

test_features1 <- fire_test1 %>% select(temperature, isi) %>% data.frame()

nn1_pred1 <- predict(fire_knn_fit1, test_features1, type = "raw")

fire_results1 <- fire_test1 %>% 
  select(classes) %>% 
  bind_cols(predicted = nn1_pred1)
  

#create a prediction pt grid

knn_pred_grid1 <- predict(fire_knn_fit1, plot_grid, type = "raw")

prediction_table1 <- bind_cols(plot_grid, data.frame(classes = knn_pred_grid1))



ts2 <- ggplot(data = fire_train1) +
  geom_point(data = prediction_table1, aes(x = temperature, y = isi, color = classes), alpha = 0.05) +
  geom_point(aes(x = temperature, y = isi, color = classes), alpha = 0.5) +
  labs(x = "Temperature", y = "Initian Spread Index (ISI)") +
  scale_fill_wsj() +
  scale_color_wsj() +
  theme_light() +
  theme(legend.position = "none") +
  ggtitle("Training set #2") + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(ts1,ts2, ncol = 2) 

```


---

background-image: url(images/k-folds.png)
background-position: 50% 90%
background-size: 90%

## Cross validation

.bq[
.bold[Idea:] Split the training data up into multiple training-validation pairs, evaluate the classifier on each split and average the performance metrics
]

.footnote[Image courtesy of Dennis Sun]

---

class: middle

## k-fold cross validation

.bq.font90[
1. split the data into $k$ subsets

2. combine the first $k-1$ subsets into a training set and train the classifier

3. evaluate the model predictions on the last (i.e. $k$th) held-out subset

4. repeat steps 2-3 $k$ times (i.e. $k$ "folds"), each time holding out a different one of the $k$ subsets

5. calculate performance metrics from each validation set

6. average each metric over the $k$ folds to come up with a single estimate of that metric
]

---

```{r echo=FALSE, out.width = "75%", fig.width = 5, fig.height = 3, fig.align='center'}
#rm(accuracy, ppv)
#detach(package:caret)
train_complete <- fire_raw

fire_recipe <- recipe(
  classes ~  temperature + isi, 
  data = train_complete
) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors())

knn_spec <- nearest_neighbor(
  weight_func = "rectangular", 
  neighbors = tune() #<<
) %>%
  set_engine("kknn") %>%
  set_mode("classification")

set.seed(1234)

fire_vfold <- vfold_cv(fire_raw, v = 5, strata = classes)

k_vals <- tibble(neighbors = seq(from = 1, to = 15, by = 1))

knn_fit <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(knn_spec) %>%
  tune_grid(
    resamples = fire_vfold, 
    grid = k_vals,
    metrics = metric_set(accuracy, sensitivity, specificity, ppv, kap, mcc)
    )

cv_metrics <- collect_metrics(knn_fit)

ggplot(cv_metrics %>% filter(.metric == "accuracy"), aes(x = neighbors, y = mean)) +
  geom_point() + 
  geom_line() +
  labs(x = "#Neighbors", y = "Accuracy (Cross-Validation)") +
  theme_light() +
  scale_x_continuous(breaks = 1:15, minor_breaks = NULL) + 
  scale_y_continuous(limits = c(0.93,1.0))

```

.hljs.purple[
- Based on accuracy, $k=7$ appears best, We C=can look at other metrics. Notice that accuracy doesn't always decrease with $k$
]

---

class: middle

## 5-fold cross validation

Creating the recipe


```{r}
fire_recipe <- recipe(classes ~  temperature + isi, data = fire_train) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>% 
  prep()
```


---

class: middle

## 5-fold cross validation

Create your model specification and use `tune()` as a placeholder for the number of neighbors

```{r}
knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular", 
                             neighbors = tune())  #<<
```


Split the `fire_train` data set into `v = 5` folds, stratified by `classes`

```{r}
fire_vfold <- vfold_cv(fire_train, v = 5, strata = classes)
```


---

class: middle

## 5-fold cross validation

Create a grid of $K$ values, the number of neighbors

```{r}
k_vals <- tibble(neighbors = seq(from = 1, to = 15, by = 1))
```


Run 5-fold CV on the `k_vals` grid, storing four performance metrics

```{r}
knn_fit <- workflow() %>%
  add_recipe(fire_recipe) %>%
  add_model(knn_spec) %>%
  tune_grid(resamples = fire_vfold, 
            grid = k_vals,
            metrics = metric_set(accuracy, sensitivity, specificity, ppv))
```


---

class: middle

## Choosing K

Collect the performance metrics and find the best model

.font110[
```{r}
cv_metrics <- collect_metrics(knn_fit)
cv_metrics %>% head(6)
```
]

---

class: middle

## Choosing K

```{r}
cv_metrics %>%
  group_by(.metric) %>%
  slice_max(mean) 

```

---

# Choosing K

```{r, echo=FALSE, fig.width=8, fig.height=4.5, fig.align='center', out.width = "90%"}
final.results <- cv_metrics %>%  mutate(.metric = as.factor(.metric)) %>%
  select(neighbors, .metric, mean)

final.results %>%
  ggplot(aes(x = neighbors, y = mean, color = forcats::fct_reorder2(.metric, neighbors, mean))) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  theme_minimal() +
  scale_color_wsj() + 
  scale_x_continuous(breaks = k_vals[[1]]) +
  theme(panel.grid.minor.x = element_blank())+
  labs(color='Metric', y = "Estimate", x = "K")
```

---
background-image: url(images/cv-5.png)
background-position: middle, center
background-size: 55%
## The full process

.footnote[Image source: rafalab.github.io/dsbook/]

---


class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Get the class activity 23.Rmd file from  [moodle](https://moodle.carleton.edu/course/view.php?id=41417) 
- Let's work on group activity 1 together
]



]

`r countdown(minutes = 5, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`

---



class: inverse, middle

# .Large[Let's see further example of classification using simple logistic regression!]

---

.panelset[
.panel[.panel-name[Logistic Regression (LR)]
.bq.font80[
- Binary response, $Y$,  with an explanatory (predictor, features) variables, $X_1$.
- We model the probability that $Y$ belongs to a particular category.


$$P(Y = 1 ) = \frac{e^{\beta_0 + \beta_1X_1}}{1 + e^{\beta_0 + \beta_1X_1}}$$

$$\text{Odds} = \frac{P(Y = 1 )}{1 - P(Y = 1 )} = e^{\beta_0 + \beta_1X_1}$$

$$\text{Log Odds} = \beta_0 + \beta_1X_1$$
]

]


.panel[.panel-name[Visual]

```{r, echo = FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
db_plot <- db %>% mutate(y = ifelse(diabetes == "pos", 1, 0))
ggplot(db_plot, aes(x=glucose, y=y)) + 
  geom_point(aes(color=diabetes, shape = diabetes)) + 
  geom_smooth(method = glm, method.args = list(family = binomial), se = FALSE) + 
  labs(y = "Probability of diabetes", title = "Logistic regression probability of diabetes given glucose")+
  theme_tufte()+
  scale_color_wsj()
```

]

.panel[.panel-name[Data Preparation]

```{r}
# Create data split for train and test
set.seed(12345)
db_single <- db %>% select(diabetes, glucose) %>% 
  mutate(diabetes = fct_relevel(diabetes, ref = "neg" )) 
db_split <- initial_split(db_single, prop = 0.80)

# Create training data
db_train <- db_split %>% training()

# Create testing data
db_test <- db_split %>% testing()
```
]


.panel[.panel-name[Modeling]

```{r}
set.seed(12345)
db_recipe <- recipe(diabetes ~ ., data = db_train) %>%
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) %>% prep()

fitted_logistic_model <- logistic_reg(engine = "glm",  # Call the model
                                      mode = "classification") %>% 
                        fit(diabetes~., data = db_train)  # Fit the model
```

]
]

---

class: middle

# Tidy the Summary

```{r}
broom::tidy(fitted_logistic_model)
```


---

class: middle

# Odds Ratio

$$ODDS = \frac{probability}{1 - probability}$$


```{r}
broom::tidy(fitted_logistic_model, exponentiate = TRUE)
```


---

# Threshold for classification

```{r, echo= FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
set.seed(12345)
t <- 0.5
x.thres<- (log(t/(1-t))-fitted_logistic_model$fit$coefficients[1])/fitted_logistic_model$fit$coefficients[2]

db_plot <- db %>% mutate(y = ifelse(diabetes == "pos", 1, 0))

ggplot(db_plot, aes(x=glucose, y=y)) + 
  geom_jitter(aes(color=diabetes, shape =diabetes), height = 0.01) + 
  geom_smooth(method = glm, method.args = list(family = binomial), se = FALSE) + 
  labs(y="Probability of diabetes", title = "Probability of diabetes given glucose") + 
  geom_segment(aes(x=x.thres, xend=180,y=0.50,yend=0.50), linetype = "dashed", col = "firebrick") + 
  geom_segment(aes(x=x.thres, xend=x.thres,y=1,yend=0), linetype = "dashed") +
  geom_text(x=181, y=.49, label = "threshold = 143.11", color="blue",family="Times") + 
  annotate(geom = "rect", xmin = x.thres, xmax = 200, ymin = -.05, ymax = .05, fill = "blue", alpha = 0.1) + 
  geom_text(x=170, y = .15,label = "diabetes", color="blue",family="Times")+
  theme_tufte() + 
  xlim(c(54,200))
```


---


class: middle

# Class Prediction

.code80[
```{r, fig.width=6, fig.height=4.5, fig.align='center', out.width = "50%"}
set.seed(12345)
pred_class <- predict(fitted_logistic_model,  new_data = db_test) 
bind_cols(db_test %>% select(diabetes), pred_class) %>% 
  conf_mat(diabetes, .pred_class) %>% # confusion matrix
  autoplot(type = "heatmap") # with graphics
```
]

---

# Class Probabilities with `threshold = 0.50`

```{r}
# Prediction Probabilities
library(probably)
pred_prob <- predict(fitted_logistic_model,  new_data = db_test,   type = "prob")

db_results <- db_test %>% bind_cols(pred_prob) %>%
  mutate(.pred_class = make_two_class_pred(.pred_neg, levels(diabetes), threshold = .5)) %>%
  select(diabetes, glucose, contains(".pred"))
```

```{r, echo=FALSE}
head(db_results,10)
```


---

class: middle

# Custom Metrics

.pull-left[
```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "90%"}
db_results %>%  
  conf_mat(diabetes,.pred_class) %>% 
  autoplot(type = "heatmap")
```
]

.pull-right[
```{r}
custom_metrics <- metric_set(accuracy, 
                             sens, 
                             spec, 
                             ppv)
custom_metrics(db_results,
               truth = diabetes,
               estimate = .pred_class)
```
]

---


# Class Probabilities with `threshold = 0.70`

```{r}
# Prediction Probabilities
library(probably)
pred_prob <- predict(fitted_logistic_model,  new_data = db_test,   type = "prob")

db_results <- db_test %>% bind_cols(pred_prob) %>%
  mutate(.pred_class = make_two_class_pred(.pred_neg, levels(diabetes), threshold = .70)) %>%
  select(diabetes, glucose, contains(".pred"))
```

```{r, echo=FALSE}
head(db_results,10)
```


---

class: middle

# Custom Metrics

.pull-left[
```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "90%"}
db_results %>%  
  conf_mat(diabetes,.pred_class) %>% 
  autoplot(type = "heatmap")
```
]

.pull-right[
```{r}
custom_metrics <- metric_set(accuracy, 
                             sens, 
                             spec, 
                             ppv)
custom_metrics(db_results,
               truth = diabetes,
               estimate = .pred_class)
```
]

---

class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Please continue working on group activity 2
]

]

`r countdown(minutes = 10, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`

