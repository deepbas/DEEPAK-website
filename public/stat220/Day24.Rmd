---
title: "Logistic Regression"
subtitle: "<br/> Spring 2023"
author: "Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  xaringan::moon_reader:
    css: ["default", css/xaringan-themer-solns.css, css/my-theme.css, css/my-font.css]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    seal: false
    nature:
      highlightStyle: googlecode  #http://arm.rbind.io/slides/xaringan.html#77 # idea, magula
      highlightLines: true
      highlightLanguage: ["r", "css", "yaml"]
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
      titleSlideClass: ["left", "middle", "inverse"]
      ratio: "16:9"
    includes:
      in_header: header.html
editor_options: 
  chunk_output_type: console
header-includes:
    - \usepackage{caption}
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(htmltools.preserve.raw = FALSE)
options(ggrepel.max.overlaps = Inf)

knitr::opts_chunk$set(echo = TRUE, 
                      dev = 'svg',
                      collapse = TRUE, 
                      comment = NA,  # PRINTS IN FRONT OF OUTPUT, default is '##' which comments out output
                      prompt = FALSE, # IF TRUE adds a > before each code input
                      warning = FALSE, 
                      message = FALSE,
                      fig.height = 3, 
                      fig.width = 4,
                      out.width = "100%",
                      prompt = FALSE,
                      rows.print=7
                      )



# load necessary packages
library(tidyr)
library(dplyr)
library(ggplot2)
library(countdown)
library(ggthemes)
library(tidyverse)
library(stringr)
library(xaringanExtra)
xaringanExtra::use_panelset()
xaringanExtra::use_tachyons()
library(flipbookr)
library(htmlwidgets)
library(lubridate)
library(palmerpenguins)
library(fontawesome)
library(class)
library(patchwork)
library(tidymodels)
library(mlbench)     # for PimaIndiansDiabetes2 dataset
library(janitor)
library(parsnip)
library(kknn)
library(paletteer)
library(corrr)
library(scico)
library(gridExtra)


select <- dplyr::select

# Set ggplot theme
# theme_set(theme_stata(base_size = 10))

yt <- 0

standardize <- function(x, na.rm = FALSE) {
  (x - mean(x, na.rm = na.rm)) / sd(x, na.rm = na.rm)
}

# Load the fire data
fire <- read_csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/Algeriafires.csv")
fire <- fire %>% clean_names() %>% na.omit() %>% mutate_at(c(10,13), as.numeric)
fire1 <- fire %>% mutate(across(where(is.numeric), standardize))

fire_raw <- fire %>% select(temperature, isi, classes)


fire1 <- fire %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  mutate(classes = as.factor(classes))


fire_recipe <- recipe(classes ~ ., data = fire_raw) %>%
 step_scale(all_predictors()) %>%
 step_center(all_predictors()) %>%
 prep()

fire_scaled <- bake(fire_recipe, fire_raw)

fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_fit <- fire_knn_spec %>%
 fit(classes ~ ., data = fire_scaled)


data(PimaIndiansDiabetes2)
db <- PimaIndiansDiabetes2
db <- db %>% drop_na()  %>% 
  mutate(diabetes = fct_relevel(diabetes, c("neg", "pos"))) # Relevels 'diabetes' factor to ensure 'neg' comes before 'pos'
```



```{r xaringanExtra-clipboard, echo=FALSE}
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```


layout: true
  
---

class: title-slide, middle

# .fancy[Logistic Regression]

### .fancy[Stat 220]

`r format(Sys.Date(), ' %B %d %Y')`


---


class: inverse, middle

# .Large[Let's see an example of classification using simple logistic regression!]


---

class: middle

# Outline

.hljs[
- .b[Goal:] Understand why logistic regression is used for categorical outcomes.
- .b[Context:] Traditional linear regression is limited to continuous outcomes.
- .b[Solution:] Logistic regression models the probability of a categorical outcome.
]

---

class: middle

# Why Not Linear Regression?

.b[
- Issue with categorical outcomes: Predictions can fall outside the $[0,1]$ range.
- Consequence: Nonsensical probability predictions for binary outcomes $(Y=0\text{ or }Y=1)$).
]

---

class: middle

# Introduction to Logistic Regression

.bold[
- Solution: Transform the outcome into a format that linear regression can handle.
- Method: Model the log odds of the probability of success versus failure.
]

---

class: middle

# From Probability to Odds

.bql[
- Probability: Chance of success ( $Y=1$ ) over total possibilities.
$$P(Y=1)=\frac{\text { number of successes }}{\text { total trials }}$$
- Odds: Ratio of the probability of success to the probability of failure.
$$Odds =\frac{P(Y=1)}{1-P(Y=1)}$$
]


---

class: middle

# Logit Transformation


.bq[
- Goal: Convert odds to a continuous scale that can span negative to positive infinity.
- Logit Function: Natural logarithm of the odds.
$$\log \left(\frac{P(Y=1)}{1-P(Y=1)}\right)$$
- Why? Makes it possible to use linear regression techniques.
]


---

.panelset[

.panel[.panel-name[LR Visualization]

```{r, echo = FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
db_plot <- db %>% mutate(y = ifelse(diabetes == "pos", 1, 0))
ggplot(db_plot, aes(x=glucose, y=y)) + 
  geom_jitter(aes(color=diabetes), width = 0.1, height = 0.01, shape = 16) +  # Adds jitter to points for better visibility
  geom_smooth(method = glm, method.args = list(family = binomial), se = FALSE, color = "black") +  # Logistic regression line
  labs(y = expression(paste("Probability of diabetes, ", p == P(Y == 1))), 
       title = "Logistic regression probability of diabetes given glucose")+
  theme_tufte()+
  scale_color_wsj()
```

]



.panel[.panel-name[LR Summary]
.bq.font80[
- Binary response, $Y$,  with an explanatory (predictor, features) variables, $X_1$.
- We model the probability that $Y$ belongs to a particular category.


$$P(Y = 1 ) = \frac{e^{\beta_0 + \beta_1X_1}}{1 + e^{\beta_0 + \beta_1X_1}}$$

$$\text{Odds} = \frac{P(Y = 1 )}{1 - P(Y = 1 )} = e^{\beta_0 + \beta_1X_1}$$

$$\text{Log Odds} = \beta_0 + \beta_1X_1$$
]

]




.panel[.panel-name[Data Preparation]

```{r}
# Create data split for train and test
set.seed(12345) # Ensures reproducibility of the data split
db_single <- db %>% 
  select(diabetes, glucose) %>%  
  mutate(diabetes = fct_relevel(diabetes, c("neg", "pos"))) # Relevels 'diabetes' factor to ensure 'neg' comes before 'pos'
db_split <- initial_split(db_single, prop = 0.80) 
db_train <- db_split %>% training()
db_test <- db_split %>% testing() 
```
]


.panel[.panel-name[Modeling]

```{r}
set.seed(12345) # Ensures reproducibility in model results
db_recipe <- recipe(diabetes ~ ., data = db_train) %>% # Defines preprocessing recipe for modeling
  step_scale(all_predictors()) %>% # Scales all predictor variables
  step_center(all_predictors()) %>% prep() # Centers all predictor variables and prepares recipe

fitted_logistic_model <- logistic_reg(engine = "glm",  # Specifies logistic regression model using glm engine
                                      mode = "classification") %>% # Sets model to classification mode
                        fit(diabetes~., data = db_train)  # Fits model to training data
```

]
]

---

class: middle

# Tidy the Summary

```{r}
broom::tidy(fitted_logistic_model)
```



$$\log \left(\frac{p}{1-p}\right)=-5.61+0.0392 \cdot \text { glucose }$$

Where $p = P(Y = 1)$ is the probability of having diabetes 

---

<br>
<br>
<br>


# Interpreting Coefficients: Log Odds

.b[
- Coefficient Meaning: Change in log odds of the outcome for a one-unit increase in the predictor.
]

--

.hljs.blue[A coefficient of 0.0392 for glucose means...]

--

.hljs.blue[...a one-unit increase in glucose level increases the log odds of diabetes by 0.0392.]

---

<br>
<br>

# Exponentiating Coefficients: Odds Ratios

$$Odds = \frac{probability}{1 - probability}$$


```{r}
broom::tidy(fitted_logistic_model, exponentiate = TRUE)
```

--

$$\text { Odds }=0.00364 \times(1.04)^{\text {glucose }}$$
--

.blue-h[An odds ratio of 1.04 means that for each additional unit of glucose, the odds of having diabetes increase by 4%.]

---

class: middle

# Understanding Odds Ratios

.bold[
- Transformation: Turning log-odds coefficients into odds ratios.
- Odds Ratio $=e^{\beta_i}$
- Interpretation: For each unit increase in the predictor, the odds multiply by the odds ratio.
]


---



class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[

.bq[
- Get the class activity 24.Rmd file from  [moodle](https://moodle.carleton.edu/course/view.php?id=43045) 
- Let's work on group activity 1 together
]

.b[Hint:
- Calculate log odds: $-5.61 + (0.0392 \times 150)$
- Convert log odds to odds: $e^{\text{log odds}}$
- Convert odds to probability: $\frac{\text{Odds}}{1 + \text{Odds}}$]

]

`r countdown(minutes = 15, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`


---

# Threshold for classification

```{r, echo= FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
set.seed(12345)
t <- 0.5
x.thres <- (log(t / (1 - t)) - fitted_logistic_model$fit$coefficients[1]) / fitted_logistic_model$fit$coefficients[2]

db_plot <- db %>% mutate(y = ifelse(diabetes == "pos", 1, 0))

ggplot(db_plot, aes(x = glucose, y = y)) + 
  geom_jitter(aes(color = diabetes), width = 0.1, height = 0.01, shape = 16) + 
  geom_smooth(method = glm, method.args = list(family = binomial), se = FALSE, color = "black") + 
  labs(y = expression(paste("Probability of diabetes, ", p == P(Y == 1))), 
       x = "Glucose Level", 
       title = "Probability of Diabetes Given Glucose Level") +   geom_vline(xintercept = x.thres, linetype = "dashed", color = "firebrick") + 
  annotate("text", x = x.thres + 3, y = 0.45, label = sprintf("Threshold: %.2f", x.thres), hjust = 0, color = "firebrick") + 
  annotate(geom = "rect", xmin = x.thres, xmax = 200, ymin = -.05, ymax = .05, fill = "black", alpha = 0.1) + 
  annotate(geom = "rect", xmin = 55, xmax = x.thres, ymin = 0.95, ymax = 1.05, fill = "firebrick", alpha = 0.1) + 
  theme_tufte() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom") +
  xlim(c(54, 200))
```


---


class: middle

# Class Prediction

.code80[
```{r, fig.width=6, fig.height=4.5, fig.align='center', out.width = "50%"}
set.seed(12345)
pred_class <- predict(fitted_logistic_model,  new_data = db_test) 
bind_cols(db_test %>% select(diabetes), pred_class) %>% 
  conf_mat(diabetes, .pred_class) %>% # confusion matrix
  autoplot(type = "heatmap") # with graphics
```
]

---

# Class Probabilities with `threshold = 0.70`

```{r}
# Prediction Probabilities
library(probably)
pred_prob <- predict(fitted_logistic_model,  new_data = db_test,   type = "prob")

db_results <- db_test %>% bind_cols(pred_prob) %>%
  mutate(.pred_class = make_two_class_pred(.pred_neg, levels(diabetes), threshold = .7)) %>%
  select(diabetes, glucose, contains(".pred"))
```

```{r, echo=FALSE}
head(db_results,10)
```


---

class: middle

# Custom Metrics

.pull-left[
```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "90%"}
db_results %>%  
  conf_mat(diabetes,.pred_class) %>% 
  autoplot(type = "heatmap")
```
]

.pull-right[
```{r}
custom_metrics <- metric_set(accuracy, 
                             sens, 
                             spec, 
                             ppv)
custom_metrics(db_results,
               truth = diabetes,
               estimate = .pred_class)
```
]

---


# Class Probabilities with `threshold = 0.70`

```{r}
# Prediction Probabilities
library(probably)
pred_prob <- predict(fitted_logistic_model,  new_data = db_test,   type = "prob")

db_results <- db_test %>% bind_cols(pred_prob) %>%
  mutate(.pred_class = make_two_class_pred(.pred_neg, levels(diabetes), threshold = .70)) %>%
  select(diabetes, glucose, contains(".pred"))
```

```{r, echo=FALSE}
head(db_results,10)
```


---

class: middle

# Custom Metrics

.pull-left[
```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "90%"}
db_results %>%  
  conf_mat(diabetes,.pred_class) %>% 
  autoplot(type = "heatmap")
```
]

.pull-right[
```{r}
custom_metrics <- metric_set(accuracy, 
                             sens, 
                             spec, 
                             ppv)
custom_metrics(db_results,
               truth = diabetes,
               estimate = .pred_class)
```
]

---

class: action, middle

# <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple">&nbsp;Group&nbsp;Activity&nbsp;`r (yt <- yt + 1)`</i>    


.pull-left-40[
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
]
.pull-right-60[
<br>
<br>
.bq[
- Please continue working on group activity 2
]

]

`r countdown(minutes = 30, seconds = 00, top = 0 , color_background = "inherit", padding = "3px 4px", font_size = "2em")`

