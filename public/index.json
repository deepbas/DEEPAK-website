[{"authors":null,"categories":null,"content":"Deepak Bastola is a visiting assistant professor of statistics at Carleton College. His research interests include bayesian statistics, time series methodology, and statistical computation.\n  Download my resum√©.\n","date":1647993600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1647993600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://deepbas.netlify.app/author/deepak-bastola/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/deepak-bastola/","section":"authors","summary":"Deepak Bastola is a visiting assistant professor of statistics at Carleton College. His research interests include bayesian statistics, time series methodology, and statistical computation.\n  Download my resum√©.","tags":null,"title":"Deepak Bastola","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://deepbas.netlify.app/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"Meet your instructor Deepak Bastola ","date":1616544000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1616544000,"objectID":"e9ff2be4bfb6e5a4a69a669a6fbfbe97","permalink":"https://deepbas.netlify.app/courses/stat230/","publishdate":"2021-03-24T00:00:00Z","relpermalink":"/courses/stat230/","section":"courses","summary":"In depth modeling and application of regression","tags":null,"title":"üìä STAT 230: Applied Regression Analysis","type":"book"},{"authors":null,"categories":null,"content":"Meet your instructor Deepak Bastola ","date":1616544000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1616544000,"objectID":"08e80848897097ab85e97a023372f8bd","permalink":"https://deepbas.netlify.app/courses/stat120/","publishdate":"2021-03-24T00:00:00Z","relpermalink":"/courses/stat120/","section":"courses","summary":"A first course in statistics","tags":null,"title":"üìä STAT 120: Introduction to Statistics","type":"book"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Course overview Learning Objectives Course Requirements Meet your instructor    What you will learn  Data wrangling and formatting Using the tidyverse set of libraries Exploratory data analysis including data visualization using ggplot2 Data acquisition using web-scraping and APIs Statistical learning basics of supervised and unsupervised learning Text mining and language processing Reproducible research and dynamic programming using R/RStudio/RShiny  Course overview Welcome to introduction to data science! This course will cover the computational side of data analysis, including data acquisition, management, and visualization tools. The course introduces principles of data-scientific, reproducible research and dynamic programming using the R/RStudio ecosystem.\nIf you took Stat 120, 230, or 250 at Carleton, then you are in good shape. It is essential to recap your basic R and R-markdown skills by the first week of the class. Specifically, I expect that everyone can load a data set into R, calculate basic summary statistics, and create basic exploratory data analysis. I will expose you to Git and GitHub version control in the first week of the class and prior exposure to these is not required.\nLearning Objectives  Develop research questions that can be answered by data. Import/scrape data into R and reshape it to the form necessary for analysis. Manipulate common types of data, including numeric, categorical (factors), text, date-times, geo-location variables in order to provide insight into your data and facilitate analysis. Explore data using both graphical and numeric methods to provide insight and uncover relationships/patterns. Utilize fundamental programming concepts such as iteration, conditional execution, and functions to streamline your code. Build, tune, use, and evaluate basic statistical learning models to uncover clusters and classify observations. Draw informed conclusions from your data and communicate your findings using both written and interactive platforms.  Course Requirements  Group Assignments (5%) Individual assignments (20%) Paired projects (10%) Midterm Exams (45%) Final Project (20%)  VIEW THE SYLLABUS\nMeet your instructor Deepak Bastola ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"206634b0871fe67b6eec133296b05e90","permalink":"https://deepbas.netlify.app/courses/stat220/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/stat220/","section":"courses","summary":"A course focusing on the computational side of data analysis","tags":null,"title":"üìä STAT 220: Introduction to Data Science","type":"book"},{"authors":null,"categories":null,"content":"Day 10 Slides (pdf/html)\n","date":1650240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650240000,"objectID":"706b02d9e6fb8cbcf558735b8872cd32","permalink":"https://deepbas.netlify.app/courses/stat120/week4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat120/week4/","section":"courses","summary":"Day 10 Slides (pdf/html)","tags":null,"title":"Week 4","type":"book"},{"authors":null,"categories":null,"content":"Day 10 Slides (pdf/html)\n","date":1650240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650240000,"objectID":"c866ad397fa3702519d34124b14599b6","permalink":"https://deepbas.netlify.app/courses/stat230/week4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat230/week4/","section":"courses","summary":"Day 10 Slides (pdf/html)","tags":null,"title":"Week 4","type":"book"},{"authors":null,"categories":null,"content":"Day 7 Slides (pdf/html)\nDay 8 Slides (pdf/html)\nDay 9 Slides (pdf/html)\n","date":1649635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649635200,"objectID":"7aeff46ae3afd7ba1aec8f7432a8974a","permalink":"https://deepbas.netlify.app/courses/stat120/week3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat120/week3/","section":"courses","summary":"Day 7 Slides (pdf/html)\nDay 8 Slides (pdf/html)\nDay 9 Slides (pdf/html)","tags":null,"title":"Week 3","type":"book"},{"authors":null,"categories":null,"content":"Day 7 Slides (pdf/html)\nDay 8 Slides (pdf/html)\nDay 9 Slides (pdf/html)\n","date":1649635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649635200,"objectID":"4b02bbc11b3b707a13077e6347cc51d7","permalink":"https://deepbas.netlify.app/courses/stat230/week3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat230/week3/","section":"courses","summary":"Day 7 Slides (pdf/html)\nDay 8 Slides (pdf/html)\nDay 9 Slides (pdf/html)","tags":null,"title":"Week 3","type":"book"},{"authors":null,"categories":null,"content":"Day 4 Slides (pdf/html)\nDay 5 Slides (pdf/html)\nDay 6 Slides (pdf/html)\n","date":1648857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648857600,"objectID":"d0d63021090bd5f394572c48d5c6247d","permalink":"https://deepbas.netlify.app/courses/stat120/week2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat120/week2/","section":"courses","summary":"Day 4 Slides (pdf/html)\nDay 5 Slides (pdf/html)\nDay 6 Slides (pdf/html)","tags":null,"title":"Week 2","type":"book"},{"authors":null,"categories":null,"content":"Day 4 Slides (pdf/html)\nDay 5 Slides (pdf/html)\nDay 6 Slides (pdf/html)\n","date":1648857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648857600,"objectID":"78480c4a54a0c32157ecd36ba5afdf63","permalink":"https://deepbas.netlify.app/courses/stat230/week2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat230/week2/","section":"courses","summary":"Day 4 Slides (pdf/html)\nDay 5 Slides (pdf/html)\nDay 6 Slides (pdf/html)","tags":null,"title":"Week 2","type":"book"},{"authors":null,"categories":null,"content":"Day 1 Slides (pdf/html)\nDay 2 Slides (pdf/html)\nDay 3 Slides (pdf/html)\n","date":1648252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648252800,"objectID":"e234f049dd42d6117f7059542da57c47","permalink":"https://deepbas.netlify.app/courses/stat120/week1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat120/week1/","section":"courses","summary":"Day 1 Slides (pdf/html)\nDay 2 Slides (pdf/html)\nDay 3 Slides (pdf/html)","tags":null,"title":"Week 1","type":"book"},{"authors":null,"categories":null,"content":"Day 1 Slides (pdf/html)\nDay 2 Slides (pdf/html)\nDay 3 Slides (pdf/html)\n","date":1648252800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648252800,"objectID":"3ade9820d006faca63ac14e1b36932e2","permalink":"https://deepbas.netlify.app/courses/stat230/week1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat230/week1/","section":"courses","summary":"Day 1 Slides (pdf/html)\nDay 2 Slides (pdf/html)\nDay 3 Slides (pdf/html)","tags":null,"title":"Week 1","type":"book"},{"authors":null,"categories":null,"content":"Monday, 03/07\nBefore Class  Skim through tree-based methods from ISLR labs. We will gently go over classification trees and random forest models only.   In Class  Basic decision tree and random forest Final project description Midterm 3 review  Day 24 Slides (pdf/html)\n Link to In Class Tutorial  Wednesday, 03/09\n Midterm III   ","date":1646524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646524800,"objectID":"8407b47c96fa1e3d87dbcd9eff3ae924","permalink":"https://deepbas.netlify.app/courses/stat220/week10/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week10/","section":"courses","summary":"Monday, 03/07\nBefore Class  Skim through tree-based methods from ISLR labs. We will gently go over classification trees and random forest models only.   In Class  Basic decision tree and random forest Final project description Midterm 3 review  Day 24 Slides (pdf/html)","tags":null,"title":"Week 10","type":"book"},{"authors":null,"categories":null,"content":"Monday, 02/28\nBefore Class  Read sections 5.7-5.8 and sections 6.3-6.7 of Data Science: A First Introduction   In Class  KNN Model accuracy  Day 21 Slides (pdf/html)\n Link to In Class Tutorial  Wednesday, 03/02\nBefore Class  Continue reading section 6.3-6.7 of Data Science: A First Introduction. Also, skim through section 4.3 (page 133-139) of An Introduction to Statistical Learning. You need to download the book!   In Class  More on resampling and logistic regression  Day 22 Slides (pdf/html)\n Link to In Class Tutorial  Friday, 03/04\nBefore Class  Read Chapter 9 of Data Science: A First Introduction. Also, skim through section 12.4.2 (page 521-526) of An Introduction to Statistical Learning on hierarchical clustering.   In Class  K-means Clustering  Day 23 Slides (pdf/html)\n Link to In Class Tutorial  ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"50c41049ef7ec433c630cca69b84f0b0","permalink":"https://deepbas.netlify.app/courses/stat220/week9/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week9/","section":"courses","summary":"Monday, 02/28\nBefore Class  Read sections 5.7-5.8 and sections 6.3-6.7 of Data Science: A First Introduction   In Class  KNN Model accuracy  Day 21 Slides (pdf/html)","tags":null,"title":"Week 9","type":"book"},{"authors":null,"categories":null,"content":"Monday, 02/21\nBefore Class  Please go over the solution of the tutorial from last Friday and practice further using the resources provided there. Read through Mastering Shiny Chapter 2-4. Please watch through a video on a simple demonstration of building a Shiny app below if you are still disoriented about how Shiny actually works.    In Class We will again do some more hands-on activity to build Shiny apps. We will also see how to integrate interactive maps like leaflet in Shiny.\nDay 18 Slides (pdf/html)\n Link to In Class Tutorial  Wednesday, 02/23\nBefore Class  Continue reading Mastering Shiny Chapter 2-4, especially focusing on reactivity.   We will go over some more Shiny app examples and will also integrate text analysis with Shiny using Twitter sentiment analysis. If you don\u0026rsquo;t have a twitter account yet, please open a twitter account before class and have it verified.\nIn Class We will once more do some more hands-on activity to build Shiny apps in the context of twitter sentiment analysis.\nDay 19 Slides (pdf/html)\n Link to In Class Tutorial  Friday, 02/25\nBefore Class  Skim sections 5.1-5.6 of Data Science: A First Introduction   In Class We will start basic classification and learn the basics of k-nearest-neighbors (kNN).\nDay 20 Slides (pdf/html)\n Link to In Class Tutorial  ","date":1645315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645315200,"objectID":"9b40e3fd7e8eb1c68f93476c839c78b8","permalink":"https://deepbas.netlify.app/courses/stat220/week8/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week8/","section":"courses","summary":"Monday, 02/21\nBefore Class  Please go over the solution of the tutorial from last Friday and practice further using the resources provided there. Read through Mastering Shiny Chapter 2-4. Please watch through a video on a simple demonstration of building a Shiny app below if you are still disoriented about how Shiny actually works.","tags":null,"title":"Week 8","type":"book"},{"authors":null,"categories":null,"content":"Monday, 02/14\n In-class Midterm II   Wednesday, 02/16\nBefore Class  Please continue reading Irizarry Chapter 24 on web scraping. Go over the slides from last Friday and practice using CSS Selector Gadget.   In class  Scraping multiple pages Some Interactive Plots and Data Tables  Day 16 Slides (pdf/html)\n Link to In Class Activity  Friday, 02/18\nBefore Class  Read through basic intro to Shiny app and watch through a simple video on building a Shiny App below   It would be great if you could go through all of the video, but please go through until the 15 min mark. Here\u0026rsquo;s the video:\n In Class Today\u0026rsquo;s class will be more like a lab session, where we will see some basic Shiny App implementations. We will practice some more towards the end, so please come prepared to do some hands-on activities!!\n Link to Shiny Tutorial  ","date":1644710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644710400,"objectID":"cbf5e92232f783a5b58c90603a30e169","permalink":"https://deepbas.netlify.app/courses/stat220/week7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week7/","section":"courses","summary":"Monday, 02/14\n In-class Midterm II   Wednesday, 02/16\nBefore Class  Please continue reading Irizarry Chapter 24 on web scraping. Go over the slides from last Friday and practice using CSS Selector Gadget.","tags":null,"title":"Week 7","type":"book"},{"authors":null,"categories":null,"content":"There will be an in-class Midterm II on Monday, 02/14 next week!  Monday, 02/07\n Midterm Break!   Wednesday, 02/09\nBefore Class  Please read GW 21.1-21.5 on iterations and functionals.   In Class  Iterations and functionals  Day 14 Slides (pdf/html)\n Link to In Class Activity  Friday, 02/11\nBefore Class  Please read Irizarry Chapter 24 on web scraping.   Also, it may be useful to go over this short video on the simple demonstration of web scraping before class. We will go over this in detail in class.\n In Class  Web Scraping Basics  Day 15 Slides (pdf/html)\n Link to In Class Activity  ","date":1644278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644278400,"objectID":"881b312299e43cef9596c3f292defe7b","permalink":"https://deepbas.netlify.app/courses/stat220/week6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week6/","section":"courses","summary":"There will be an in-class Midterm II on Monday, 02/14 next week!  Monday, 02/07\n Midterm Break!   Wednesday, 02/09\nBefore Class  Please read GW 21.1-21.5 on iterations and functionals.","tags":null,"title":"Week 6","type":"book"},{"authors":null,"categories":null,"content":"Monday, 01/31\nBefore Class  Please read GW Chapter 14 on strings and regular expressions. Continue reading Irizarry Case Studies 1 and 2 from chapter 25 on string parsing.   In Class  More string manipulation tools  Day 11 Slides (pdf/html)\n Link to In Class Activity  Wednesday, 02/02\nBefore Class  Please gently read IDS Chapter 27 on text mining. Also, read this article by David Robinson.   In Class  advanced text mining analyze tweets brief overview of tidytext  Day 12 Slides (pdf/html)\n Link to In Class Activity  Friday, 02/04\nBefore Class  Please read GW Chapter 19 on writing functions in R. If you have no prior experience in writing functions, the following video would give you a nice overview (using R).    In Class  Basic functions Application to word cloud visualizations  Day 13 Slides (pdf/html)\n Link to In Class Activity  ","date":1643500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643500800,"objectID":"568c73761430176f46860cb3c68a4e66","permalink":"https://deepbas.netlify.app/courses/stat220/week5/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week5/","section":"courses","summary":"Monday, 01/31\nBefore Class  Please read GW Chapter 14 on strings and regular expressions. Continue reading Irizarry Case Studies 1 and 2 from chapter 25 on string parsing.   In Class  More string manipulation tools  Day 11 Slides (pdf/html)","tags":null,"title":"Week 5","type":"book"},{"authors":null,"categories":null,"content":"Monday, 01/24\nBefore Class  Please read GW Chapter 12 on tidy data and reshaping data. Start working on individual HW3 and submit Group HW2 on GitHub before class.   In Class  Wide Vs Long data Some more data cleaning tools  Day 8 Slides (pdf/html)\n Link to In Class Activity  Wednesday, 01/26\nBefore Class  Please read GW Chapter 11 and skim through IDS Chapter 5 on importing data. If you need more indepth description of dates/times in lubridate, read GW Chapter 16   In Class  Data Import tools More factors using forcats  Day 9 Slides (pdf/html)\n Link to In Class Activity  Friday, 01/28\nBefore Class Please go over the group homework 3 and come to class with questions. I will also go over previous homeworks and will take any questions you may have on the homeworks. Come prepared! Homework solutions will be uploaded to the github repo on class resources. Group homework solutions will be provided to the respective repos.\nI will also detail our first paired mini-project for the quarter during class.\n Please gently read Irizarry Case Studies 1 and 2 from chapter 25 on string parsing.   In Class  Mini project 1 overview More on date/time Basic overview of stringr  Day 10 Slides (pdf/html)\n Link to In Class Activity  ","date":1642896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642896000,"objectID":"0678af490f47d8b85e40ef819921412b","permalink":"https://deepbas.netlify.app/courses/stat220/week4/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week4/","section":"courses","summary":"Monday, 01/24\nBefore Class  Please read GW Chapter 12 on tidy data and reshaping data. Start working on individual HW3 and submit Group HW2 on GitHub before class.   In Class  Wide Vs Long data Some more data cleaning tools  Day 8 Slides (pdf/html)","tags":null,"title":"Week 4","type":"book"},{"authors":null,"categories":null,"content":"Monday, 01/17\nBefore Class  Please read GW Chapter 5, Irizarry Sections 4.1-4.11(optional), and start working on individual HW2.   Also, it may be useful to go over this short video on the basics of dplyr before class.\n In Class  Basic data wrangling Implementation of the five verbs  Day 6 Slides (pdf/html)\n Link to In Class Activity  Wednesday, 01/19\nBefore Class Finish Individual HW2 by 10 PM, and start working on Group HW2. The first midterm will be held in class on Friday, 01/22.\n Please read R Style Guide Chapters 2,4, and 5, Irizarry Chapter 23, GW Chapter 13(optional)   It would also be nice to go over this short video in joining data with dplyr.\n In Class  More data wrangling Basic data joins (two-table verbs)  Day 7 Slides (pdf/html)\n Link to In Class Activity  Friday, 01/21\n MIDTERM I   ","date":1642291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642291200,"objectID":"d207397eed9dc38be3242c70a1ec0bb9","permalink":"https://deepbas.netlify.app/courses/stat220/week3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week3/","section":"courses","summary":"Monday, 01/17\nBefore Class  Please read GW Chapter 5, Irizarry Sections 4.1-4.11(optional), and start working on individual HW2.   Also, it may be useful to go over this short video on the basics of dplyr before class.","tags":null,"title":"Week 3","type":"book"},{"authors":null,"categories":null,"content":"Monday, 01/10\nBefore Class  Please read GW Chapter 3, GW Chapter 28 and Jenny Bryan\u0026rsquo;s Chapter 25.   It is useful to go over this interesting video on Basics of ggplot2 before class.\n Also, please start to work on HW1 that will be due at 10 PM on Wednesday, 01/12. You are required to submit the HW via GitHub (i.e., push your submission to the GitHub repo named \u0026lsquo;hw1-username\u0026rsquo; under the class GitHub organization, Stat220).\nIn Class  Basic R Objects Basic Visualizations with ggplot2  Day 3 Slides (pdf/html)\n Link to In Class Activity  Wednesday, 01/12\nBefore Class We learned about basics of R objects and just scratched the surface of data visualization using ggplot2. We will delve deeply into ggplot2 in this class.\n Continue reading GW Chapter 3, GW Chapter 28, and Irizarry Chapter 10   In Class  More ggplot2 layers Advanced visualizations  Day 4 Slides (pdf/html)\n Link to In Class Activity  Friday, 01/14\nBefore Class  Go over previous in-class activities, lectures. Start working on Group HW 1.   Please skim through Coordinate Systems and Axes, Visualizing Spatial Data and Leaflet R Package   In Class  Some more useful aesthetics More advanced maps Simple interactive maps with leaflet  Day 5 Slides (pdf/html)\n Link to In Class Activity  ","date":1641600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641600000,"objectID":"f229677111a04328cdaa846757156cc3","permalink":"https://deepbas.netlify.app/courses/stat220/week2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week2/","section":"courses","summary":"Monday, 01/10\nBefore Class  Please read GW Chapter 3, GW Chapter 28 and Jenny Bryan\u0026rsquo;s Chapter 25.   It is useful to go over this interesting video on Basics of ggplot2 before class.","tags":null,"title":"Week 2","type":"book"},{"authors":null,"categories":null,"content":"Wednedsay, 01/05 Welcome!\nDay 1 Slides\nBefore next class Students are expected to open a GitHub account, verify it, and connect RStudio with that account using personal access token (PAT) credential methods. Once this step is finished, students should clone a git repository on their local machines using git version control methods (tutorial), modify the files, save, commit, and push the changes to the remote repository associated with their GitHub username. Here is one useful video about Git and Github, Introduction to Git and Github.\n Also, please read through all the course set-up modules and test your R skills by doing the test-assignment named hw0-username accessible here, Stat 220 Organization GitHub page. If you could comfortably do the R component of this test assignment, then you are in pretty good shape. In addition, if you can successfully push your modifications, then you should be set up for this course, technically.\nFriday, 01/07 Before Class  Please read GW RMarkdown, GW Chapter 1 and GW Chapter 20 to prepare for the class.   In Class   R-markdown and Reproducible workflows\n  Getting up and running with R markdown\n  Basic R-codes practice\n  Link to In Class Activity\n  Day 2 Slides (pdf/html)\nOptional (if you need more review):\nIf it has been a LONG time since you took STAT 120 or used R, then this crash course is helpful: programming basics primer from RStudio. These resources are also useful if you feel like you are starting from scratch: R and RStudio Basics and R Markdown.\n","date":1641427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641427200,"objectID":"caf90b5615af7388cd73d0d55609001d","permalink":"https://deepbas.netlify.app/courses/stat220/week1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/week1/","section":"courses","summary":"Wednedsay, 01/05 Welcome!\nDay 1 Slides\nBefore next class Students are expected to open a GitHub account, verify it, and connect RStudio with that account using personal access token (PAT) credential methods.","tags":null,"title":"Week 1","type":"book"},{"authors":null,"categories":null,"content":"Do‚Äôs and Don‚Äôt of collaboration for group assignments  All group members should contribute to each question. Don‚Äôt delegate questions among group members. If a group member didn‚Äôt contribute sufficiently to an assignment, please let me know via email. If you aren‚Äôt able to contribute sufficiently to an assignment, please send me an email explaining your situation. An isolated ‚Äúmiss‚Äù of group work due to personal issues is understandable, but you will lose points from your (individual) group grade if I see a pattern of multiple misses without a reasonable explanation.  Do‚Äôs and Don‚Äôt of collaboration for individual assignments  You can discuss homework problems with classmates but you must write up your own homework solutions and do your own work in R (no sharing commands or output).  Do not share R commands/code in any way, including, but not limited to, sending commands via email, slack, text, or showing commands in a shared screen with the intention of showing a classmate your solution to a problem. You can share a screen to help troubleshoot a coding problem in R.   You can use the following resources to complete your homework:  Carleton faculty (myself, other math fac, etc) discussions with classmates (see above) or knowledgeable friends Carleton resources like stats lab assistants student solutions provided in the back of your student textbook or in the student solution manual.   You cannot use any resources other than the ones listed above to complete assignments (homework, reports, etc) for this class. (e.g. you cannot use a friend‚Äôs old assignments or reports, answers found on the internet, textbook (instructor) solutions manual, etc.)  Examples that violate the academic integrity policy  sending your .Rmd homework file to another person in the class receiving an .Rmd homework file from another person sharing a screen and copying code, verbatim, from another person sending/receiving R commands neglecting to acknowledge classmates with whom you worked with on an assignment  Format and Content Submit via GitHub (for most assignments) an organized and correctly ordered assignment.\n Content: Good data scientists need to do more than just write code; they should be able to interpret and explain their analyzes.  Provide a written answer first, followed by any required R code and output. Use complete sentences when answering any problem that requires an explanation or overall problem summary.   When including code:  Be sure to show the natural sequence of work needed to answer the problem. Include brief comments explain your code steps. Do not include typos or unnecessary commands/output. Always include code output.   At the top of each individual assignment include the names of classmates that you worked with on all or part of the assignment (but each person must write up their assignment on their own)   Disability Accommodations: Carleton College is committed to providing equitable access to learning opportunities for all students. The Disability Services office (Henry House, 107 Union Street) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability (e.g., mental health, attentional, learning, autism spectrum disorders, chronic health, traumatic brain injury and concussions, vision, hearing, mobility, or speech impairments), please contact disability@carleton.edu or call Sam Thayer (‚Äô10), Accessibility Specialist (x4464) or Chris Dallager, Director of Disability Services (x5250) to arrange a confidential discussion regarding equitable access and reasonable accommodations.\nAcademic Honesty: All work that you turn in under your name must follow Carleton‚Äôs academic integrity policy. The use of textbook solution manuals (physical or online solutions), homework, reports or exams done by past students are not allowed. Look at the College‚Äôs Writing Across the Curriculum website for additional guidance on plagiarism and how to avoid plagiarism in their writing.\n","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"4513a40e7e6396766663a099b861976b","permalink":"https://deepbas.netlify.app/courses/stat220/homework-in-stat-220/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/courses/stat220/homework-in-stat-220/","section":"courses","summary":"Do‚Äôs and Don‚Äôt of collaboration for group assignments  All group members should contribute to each question. Don‚Äôt delegate questions among group members. If a group member didn‚Äôt contribute sufficiently to an assignment, please let me know via email.","tags":null,"title":"Assignments in Stat 220","type":"book"},{"authors":null,"categories":null,"content":"Overview If you are using the maize RStudio server, then you can connect to GitHub without any extra software downloads. If you are using RStudio on your computer, then you will need to download Git software (as directed in Software in Stat 220) to use GitHub connected projects.\nI will host all of our course materials on GitHub, and you will use GitHub to submit homework and collaborate on projects.\nGetting setup with Git and GitHub If you are not working on the maize RStudio server, then make sure that you have installed all of the software mentioned in Software in Stat 220. In addition, you should install the usethis and gitcreds R packages.\nEveryone needs to connect Git and GitHub by doing the following:\n  Register for account on GitHub (https://github.com/). I recommend using a username that incorporates your name (e.g., dbastola). Please use your Carleton email with this account.\n  Setup options in Git by running the following code chunk in your console:\nlibrary(usethis) use_git_config(user.name = \u0026quot;Jane Doe\u0026quot;, user.email = \u0026quot;jane@example.org\u0026quot;)  changing the first two lines to your own name and email (this should be the email associated with your GitHub account).\n  (Optional) This step explains how to use GitHub on RStudio without having to enter your password every time to connect. This is an optional step since you can still use GitHub without setting up a SSH key or caching credentials, you will just need to enter your password every time you push or pull with GitHub. Pick one method below: cache credentials for HTTPS or SSH. GitHub recommends HTTPS, so I recommend trying that first.\n  Strongly recommended: Cache credential using a HTTPS linked project.\nComplete the steps in Section 9.3 of Happy Git with R to get a personal access token (you will need to install the usethis package first). Then, follow the directions in Section 9.4.1 of Happy Git with R (I\u0026rsquo;ve had good luck with the gitcreds package, and it is installed on maize.)\n  A second method is to generate a SSH key. First check to see if you have a SSH key. Go into the shell (again, through RStudio Tools -\u0026gt; Shell) and complete on this page http://happygitwithr.com/ssh-keys.html, which is Chapter 11 in Happy Git with R.\n    (optional) Follow the instructions here (http://happygitwithr.com/push-pull-github.html) to ensure you can connect to GitHub from your computer. If you can\u0026rsquo;t get this command line push/pull to work that is fine. Try connecting to GitHub via RStudio, as detailed next. If you can\u0026rsquo;t connect via RStudio then talk to me in student hours.\n  Individual assignments If you followed the suggestions in the Using Rstudio in Stat 220 page, then you should already have an assignments folder on your computer or maize account.\nEach new assignment/project will be posted as a repository on GitHub and added directly to your account (within the Stat220 organization). This repository will contain assignment details (README, .Rmd).\nCreating an individual assignment repo and project   Go to our course GitHub organization page (https://github.com/stat220/) and find your homework repo, such as hw-1-username (where your username is attached).\n  Enter the online assignment repository on GitHub. Click the green \u0026ldquo;Code\u0026rdquo; button. Most of you should just use the default setting which is to \u0026ldquo;clone\u0026rdquo; (copy) using HTTPS. Click the clipboard to the right of the URL to copy the repo location. (If you are using SSH, make sure it says \u0026ldquo;Clone with SSH\u0026rdquo; in bold in the top left of the pop-up box. If not, click the \u0026ldquo;SSH\u0026rdquo; button and copy the link in the box to your clipboard.)\n  Now open up RStudio and create a project as follows:\n   Click the Project button in the upper right corner of your RStudio window and select New Project\u0026hellip;.      Select Version Control and then New Project         Paste the link you just copied into the Repository URL box. Leave the Project directory name blank (or keep the auto-filled name). Use the Browse button to find your assignments folder, then click Create Project     Warning: If you received an error in the above steps, you may have to clone with HTTPS instead of SSH (or vice versa). You can do this by again clicking on the \u0026ldquo;Clone or Download\u0026rdquo; button in the repository page, then clicking \u0026ldquo;Use HTTPS\u0026rdquo; in the top right of the pop-up box. Now copy the link and repeat this step.\nWorking on your assignment An RStudio project should now open, which will allow you to start working on your homework assignment. You should see the project assignment name in the top right side of Rstudio. You will probably see a blank console screen when you open a new project. Look in the Files tab for your homework .Rmd file. Click on whatever file you want to edit (probably the .Rmd file) and edit away. Make sure that your current assignment\u0026rsquo;s project is the one open and showing in the upper rightproject name. To open a project, click on the .Rproj file or use the Open Project\u0026hellip; option available in the upper right project link.\nCommits After you make changes to the homework assignment, commit them. What are commits you ask? Commits are essentially taking a snapshot of your projects. Commits save this snapshot to your local version of Git (located on your hard drive or the maize server). For example, if I make changes to a code so that it prints \u0026ldquo;Hello world\u0026rdquo;, and then commit them with an informative message, I can look at the history of my commits and view the code that I wrote at that time. If I made some more changes to the function that resulted in an error, I could go back to the commit where the code was originally working. This prevents you from creating several versions of your homework (homework-v1, homework-v2, \u0026hellip;) or from trying to remember what your code originally looked like.\nYou can make commits in the Git tab in RStudio.\n   Click the Commit button in the Git tab. Check the boxes of the files that you want to commit, enter your commit message (briefly state what changes have been made), then hit Commit. You can read how to do this in RStudio in more detail here: http://r-pkgs.had.co.nz/git.html#git-commit.\nTwo things about committing.\n You should commit somewhat frequently. At minimum, if you\u0026rsquo;re doing a homework assignment, you should make a commit each time that you\u0026rsquo;ve finished a question. Leave informative commit messages. \u0026ldquo;Added stuff\u0026rdquo; will not help you if you\u0026rsquo;re looking at your commit history in a year. A message like \u0026ldquo;Added initial version of hello-world function\u0026rdquo; will be more useful.  Pushing changes to Github At some point you\u0026rsquo;ll want to get the updated version of the assignment back onto GitHub, either so that we can help you with your code or so that it can be graded. You will also want to push work frequently when you have a shared GitHub repo for project collaborations (i.e. more than one person is working on a project and code). If you are ready to push, you can again click on the \u0026ldquo;Up\u0026rdquo; Push arrow in the Git tab or in the Commit pop-up window or in the Git tab (shown above).\nTo \u0026ldquo;turn in\u0026rdquo; an assignment, all you need to do is push all your relevant files to Github by the deadline.\nGroup work Collaborative Github assignments are pretty similar to individual assignments.\nCreating a group/partner assignment repo and project Go to our course GitHub organization page(https://github.com/stat220/) and find the repo for your group, for example if your group name is \u0026ldquo;team01\u0026rdquo; the you might find the mp1-team01 repo. Clone this repo to your computer/maize account using the same steps done for an individual assignment (see steps 2-3).\nWorking with collaborative repos For group homework, I suggest that only the recorder edit the group-homework-x.Rmd file to avoid merge conflicts! Other group members can create a new Markdown doc to run and save commands. Only the recorder needs to push changes (answers) to the Github repo and all others can then pull these changes (i.e. the final answers) after the HW is submitted.\nWhen you are working together on a Github project, you should commit and push your modifications frequently. You will also need to frequently pull updates from Github down to your local version of RStudio. These updates are changes that your teammates have made since your last pull. To pull in changes, click the \u0026ldquo;Down\u0026rdquo; Pull arrow in the Git tab (shown above).\nIf you get an error about conflict after pulling or pushing, don\u0026rsquo;t freak out! This can happen if you edit a file (usually an .Rmd or .R file) in a location that was also changed by a teammate. When this happens you should attempt to fix the merge conflict. Take a look at this resource site and try to fix the merge conflict in Rstudio. Katie also created a diagram shown below that \u0026ldquo;explains\u0026rdquo; how and when conflicts will likely happen and how you can resolve the problems in Rstudio. If that doesn\u0026rsquo;t work contact me!\n   Additional resources  Happy Git and GitHub for the useR Rstudio, Git and GitHub Interactive learning guide for Git GitHub Guides Git setup for Windows (video) Git setup for Mac (video) How to clone, edit, and push homework assignments with GitHub Classroom (video)  Acknowledgements Most of this content in this guide was taken from https://github.com/jfiksel/github-classroom-for-students, edited for our classroom use by Katie St. Clair.\nReuse This guide is licensed under the CC BY-NC 3.0 Creative Commons License.\n","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"32bc61ce2c494b4b91b34a8f3357223f","permalink":"https://deepbas.netlify.app/courses/stat220/github-in-stat-220/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/courses/stat220/github-in-stat-220/","section":"courses","summary":"Overview If you are using the maize RStudio server, then you can connect to GitHub without any extra software downloads. If you are using RStudio on your computer, then you will need to download Git software (as directed in Software in Stat 220) to use GitHub connected projects.","tags":null,"title":"GitHub Classroom Guide for Students in Stat 220","type":"book"},{"authors":null,"categories":null,"content":"Key points\nThere will be a lot of Rstudio content thrown your way this term, most in the form of .Rmd Markdown files. To stay organized, I strongly suggest you create a stat220 folder that contains the following subfolders:\n stat220 folder   Assignments: This folder will contain subfolders for each assignment. Each assignment subfolder (e.g.¬†homework1, homework2, ‚Ä¶) will be a Github connected RStudio project that you will create once an assignment is posted.\n  Content: This folder should be used to save any non-assignment files (e.g.¬†slides, examples) for this class. You will create this subfolder by creating an RStudio project (see step 5 below).\n    To get started with this organization, follow the steps below.\n File organization: Using maize The server (online) version of Rstudio is run from a unix server. You can navigate this file system using unix commands, but I assume that most or all of you will just use Rstudio to access your files on this server.\n 1. In Rstudio, click the Files tab in the lower right-hand window. Note: this is not the same as the File menu option.      2. Verify that you are in your HOME folder (should simply say Home right under the New Folder button). To navigate to your Home folder (if somehow you are not in it), click the ‚Ä¶ button (far right side of the Files tab) and enter a ~ (tilde) symbol      3. Click the New Folder button and name the folder stat220.      4. Click on this newly created (empty) stat220 folder. Within the folder create another New Folder and name it assignments.       5. Within the stat220 folder, create an RStudio project called content with the following steps:\n a. Click the Project button in the upper righthand corner of your RStudio window and select New Project‚Ä¶.        b. Select New Directory and then New Project         c. Enter content as the Directory name and use the Browse button to find your stat220 folder. Then click Create Project.      d. You should now have a new folder called content in your stat220 folder and this folder will contain an RStudio project .Rproj. Feel free to add subfolders to this content folder (e.g.¬†slides, examples, etc).     Warning: Do not create an RStudio project in the main stat220 folder because it is not good practice to have RStudio projects in subfolders of another project (e.g. a project within a project is not recommended).\n File organization: Using your own Rstudio Create a folder called stat220 somewhere on your computer. Within this folder create an assignments subfolder. Then complete step 5 from above to create a content RStudio project folder.\n RStudio projects Once you‚Äôve created a project, your R session should be running within that project folder. You can check which project you are in by checking the project name in the upper righthand part of your RStudio window. Here we see the content project is open:\n   Running R from an RStudio project sets your working directory to the project folder:\n   This allows for easy file path access to all files related to this project.\nTo start a project, click on the .Rproj file or use the Open Project‚Ä¶ option shown in step 5 above.\n Best practices (or what not to do)  Never save files to a lab computer hard drive (e.g. desktop, downloads, etc). They will be erased when you log off. Do not use gmail as a file storage system! Avoid emailing yourself files that you created (and saved) on a lab computer. Eventually you will lose work this way. Avoid using online versions of google drive and dropbox. Similar to gmail, downloading, editing a doc, then uploading it back to drive/dropbox is another great way to lose work. Avoid this and this.  Acknowledgments This installation guide is based on the guide from Katie St. Clair\nReuse Text and figures are licensed under Creative Commons Attribution CC BY-NC 3.0.\n","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"2ed2ef5cf1df124b74e607f04b145ca4","permalink":"https://deepbas.netlify.app/courses/stat220/rstudio-in-stat-220/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/courses/stat220/rstudio-in-stat-220/","section":"courses","summary":"Key points\nThere will be a lot of Rstudio content thrown your way this term, most in the form of .Rmd Markdown files. To stay organized, I strongly suggest you create a stat220 folder that contains the following subfolders:","tags":null,"title":"Using Rstudio in Stat 220","type":"book"},{"authors":null,"categories":null,"content":"R markdown Cheatsheet\nggplot2 Cheatsheet\ndplyr Cheatsheet\nlubridate Cheatsheet\nstringr Cheatsheet\nregex Cheatsheet\nshiny Cheatsheet\ncaret Cheatsheet\nleaflet Cheatsheet\n","date":1641427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641427200,"objectID":"5376380bb06df1ead3ca58ff8f84290c","permalink":"https://deepbas.netlify.app/courses/stat220/cheatsheets/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/cheatsheets/","section":"courses","summary":"R markdown Cheatsheet\nggplot2 Cheatsheet\ndplyr Cheatsheet\nlubridate Cheatsheet\nstringr Cheatsheet\nregex Cheatsheet\nshiny Cheatsheet\ncaret Cheatsheet\nleaflet Cheatsheet","tags":null,"title":"Link to cheatsheets","type":"book"},{"authors":null,"categories":null,"content":"Simple Data file\nMedium Data file\nTricky Data file\n","date":1641427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641427200,"objectID":"dccd113fd665dd04fa7ca86a9c0ec59f","permalink":"https://deepbas.netlify.app/courses/stat220/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/data/","section":"courses","summary":"Simple Data file\nMedium Data file\nTricky Data file","tags":null,"title":"Link to datasets","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://deepbas.netlify.app/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Deepak Bastola"],"categories":["machine learning","data science","prediction"],"content":"It would be nice to predict the number of positive covid cases depending on past cases evolution. Regression models based on recurrent neural networks (RNNs) are proven to identify patterns in time series data and this allows us to make accurate short-term predictions.\nThe model used in the following example is based on long-term short-term memory (LSTM) model that uses more than one features to make informed predictions. LSTMs are recurrent neural networks that avoid the vanishing gradient problem prevalent in feed-forward type of algorithms by imposing filtering mechanisms in the gates using a technique known as back-propagation.\nThe following set of codes loads all the required Python libraries, packages, and subroutines required for LSTM modeling. This blog post is just intended to give a high level summary of how to realize a covid case count prediction in the United States using some convenient features readily available.\n# Import various libraries and routines needed for computation import math import pandas as pd import numpy as np import tensorflow as tf import matplotlib.pyplot as plt import keras.backend as K from math import sqrt from numpy import concatenate from matplotlib import pyplot from pandas import read_csv, DataFrame from sklearn.preprocessing import MinMaxScaler, LabelEncoder from sklearn.metrics import mean_squared_error, mean_absolute_error from keras.models import Sequential from keras.layers import Dense, Dropout, LSTM from keras.callbacks import EarlyStopping from datetime import date, timedelta, datetime  # Read in the data file that has relevant features df = pd.read_csv('covid_final.csv') dataset = df.set_index(['date']) # Drop the last 10 row as they are incomplete dataset.drop(dataset.tail(10).index, inplace = True) values = dataset.values # Store the indexes (i.e., dates) date_index = dataset.index  # Clean up the dataset more for predictions and inverse transformations (Re-scaling) data_clean = dataset.copy() data_clean_ext = dataset.copy() data_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed'] data_clean.tail()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  new_cases_smoothed reproduction_rate new_tests_smoothed_per_thousand new_vaccinations_smoothed_per_million people_fully_vaccinated_per_hundred total_boosters_per_hundred stringency_index   date            2022-03-08 38934.286 0.65 2.748 621 65.24 28.89 53.24   2022-03-09 36641.429 0.66 2.699 601 65.25 28.91 53.24   2022-03-10 36330.429 0.69 2.613 583 65.27 28.94 53.24   2022-03-11 36104.714 0.71 2.580 557 65.29 28.97 53.24   2022-03-12 35464.143 0.71 2.561 540 65.30 28.99 53.24     # number of rows in the data nrows = data_clean.shape[0]  The day-to-day case counts can be regarded as a time series and the data needs to be prepared before training a supervised learning model. For LSTM, the data is composed of inputs and outputs, and the inputs can be seen as a moving window blocks consisting of the feature values to predict the outcome. The size of the window is a free parameter that the user must optimize.\n# Convert the data to numpy values np_data_unscaled = np.array(data_clean) np_data = np.reshape(np_data_unscaled, (nrows, -1))  # ensure all data is float values = values.astype('float64')  # Transform the data by scaling each feature to a range between 0 and 1 scaler = MinMaxScaler() np_data_scaled = scaler.fit_transform(np_data_unscaled)  # Creating a separate scaler that works on a single column for scaling predictions scaler_pred = MinMaxScaler() df_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed']) np_cases_scaled = scaler_pred.fit_transform(df_cases)  In LSTM methodology, it is required to reshape the input to be a 3D tensor of samples, time steps, and features. This is more important when we are fitting the model later.\n# Set the sequence length - this is the timeframe used to make a single prediction sequence_length = 31 # rolling window size # Prediction Index index_cases = dataset.columns.get_loc(\u0026quot;new_cases_smoothed\u0026quot;) # Split the training data into train and train data sets # As a first step, we get the number of rows to train the model on 80% of the data train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8) # Create the training and test data train_data = np_data_scaled[0:train_data_len, :] test_data = np_data_scaled[train_data_len - sequence_length:, :] # The RNN needs data with the format of [samples, time steps, features] # Here, we create N samples, sequence_length time steps per sample, and 6 features def partition_dataset(sequence_length, data): x, y = [], [] data_len = data.shape[0] for i in range(sequence_length, data_len): x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn y.append(data[i, index_cases]) #contains the prediction values for validation, for single-step prediction # Convert the x and y to numpy arrays x = np.array(x) y = np.array(y) return x, y # Generate training data and test data x_train, y_train = partition_dataset(sequence_length, train_data) x_test, y_test = partition_dataset(sequence_length, test_data)  # Configure the neural network model model = Sequential() # Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables n_neurons = x_train.shape[1] * x_train.shape[2] model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2]))) model.add(Dense(1))  # Check-points and early stopping parameters make our modeling easier from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # Compiling the LSTM model.compile(optimizer = 'adam', loss = 'mean_squared_error')  # Specfy the file and file path for the best model checkpoint_path = 'my_best_model.hdf5' checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') earlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0) callbacks = [checkpoint, earlystopping]  # Training the model epochs = 300 batch_size = 20 history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks = callbacks, verbose = 0)  # Load the best model from tensorflow.keras.models import load_model model_from_saved_checkpoint = load_model(checkpoint_path)  # Plot training \u0026amp; validation loss values plt.figure(figsize=(16,7)) plt.plot(history.history['loss'], label='train') plt.plot(history.history['val_loss'], label='test') plt.legend() plt.show()     # Get the predicted values y_pred_scaled = model_from_saved_checkpoint.predict(x_test)  # Unscale the predicted values y_pred = scaler_pred.inverse_transform(y_pred_scaled)  # reshape y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))  # Mean Absolute Error (MAE) MAE = mean_absolute_error(y_test_unscaled, y_pred) print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}') # Mean Absolute Percentage Error (MAPE) MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100 print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %') # Median Absolute Percentage Error (MDAPE) MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100 print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')  # Plot of the true and predicted case counts plt.plot(y_test_unscaled, label='True') plt.plot(y_pred, label='LSTM') plt.title(\u0026quot;LSTM's_Prediction\u0026quot;) plt.xlabel('Time steps') plt.ylabel('Cases') plt.legend() plt.show()     # New data frame for predicting the next day count new_df = data_clean[-sequence_length:] # gets the last N days N = sequence_length  # Get the values of the last N day cases counts # scale the data to be values between 0 and 1 last_N_days = new_df[-sequence_length:].values last_N_days_scaled = scaler.transform(last_N_days)  # Create an empty list and Append past N days X_test_new = [] X_test_new.append(last_N_days_scaled) # Convert the X_test data set to a numpy array and reshape the data pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new)) pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))  # Print last price, predicted price, and change percent for the next day cases_today = np.round(new_df['new_cases_smoothed'][-1]) predicted_cases = np.round(pred_cases_unscaled.ravel()[0]) change_percent = np.round(100 - (cases_today * 100)/predicted_cases)  # Code used to produce this article in jupyter notebook in hugo academic blog post !jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=. !cat covid_analysis.md | tee -a index.md !rm covid_analysis.md  ","date":1647993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647993600,"objectID":"290915bf36768f5fbd9b8dae910b59e0","permalink":"https://deepbas.netlify.app/2022/03/23/how-to-predict-covid-case-counts-using-machine-learning-models/","publishdate":"2022-03-23T00:00:00Z","relpermalink":"/2022/03/23/how-to-predict-covid-case-counts-using-machine-learning-models/","section":"post","summary":"It would be nice to predict the number of positive covid cases depending on past cases evolution. Regression models based on recurrent neural networks (RNNs) are proven to identify patterns in time series data and this allows us to make accurate short-term predictions.","tags":[],"title":"How to predict covid case counts using machine learning models?","type":"post"},{"authors":null,"categories":null,"content":"  R/RStudio The use of the R programming language with the RStudio interface is an essential component of this course. You have two options for using RStudio:\n The server version of RStudio on the web at (https://maize.mathcs.carleton.edu). The advantage of using the server version is that all of your work will be stored in the cloud, where it is automatically saved and backed up. This means that you can access your work from any computer on campus using a web browser. The downside is that you have to share limited computational resources with each other! This limitation is especially pronounced this term when classes are being taught remotely. We are unsure what type of load will be placed on maize, so if you can download a local version of R and RStudio, I recommend you do so. Use your Carleton credentials to access your account and you need to be running the Carleton VPN (below) to access this server.\n A local version of RStudio installed on your machine. This option is highly recommended due to the computational resources this course demands. The downside to this approach is that your work is only stored locally, but I get around this problem by keeping all of my work on GitHub. You will learn how to use GitHub throughout the course. Both R and RStudio are free and open-source.\nInstalling R/RStudio (not needed if you are using the maize server)\n Download the latest version of R: https://cran.r-project.org/\n Download the free Rstudio desktop version: https://www.rstudio.com/products/rstudio/download/\n  Use the default download and install options for each. For R, download the ‚Äúprecompiled binary‚Äù distribution rather than the source code\nUpdating R/RStudio (not needed if you are using the maize server)\nIf you have used a local version of R/RStudio before and it is still installed on your machine, then you should make sure that you have the most recent versions of each program.\n To check your version of R, run the command getRversion() and compare your version to the newest version posted on https://cran.r-project.org/. If you need an update, then install the newer version using the installation directions above.\n In RStudio, check for updates with the menu option Help \u0026gt; Check for updates. Follow directions if an update is needed.\n  Did it work? (A sanity check after your install/update) Do whatever is appropriate for your operating system to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you haven‚Äôt written any code or made any figures yet!\nPut your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x \u0026lt;- 2 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 8 printed. If this happened, you‚Äôve succeeded in installing R and RStudio!\n   Git and GitHub Git is version control software that you install locally on your computer. Git is already installed on the maize RStudio server.\nGithub is a cloud-based service for hosting git projects. It allows multiple users to share and contribute to projects and it is how you will be submitting homework assignments and projects for this class. More information about Git and Github can also be found in Getting setup with Git and GitHub and Git and Github.\nIf you are using a local install of R/RStudio, then you will need to install Git.\nInstalling Git\nDirections for both Windows \u0026amp; Mac here: http://happygitwithr.com/install-git.html.\n If you are using maize, then there is nothing you need to install.\n Windows users should follow Option 1 in 6.2.\n Mac users can follow Option 1 in 6.3 if comfortable, otherwise follow Option 2\n Linux users can follow 6.4.\n   Slack We will use Slack for all course communication. Sign up for our course Slack team here! You will need to create an account with a username, and log in to read and post. You can download a standalone Slack application to your Mac, Windows, Linux and/or Android/iOS device. You can control whether you receive notifications on new posts by going to Preferences, as well as decide which ‚Äòchannels‚Äô to subscribe to. A ‚Äòchannel‚Äô is a discussion thread, which is used to organize communications into topics. You can learn more about Slack features here.\nSeveral channels have been set up for specific parts of the course. Feel free to ask questions anytime. You can browse the available channels in our team by clicking on ‚ÄúChannels‚Äù on the left-hand panel.\nThere is also an #anonymous channel that you can use if you‚Äôd like to ask a question or provide a comment anonymously. To have a post appear anonymously in this channel, compose a direct message to yourself (by clicking on your username in the ‚ÄòDirect Messages‚Äô menu on the left panel) that begins with /anon #anonymous your question. Then the text your question will be sent to the #anonymous channel by the anonymous bot. Zoom For synchronous class meeting and office hours we will use the Zoom video conferencing software. I recommend downloading the Zoom app for your operating system.\n LaTeX If you want to render R Markdown documents to PDFs, then you need to install LaTeX. Below are the recommended installers for Windows and Mac:\n MacTeX for Mac (3.2GB)\n MiKTeX for Windows (190MB)\n Alternatively, you can install the tinytex R package by running install.packages(\"tinytex\") in the console.\n  If you are using maize, then there is nothing you need to install.\n Acknowledgements This installation guide is based on the guide from Adam Loy.\n Resuse This work is licensed under the CC BY-NC 3.0 Creative Commons License.\n ","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641340800,"objectID":"685d99a20c764eab219d9cd094ebadb2","permalink":"https://deepbas.netlify.app/courses/stat220/software/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/courses/stat220/software/","section":"courses","summary":"R/RStudio The use of the R programming language with the RStudio interface is an essential component of this course. You have two options for using RStudio:\n The server version of RStudio on the web at (https://maize.","tags":null,"title":"Software in Stat 220","type":"courses"},{"authors":null,"categories":["R"],"content":"  R Markdown This is a R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can use asterisk mark to provide emphasis, such as *italics* or **bold** to produce italics or bold.\nYou can create lists with a combination of dash, plus, or asterisk :\n- Item 1 - Item 2 - Item 3 + Subitem 1 * Item 4  Item 1 Item 2 Item 3  Subitem 1  Item 4  You can embed Latex equations in-line as $\\frac{1}{n} \\sum_{i=1}^{n} x_{i}$ to produce \\(\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\) or in a new line as $$\\text{Var}(X) = \\frac{1}{n-1}\\sum_{i-1}^{n} (x_{i} - \\bar{x})^2 $$ to produce\n\\[\\text{Var}(X) = \\frac{1}{n-1}\\sum_{i-1}^{n} (x_{i} - \\bar{x})^2 \\]\nEmbed an R code chunk: ```r Use back ticks to create a block of code ``` You can also evaluate and display the results of R code. Each tasks can be accomplished in a suitably labeled chunk like the following:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   Read in data files simple_data \u0026lt;- read.csv(\u0026quot;https://deepbas.io/data/simple-1.dat\u0026quot;, ) summary(simple_data) ## initials state age time ## Length:3 Length:3 Min. :45.0 Length:3 ## Class :character Class :character 1st Qu.:47.5 Class :character ## Mode :character Mode :character Median :50.0 Mode :character ## Mean :52.0 ## 3rd Qu.:55.5 ## Max. :61.0 knitr::kable(simple_data, format = \u0026quot;html\u0026quot;)    initials  state  age  time      vib  MA  61  6:01    adc  TX  45  5:45    kme  CT  50  4:19      Hide the code Entering echo = FALSE option in the R chunk prevents the R code from being printed to your document and you just see the results/outputs.\n   initials  state  age  time      vib  MA  61  6:01    adc  TX  45  5:45    kme  CT  50  4:19       ","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"e4b616406659ad12fce5ad382dfb0e1f","permalink":"https://deepbas.netlify.app/2020/12/01/simple-r-markdown-example/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/2020/12/01/simple-r-markdown-example/","section":"post","summary":"R Markdown This is a R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown"],"title":"Simple R Markdown Example","type":"post"},{"authors":null,"categories":null,"content":"It would be nice to predict the number of positive covid cases depending on past case evolution. Regression models based on recurrent neural networks (RNNs) are proven to identify patterns in time series data and this allows us to make accurate short-term predictions.\nThe model used in the following example is based on long-term short-term memory (LSTM) model that uses more than one feature to make informed predictions. LSTMs are recurrent neural networks that avoid the vanishing gradient problem prevalent in feed-forward type of algorithms by imposing filtering mechanisms in the gates using a technique known as back-propagation.\nThe following set of codes loads all the required Python libraries, packages, and subroutines required for LSTM modeling. This blog post is just intended to give a high level summary of how to realize a covid case count prediction in the United States using some convenient features readily available.\n# Import various libraries and routines needed for computation import math import pandas as pd import numpy as np import tensorflow as tf import matplotlib.pyplot as plt import keras.backend as K from math import sqrt from numpy import concatenate from matplotlib import pyplot from pandas import read_csv, DataFrame from sklearn.preprocessing import MinMaxScaler, LabelEncoder from sklearn.metrics import mean_squared_error, mean_absolute_error from keras.models import Sequential from keras.layers import Dense, Dropout, LSTM from keras.callbacks import EarlyStopping from datetime import date, timedelta, datetime  # Read in the data file that has relevant features df = pd.read_csv('covid_final.csv') dataset = df.set_index(['date']) # Drop the last 10 row as they are incomplete dataset.drop(dataset.tail(10).index, inplace = True) values = dataset.values # Store the indexes (i.e., dates) date_index = dataset.index  # Clean up the dataset more for predictions and inverse transformations (Re-scaling) data_clean = dataset.copy() data_clean_ext = dataset.copy() data_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed'] data_clean.tail()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  new_cases_smoothed reproduction_rate new_tests_smoothed_per_thousand new_vaccinations_smoothed_per_million people_fully_vaccinated_per_hundred total_boosters_per_hundred stringency_index   date            2022-03-08 38934.286 0.65 2.748 621 65.24 28.89 53.24   2022-03-09 36641.429 0.66 2.699 601 65.25 28.91 53.24   2022-03-10 36330.429 0.69 2.613 583 65.27 28.94 53.24   2022-03-11 36104.714 0.71 2.580 557 65.29 28.97 53.24   2022-03-12 35464.143 0.71 2.561 540 65.30 28.99 53.24     # number of rows in the data nrows = data_clean.shape[0]  The day-to-day case counts can be regarded as a time series and the data needs to be prepared before training a supervised learning model. For LSTM, the data is composed of inputs and outputs, and the inputs can be seen as a moving window blocks consisting of the feature values to predict the outcome. The size of the window is a free parameter that the user must optimize.\n# Convert the data to numpy values np_data_unscaled = np.array(data_clean) np_data = np.reshape(np_data_unscaled, (nrows, -1))  # ensure all data is float values = values.astype('float64')  # Transform the data by scaling each feature to a range between 0 and 1 scaler = MinMaxScaler() np_data_scaled = scaler.fit_transform(np_data_unscaled)  # Creating a separate scaler that works on a single column for scaling predictions scaler_pred = MinMaxScaler() df_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed']) np_cases_scaled = scaler_pred.fit_transform(df_cases)  In LSTM methodology, it is required to reshape the input to be a 3D tensor of samples, time steps, and features. This is more important when we are fitting the model later.\n# Set the sequence length - this is the timeframe used to make a single prediction sequence_length = 31 # rolling window size # Prediction Index index_cases = dataset.columns.get_loc(\u0026quot;new_cases_smoothed\u0026quot;) # Split the training data into train and train data sets # As a first step, we get the number of rows to train the model on 80% of the data train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8) # Create the training and test data train_data = np_data_scaled[0:train_data_len, :] test_data = np_data_scaled[train_data_len - sequence_length:, :] # The RNN needs data with the format of [samples, time steps, features] # Here, we create N samples, sequence_length time steps per sample, and 6 features def partition_dataset(sequence_length, data): x, y = [], [] data_len = data.shape[0] for i in range(sequence_length, data_len): x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn y.append(data[i, index_cases]) #contains the prediction values for validation, for single-step prediction # Convert the x and y to numpy arrays x = np.array(x) y = np.array(y) return x, y # Generate training data and test data x_train, y_train = partition_dataset(sequence_length, train_data) x_test, y_test = partition_dataset(sequence_length, test_data)  # Configure the neural network model model = Sequential() # Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables n_neurons = x_train.shape[1] * x_train.shape[2] model.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2]))) model.add(Dense(1))  # Check-points and early stopping parameters make our modeling easier from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # Compiling the LSTM model.compile(optimizer = 'adam', loss = 'mean_squared_error')  # Specfy the file and file path for the best model checkpoint_path = 'my_best_model.hdf5' checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') earlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0) callbacks = [checkpoint, earlystopping]  # Training the model epochs = 300 batch_size = 20 history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks = callbacks, verbose = 0)  # Load the best model from tensorflow.keras.models import load_model model_from_saved_checkpoint = load_model(checkpoint_path)  # Plot training \u0026amp; validation loss values plt.figure(figsize=(16,7)) plt.plot(history.history['loss'], label='train') plt.plot(history.history['val_loss'], label='test') plt.legend() plt.show()     # Get the predicted values y_pred_scaled = model_from_saved_checkpoint.predict(x_test)  # Unscale the predicted values y_pred = scaler_pred.inverse_transform(y_pred_scaled)  # reshape y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))  # Mean Absolute Error (MAE) MAE = mean_absolute_error(y_test_unscaled, y_pred) print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}') # Mean Absolute Percentage Error (MAPE) MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100 print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %') # Median Absolute Percentage Error (MDAPE) MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100 print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')  # Plot of the true and predicted case counts plt.plot(y_test_unscaled, label='True') plt.plot(y_pred, label='LSTM') plt.title(\u0026quot;LSTM's_Prediction\u0026quot;) plt.xlabel('Time steps') plt.ylabel('Cases') plt.legend() plt.show()     # New data frame for predicting the next day count new_df = data_clean[-sequence_length:] # gets the last N days N = sequence_length  # Get the values of the last N day closing cases count # scale the data to be values between 0 and 1 last_N_days = new_df[-sequence_length:].values last_N_days_scaled = scaler.transform(last_N_days)  # Create an empty list and Append past N days X_test_new = [] X_test_new.append(last_N_days_scaled) # Convert the X_test data set to a numpy array and reshape the data pred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new)) pred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))  # Print last price, predicted price, and change percent for the next day cases_today = np.round(new_df['new_cases_smoothed'][-1]) predicted_cases = np.round(pred_cases_unscaled.ravel()[0]) change_percent = np.round(100 - (cases_today * 100)/predicted_cases)  # Code used to produce this article in jupyter notebook in hugo academic blog post !jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=. !cat covid_analysis.md | tee -a index.md !rm covid_analysis.md  ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://deepbas.netlify.app/project/internal-project/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Deepak Bastola"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://deepbas.netlify.app/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://deepbas.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://deepbas.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Deepak Bastola","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://deepbas.netlify.app/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Deepak Bastola","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://deepbas.netlify.app/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://deepbas.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"  After you have signed up for a GitHub account and gone through all of the account verification steps, you will need to generate a personal access token (PAT) and let R know what that is. To do this, I suggest following these steps:\nSetup options in Git by running the following code chunk in your console:  library(usethis) use_git_config(user.name = \u0026quot;Jane Doe\u0026quot;, user.email = \u0026quot;jane@example.org\u0026quot;) changing the first two lines to your own name and email (this should be the email associated with your GitHub account). Next, you should generate a PAT:  create_github_token() This function will preselect the recommended scopes. I recommend just clicking \u0026quot;Generate token\u0026quot; unless you know of a reason to tweak the scopes. Store this token somewhere, because you‚Äôll never be able to see it again once you leave that page or close the window. Treat this PAT like a password. Now that you have credentials, you need to store them. I recommend using the {gitcreds} package. Run the below chunk (after installing the package if you‚Äôre using your own laptop):  library(gitcreds) gitcreds_set() Respond to the prompt with your personal access token (PAT). You should be setup! Try to clone a GitHub project and talk to me if you still hit issues.  Acknowledgments This installation guide is based on the guide from Adam Loy.\n Reuse This work is is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"55e6948ee3f013088da2f6de7c897909","permalink":"https://deepbas.netlify.app/courses/stat220/pat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/courses/stat220/pat/","section":"courses","summary":"After you have signed up for a GitHub account and gone through all of the account verification steps, you will need to generate a personal access token (PAT) and let R know what that is.","tags":null,"title":"Cache credentials for RStudio and GitHub","type":"courses"}]